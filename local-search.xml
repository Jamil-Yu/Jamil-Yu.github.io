<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>[复习] PRML</title>
    <link href="/2023/06/12/PRML/"/>
    <url>/2023/06/12/PRML/</url>
    
    <content type="html"><![CDATA[<h1 id="prml总复习">PRML总复习</h1><p>[TOC]</p><p>TODO：</p><ul class="task-list"><li><input type="checkbox" disabled="" />BP</li><li><input type="checkbox" disabled="" />SVM</li><li><input type="checkbox" disabled="" />高斯混合聚类</li><li><input type="checkbox" disabled="" />PCA</li><li><input type="checkbox" disabled="" />决策树</li></ul><h2 id="chapter-2">Chapter 2</h2><p>五步：数据获取，预处理，特征提取与选择，学习器设计，应用部署</p><p><strong>机器学习</strong>：从样本集学习一个假设<spanclass="math inline">\(f(x)\)</span>使得<spanclass="math inline">\(f(x)\)</span>是问题世界模型<spanclass="math inline">\(F(x)\)</span>的一个近似。</p><h4 id="knn">kNN</h4><p>略</p><h4 id="解决过拟合">解决过拟合</h4><ul><li>获取更多数据</li><li>调整模型选择(比如加一些惩罚)</li></ul><h4 id="经验误差泛化误差">经验误差，泛化误差</h4><p>前者是在训练集上，后者是在测试集上；前者反映问题难易和学习器拟合能力，后者反映模型对未知数据的预测能力。样本足够多时，两者趋于一致。</p><p>经验误差大，泛化误差也大：欠拟合</p><p>经验误差小，泛化误差大：过拟合</p><h4 id="构建验证集">构建验证集</h4><p>验证集作用：帮助选择模型，确定参数超参数</p><ul><li>留出法</li><li>交叉验证(分K个也叫K折交叉验证)</li><li>自助法(不中概率趋向于<spanclass="math inline">\(\frac{1}{e}\)</span>)</li></ul><h4 id="性能度量">性能度量</h4><p>回归：均方误差；分类：错误率与准确率</p><p><strong>PR曲线</strong></p><p>查准率： <span class="math display">\[P = \frac{TP}{TP+FP}\]</span> 查全率： <span class="math display">\[R = \frac{TP}{TP+FN}\]</span> 可以根据这个绘制P-R曲线。曲线越靠右上角越好</p><p><strong>ROC曲线</strong></p><p>真正例率(TPR) = 灵敏度 <span class="math display">\[\frac{TP}{TP+FN}\]</span> 假正例率(FPR) = 1 - 特异度 <span class="math display">\[\frac{FP}{FP+TN}\]</span> FPR为横轴，TPR为纵轴可绘制ROC曲线，越靠近左上角越好</p><h4 id="两者关系">两者关系</h4><p>在样本不均衡情况下，ROC曲线保持不变，适合对学习器整体评估</p><p>PR曲线聚焦于正例，受影响比较大，适合对正例率敏感的问题</p><p>两者在空间上存在对应关系</p><p><strong>实际应用</strong></p><p>在问题中，定义 <span class="math display">\[AP = \int P(r)dr\]</span></p><p>体现准确度。</p><p>在<strong>语义分割</strong>任务中，通过IoU来体现</p><h4 id="泛化误差的理论分析">泛化误差的理论分析</h4><p>可以证明泛化误差： <span class="math display">\[E(f;\mathcal{D}) = var(x) + bias^2(x) + \varepsilon^2\]</span> 分解为方差、偏差与噪声之和。</p><p><strong>偏差</strong>：刻画学习算法本身拟合能力</p><p><strong>方差</strong>：刻画数据扰动造成的影响</p><p><strong>噪声</strong>：刻画学习问题本身的难度</p><p>欠拟合时，偏差大，通常方差也大</p><p>过拟合时，方差很大</p><h4 id="三个经典原理">三个经典原理</h4><ul><li>没有免费的午餐：不存在某种算法对所有问题都有效</li><li>丑小鸭定理：分类标准是主管定义的，评估与问题定义关联</li><li>奥卡姆剃刀定理：PRML系统不应选择比“必要”更复杂的系统</li></ul><h2 id="chapter-3">Chapter 3</h2><p><strong>线性模型</strong></p><p>形式化 <span class="math display">\[f(x) = w_1x_1 + w_2x_2 + \cdots + w_dx_d + b\]</span> 向量表示 <span class="math display">\[f(x) = w^Tx + b\]</span> 我们讨论回归任务和分类任务。</p><p>回归任务：（基于最小平方误差准则）</p><ul><li>一元线性回归</li><li>多元线性回归</li></ul><p>分类任务：</p><ul><li><p>线性判别分析(二/多分类) （基于Fisher准则）</p></li><li><p>感知机 （基于感知机准则）</p></li><li><p>Logistic回归， Softmax回归</p></li></ul><h4 id="一元线性回归">一元线性回归</h4><p>试图学习： <span class="math display">\[f(x_i) = wx_i +b\;\;使得\;\;f(x_i)\approx y_i\]</span> 定义损失函数 <span class="math display">\[E_{(w,b)} = \sum_{i=1}^n (y_i -wx_i -b)^2\]</span> 分别对<span class="math inline">\(w,b\)</span>求导令其为0，解得 <span class="math display">\[w = \frac{\sum_i y_i (x - \bar{x})}{\sum_i x_i^2 -\frac{1}{n}(\sum_{i}x_i)^2}\]</span></p><p><span class="math display">\[b = \frac{1}{n}\sum_i (y_i - wx_i)\]</span></p><h4 id="多元线性回归">多元线性回归</h4><p>试图学习： <span class="math display">\[f(x_i) = W^T x_i + b = w_1x_{i1} + \cdots + w_dx_{id} + b\]</span> 表示成矩阵 <span class="math display">\[X = \begin{pmatrix}x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1d} &amp; 1      \\\vdots &amp;  &amp; \ddots       &amp;        &amp; \vdots \\x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{nd} &amp; 1\end{pmatrix}\]</span></p><p><span class="math display">\[\hat{W} = (w;b)\]</span></p><p><span class="math display">\[y = (y_1; y_2; \cdots ;y_n)\]</span></p><p>则有 <span class="math display">\[y = X\hat{w}\]</span> 定义损失函数 <span class="math display">\[E_{\hat{W}} = (y-X\hat{W})^T(y-X\hat{W})\]</span> 对<span class="math inline">\(W\)</span>求导，可得 <spanclass="math display">\[2X^T(X\hat{W} - y) = 0\]</span> 若<span class="math inline">\(X^TX\)</span>满秩，则有 <spanclass="math display">\[\hat{W}^* = (X^TX)^{-1}X^Ty\]</span></p><ul><li>如果出现非满秩矩阵情况，说明存在多个最优解</li><li>面对多个最优解，可以加入正则化，如L1, L2</li></ul><h4 id="广义线性回归">广义线性回归</h4><p>如果有很明显的非线性关系，可以考虑 <span class="math display">\[g(y) = w^Tx + b\]</span> 则有 <span class="math display">\[y = g^{-1}(w^Tx+b)\]</span> 比如可以取<span class="math inline">\(g(\cdot) =\ln(\cdot)\)</span>, 则有 <span class="math display">\[y = e^{w^Tx+b}\]</span></p><hr /><p>接下来看看分类问题</p><h4 id="logistic回归">Logistic回归</h4><p>回顾广义线性模型，令<span class="math inline">\(g^{-1}(z) =\frac{1}{1+e^{-z}}\)</span>，则有 <span class="math display">\[y = \frac{1}{1+e^{-(w^Tx+b)}}\]</span> 将<span class="math inline">\(y\)</span>视为<spanclass="math inline">\(x\)</span>属于正类的概率<spanclass="math inline">\(p(y=1|x)\)</span>，则有 <spanclass="math display">\[p(y=1|x) = \frac{e^{w^Tx+b}}{1+e^{w^Tx + b}}\]</span> 对于二分类，可以用<strong>二值交叉熵</strong>来作为损失函数<span class="math display">\[J(w) = -\frac{1}{n}\sum_{i=1}^n \left[y_i\ln{p(y=1|x_i)}+(1-y_i)\ln{(1-p(y=1|x_i))}\right]\]</span> 为了便于讨论，令<span class="math inline">\(\hat{x} =(x;1)\;\;\beta = (w;b)\)</span>，则有<span class="math inline">\(w^Tx+b= \beta^T\hat{x}\)</span>，则 <span class="math display">\[p_1(\hat{x};\beta) = p(y=1|\hat{x};\beta) =\frac{e^{\beta^T\hat{x}}}{1+e^{\beta^T\hat{x}}}\]</span> 则 <span class="math display">\[J(\beta) = -\frac{1}{n} \sum_{i=1}^n[y_i\ln{p_1(\hat{x_i};\beta)+(1-y_i)\ln{(1-p_1(\hat{x_i};\beta)}})]\]</span></p><ul><li>梯度下降法</li></ul><p><span class="math display">\[\beta^{t+1} = \beta^{t} - \eta \frac{\partial J(\beta)}{\partial \beta}\]</span></p><p>其中 <span class="math display">\[\frac{\partial J(\beta)}{\partial \beta} = -\sum_{i=1}^n \hat{x_i}[y_i -p_1(\hat{x_i};\beta)]\]</span></p><ul><li>牛顿法</li></ul><p><span class="math display">\[\beta^{t+1} = \beta^t -(\frac{\partial^2J(\beta)}{\partial\beta\partial\beta^T})^{-1}\frac{\partialJ(\beta)}{\partial\beta}\]</span></p><h4 id="softmax回归">Softmax回归</h4><p>适用于多分类问题</p><p>描述<span class="math inline">\(x\)</span>属于第<spanclass="math inline">\(y = j\)</span>类的概率： <spanclass="math display">\[p(y = j|x;W) = \frac{e^{w_j^Tx + b_j}}{\sum_{k=1}^ce^{w_k^T+b_k}}\]</span> 同样的，为了方便 <span class="math display">\[\hat{x_i} = (x_i;1) \\\hat{w_j} = (w_j;b_j) \\W = [\hat{w_1}, \hat{w_2}, \cdots , \hat{w_c}]\]</span> 则有 <span class="math display">\[w_j^T x + b_j = \hat{w_j^T} \hat{x}\\p_j(\hat{x};\hat{W}) = p(y=j|\hat{x};\hat{W}) =\frac{e^{\hat{w}_j^T\hat{x}}}{\sum_{k=1}^c e^{\hat{w}_k^T\hat{x}}}\]</span></p><p><span class="math display">\[f(\hat{x}; \hat{W}) = \frac{1}{\sum_{k=1}^c e^{\hat{w}_k^T\hat{x}}}\begin{bmatrix}e^{\hat{w}_1^T\hat{x}} \\\vdots \\e^{\hat{w}_c^T\hat{x}}\end{bmatrix}\]</span></p><p>将交叉熵函数作为损失函数： <span class="math display">\[J(\hat{W}) = -\frac{1}{n} \sum_{i=1}^n \left[\sum_{j=1}^c\delta(y_i =j)\ln{p_j(\hat{x_i};\hat{W}})\right]\]</span> 可以用SGD来求解</p><h4 id="线性判别分析fisher判别分析">线性判别分析(Fisher判别分析)</h4><p><strong>LDA</strong></p><p>基本思想：设法将样例投影到一条直线上，使得同样样例的投影点尽量接近，不同类的互相远离</p><p><span class="math inline">\(X_i, n_i, \mu_i,S_{w_i}\)</span>分别表示第i类样本的集合、样本数、均值、类内散度</p><p>两类为例，想要：</p><ul><li>使得类内散度尽可能小，即<spanclass="math inline">\(w^TS_{w_0}w+w^TS_{w_1}w\)</span>尽可能小</li><li>使得类间离散度尽可能大，即 <span class="math inline">\(||w^T\mu_0 -w^T\mu_1||_2^2\)</span>尽可能大</li></ul><p>最大化目标函数： <span class="math display">\[J = \frac{||w^T\mu_0 - w^T\mu_1||_2^2}{w^TS_{w_0}w+w^TS_{w_1}w}\]</span> 简化计算 <span class="math display">\[J = \frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}{w^T(S_{w_0}+S_{w_1})w}\]</span> 定义类间散度矩阵 <span class="math display">\[S_b =(\mu_0-\mu_1)(\mu_0-\mu_1)^T\]</span> 定义类内散度矩阵 <span class="math display">\[S_w = S_{w0} + S_{w1} = \sum_{x\in w_0}(x-\mu_0)(x-\mu_0)^T + \sum_{x\inw_1}(x-\mu_1)(x-\mu_1)^T\]</span> 则最大化目标 <span class="math display">\[J = \frac{w^TS_bw}{w^TS_ww}\]</span> 用拉格朗日乘子法计算计算计算，最后得到 <spanclass="math display">\[w = S_w^{-1}(\mu_0-\mu_1)\]</span> <strong>多分类LDA</strong></p><p>有n个类</p><p>定义全局散度矩阵<spanclass="math inline">\(S_t\)</span>，类内散度矩阵<spanclass="math inline">\(S_w\)</span>，类间散度矩阵<spanclass="math inline">\(S_b\)</span></p><p>最大化目标函数： <span class="math display">\[J(W) = \frac{\Pi_{diag}W^TS_bW}{\Pi_{diag}W^TS_WW} = \Pi_{i=1}^k\frac{w_i^TS_bw_i}{w_i^TS_ww_i}\]</span></p><h4 id="感知器准则">感知器准则</h4><p>目标：把两类分开，利用线性判别函数 <span class="math display">\[g(x) = w^Tx + b\]</span> 增广向量：<span class="math inline">\(y_i = (1;x_{i1}; \cdots; x_{id})\)</span>, <span class="math inline">\(a = (b;w_1;\cdots;w_d)\)</span></p><p>则有 <span class="math display">\[g(y) = a^Ty\]</span> 定义<strong>线性可分性</strong>：</p><p>存在权向量<span class="math inline">\(a\)</span> 使得 <spanclass="math display">\[\forall y \in w_1 \Rightarrow a^Ty &gt;0 \\\forall y \in w_2 \Rightarrow a^Ty &lt; 0\]</span> 则为线性可分</p><p>规范化，让所有属于<span class="math inline">\(w_2\)</span>的<spanclass="math inline">\(y\)</span>取相反数，这样所有的元素都要<spanclass="math inline">\(a^Ty&gt;0\)</span></p><p>这样可以得到解向量<spanclass="math inline">\(a\)</span>的一个解空间，越靠近解区中间越准确。下面定义损失函数<span class="math display">\[J_P(a) = \sum_{y\in Y} (-a^Ty)\]</span> 其中<span class="math inline">\(Y\)</span>为错分样本的集合</p><p>利用SGD更新参数： <span class="math display">\[a_{k+1} = a_k - \eta_k\frac{\partial J_P(a)}{\partial a } = a_k + \eta_k\sum_{y\in Y_k}y\]</span> 也有几个不同的更新方法：</p><ul><li>固定增量的单样本修正方法 每次只考虑一个错分样本<spanclass="math inline">\(y^k\)</span>, 改写为<spanclass="math inline">\(a_{k+1} = a_k + y^k\)</span></li><li>可变增量单样本修正方法 <span class="math inline">\(a_{k+1} = a_k +\eta_ky^k\)</span></li></ul><p>[这边可以出计算题]</p><h4 id="多分类问题">多分类问题</h4><p><strong>几个方法</strong></p><ul><li>每次只保留两类，训练一个分类器</li><li>每次处理一个类别时，把其他所有类别看作一类</li><li>可以把几个看作一类，几个看作一类进行分类</li></ul><p><strong>类别不平衡问题</strong></p><ul><li>阈值移动</li></ul><p>​ 认为若<span class="math inline">\(\frac{y}{1-y} &gt;\frac{m^+}{m^-}\)</span>则预测为正样本</p><p>​ 或者说，比较时缩放：<spanclass="math inline">\(\frac{y&#39;}{1-y&#39;} = \frac{y}{1-y}\times\frac{m^-}{m^+}\)</span></p><p>如果正样本数量过少</p><ul><li>过采样：增加正例</li><li>欠采样：去除一些负例</li><li>数据增广：对正例进行一些变换而不改变语义</li></ul><h2 id="chapter-4">Chapter 4</h2><p>从线性到非线性</p><ul><li>之前提到过的：广义线性。但是拟合能力弱，泛化性不强。</li><li>分段线性：简单，多个超平面适应性强。但是依赖手工设计。</li></ul><p>将提出:</p><ul><li>神经网络</li><li>SVM</li></ul><h4 id="神经网络">神经网络</h4><h5 id="mp神经元">MP神经元</h5><p>最经典的神经元模型，略</p><h5 id="hebb学习规则">Hebb学习规则</h5><p>核心思想：突触前神经元<spanclass="math inline">\(j\)</span>向突触后神经元<spanclass="math inline">\(i\)</span>的重复持续的刺激可以导致突出传递效能增加，即<spanclass="math inline">\(j\)</span>到<spanclass="math inline">\(i\)</span>的权值得到加强 <spanclass="math display">\[\Delta w_{ij} = \eta a_j o_i\]</span> <span class="math inline">\(a_j\)</span>为突触前神经元输入值，<spanclass="math inline">\(o_i\)</span>为突触后神经元输出值</p><h5 id="单层感知机">单层感知机</h5><p>之前介绍过，不赘述。</p><p>可用于处理与或非的问题，不能处理异或问题。</p><h5 id="多层前馈神经网络">多层前馈神经网络</h5><p>每层与下一层全连接，在网络拓扑上不存在环与回路</p><p><strong>万有逼近定理</strong>：单一隐层，任意宽度，使用S型函数作为激活函数的前馈神经网络，可以以任意精度来近似任何从一个有限维空间到另一个有限维空间的Borel可测函数</p><h5 id="误差反向传播算法bp">误差反向传播算法（BP）</h5><h5 id="常见神经网络">常见神经网络</h5><ul><li>前馈神经网络</li><li>反馈神经网络</li><li>循环神经网络</li><li>RBF网络径向基函数网络，用一个径向基函数作为隐层神经元激活函数。比如高斯径向基函数</li></ul><p><span class="math display">\[R(x-c_i) = \exp{(-\frac{1}{2\sigma^2}||x-c_i||^2)}\]</span></p><ul><li>LeNet5 典型的卷积神经网络，手写识别</li><li>Hopfield网络属于反馈神经网络，提供了一个理解人类记忆的模型，有联想记忆的功能</li><li>Boltzmann机基于能量的模型，为网络定义能量，能量最小化达到理想状态</li><li>Elman网络针对语音处理，具有局部记忆单元和局部循环连接的循环神经网络</li><li>LSTM 长短期记忆网络</li><li>SOM网络 自组织映射网络：竞争学习类型的无监督神经网络输出是一个二维的神经元网格，输入代表真实世界的模式。可以用于求解旅行商问题</li><li>级联相关网络结构自适应网络。将网络结构作为训练目标之一，开始只有输入输入层，后面逐渐加入新的隐层结点从而创建层级结构</li></ul><h4 id="svm">SVM</h4><h2 id="chapter-5">Chapter 5</h2><p><strong>概率模型</strong></p><p>一些概率论的基础不再赘述，看一眼多元高斯密度函数 <spanclass="math display">\[p(x) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}}e^{[-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)]}\]</span> 其中<span class="math inline">\(\mu\)</span>为均值， <spanclass="math inline">\(\Sigma\)</span>为协方差</p><h4 id="贝叶斯分类器">贝叶斯分类器</h4><h5 id="贝叶斯决策理论">贝叶斯决策理论</h5><p><span class="math display">\[p(w|x) = \frac{p(x|w)p(w)}{\sum_wp(w, x)}\]</span></p><h5 id="最小错误贝叶斯决策">最小错误贝叶斯决策</h5><p><span class="math display">\[minP(e) = \int P(e|x)p(x)dx\]</span></p><p>决策： <span class="math display">\[if \;P(w_1|x)&gt; P(w_2|x), assign \;x\in w_1\\if \;P(w_1|x)&lt; P(w_2|x), assign \;x\in w_2\]</span> 等价的有 <span class="math display">\[\max_i p(x|w_i)p(w_i)\]</span> 还有一些等价形式，只是数学上简单的变换，不再赘述</p><p>一些定义：</p><ul><li><p>第一类错误率 <span class="math inline">\(\alpha =\frac{FP}{FP+TN}\)</span> 假阳</p></li><li><p>第二类错误率 <span class="math inline">\(\beta =\frac{FN}{FN+TP}\)</span> 假阴</p></li><li><p>灵敏度 <span class="math inline">\(S_n = \frac{TP}{FN+TP} =1-\beta\)</span> 真阳中检测出多少阳</p></li><li><p>特异度 <span class="math inline">\(S_p = \frac{TN}{FP+TN} =1-\alpha\)</span> 真阴中检测出多少阴</p></li></ul><p>##### 最小风险贝叶斯决策</p><p>定义对于实际状态为<spanclass="math inline">\(w_j\)</span>的样本，采取策略<spanclass="math inline">\(a_i\)</span>带来的损失为<spanclass="math inline">\(\lambda(a_i, w_j)\)</span></p><p>对于每一个样本x，期望损失为 <span class="math display">\[R(a_i| x) = \sum_{j = 1}^c \lambda(a_i, w_j)p(w_j|x)\]</span> 为了最小化决策期望损失<span class="math inline">\(R(a) = \intR(a(x)|x)p(x)dx\)</span></p><p>只要对每一个样本最小化<spanclass="math inline">\(R(a_i|x)\)</span></p><h5 id="判别函数">判别函数</h5><p>线性判别函数： <span class="math inline">\(g_i(x) = w_i^Tx+w_{i0}\)</span></p><p>对于二分类问题，若<span class="math inline">\(g_1(x) \geg_2(x)\)</span>则认为属于第一类，否则属于第二类。称<spanclass="math inline">\(g(x) = g_1(x) -g_2(x)\)</span>确定的平面为<strong>决策面</strong></p><p>现在回到原来的贝叶斯决策分类器，看最小错误率决策，那么我们以<strong>广义似然度</strong>作为判别函数：<span class="math display">\[g_i(x) = p(x|w_i)p(w_i)\]</span> 其正比于后验概率。</p><p>类似的，也可以采用对数似然的形式简化计算： <spanclass="math display">\[g_i(x) = \log{p(x|w_i)} + \log{p(w_i)}\]</span> 分类决策： <span class="math display">\[arg\max_i\;g_i(x)\]</span></p><h5 id="正态分布下的判别函数">正态分布下的判别函数</h5><p>TODO</p><h4 id="概率密度函数估计">概率密度函数估计</h4><p>贝叶斯决策目的是计算最大后验概率，但是先验概率和类条件概率不一定知道。要通过样本估计</p><p>估计概率密度函数：</p><ul><li>参数方法</li><li>非参数方法</li></ul><p>估计有效性：需要样本充足，能代表样本的真实分布，独立同分布</p><h5 id="参数估计">参数估计</h5><ul><li>最大似然估计</li><li>贝叶斯估计</li></ul><h6 id="最大似然估计">最大似然估计</h6><p>样本D <span class="math display">\[L(\theta) = p(D|\theta) = p(x_1, \cdots, x_N|\theta) =\Pi_{i=1}^np(x_i|\theta)\]</span></p><p><span class="math display">\[\hat{\theta} = argmax_{\theta}p(D|\theta)\]</span></p><p><span class="math display">\[l(\theta) = \sum_{i=1}^N \ln{p(x_i|\theta)}\]</span></p><p>求最大值，求导为0即可。</p><p>高斯情况下的最大似然估计：</p><p>TODO</p><h6 id="贝叶斯估计">贝叶斯估计</h6><p>记<span class="math inline">\(\lambda(\hat\theta,\theta)\)</span>为估计的损失，条件风险为<spanclass="math inline">\(R(\hat{\theta}|x) = \int \lambda(\hat{\theta},\theta)p(\theta|x)d\theta\)</span></p><p>在平方损失函数下，可以证明 <span class="math display">\[\hat{\theta} = \int \theta p(\theta|D)d\theta\]</span> 流程：</p><ul><li>求先验分布<span class="math inline">\(p(\theta)\)</span></li><li>求联合分布<span class="math inline">\(p(D|\theta) =\Pi_{i=1}^Np(x_i|\theta)\)</span></li><li>求<span class="math inline">\(\theta\)</span>后验概率分布<spanclass="math inline">\(p(\theta |D)\)</span></li><li>计算<span class="math inline">\(\hat{\theta} = \int \thetap(\theta|D)d\theta\)</span></li></ul><p>高斯情况下的贝叶斯估计：</p><p>TODO</p><h5 id="非参数估计">非参数估计</h5><h6 id="直方图方法">直方图方法</h6><p>N个样本，把<span class="math inline">\(x\)</span>分成<spanclass="math inline">\(k\)</span>个等间隔小窗，形成<spanclass="math inline">\(k^d\)</span>个小舱，小舱体积为<spanclass="math inline">\(V\)</span></p><p>统计进入小舱的样本数<span class="math inline">\(q_i\)</span></p><p>相应小窗的概率密度为<spanclass="math inline">\(\frac{q_i}{NV}\)</span></p><p>随着样本数增加，小舱体积应该尽可能小，但是小舱内有充分多样本，这些样本是所有样本中很小的一部分。</p><p>换个样子写： <span class="math display">\[\hat{p}(x) = \frac{k}{NV}\]</span> <span class="math inline">\(V\)</span>为指定的区域，<spanclass="math inline">\(k\)</span>为区域中样本数。选取合适的<spanclass="math inline">\(k, V\)</span></p><ul><li>Parzen 窗法：固定V，变化k</li><li>K-近邻法：固定k，变化V</li></ul><h4 id="朴素贝叶斯分类器">朴素贝叶斯分类器</h4><p>贝叶斯公式后验概率： <span class="math display">\[P(c|x) = \frac{p(c)p(x|c)}{p(x)}\]</span>想象x是很多属性，c是某个最终判定的类别。所有属性的联合概率难以直接从训练样本估计，为了简化假设它们独立<span class="math display">\[P(c|x) = \frac{P(c)}{P(x)}\Pi_{i=1}^d P(x_i|c)\]</span> 其中 <span class="math display">\[P(c) = \frac{|D_c|}{D}, P(x_i|c) = \frac{|D_{c, x_i|}}{D_c} 或 P(x_i|c)= \frac{1}{\sqrt{2\pi}\sigma_{c, i}}e^{(-\frac{(x_i - \mu_{c, i})^2}{2\sigma_{c, i}^2})}\]</span> <strong>TODO：看看书里的例题</strong></p><p>拉普拉斯修正： <span class="math display">\[P(c) = \frac{|D_c|+1}{D+1}, P(x_i|c) = \frac{|D_{c, x_i}|+1}{D_c + N_i}\]</span></p><h4 id="半朴素贝叶斯分类器">半朴素贝叶斯分类器</h4><p>考虑一部分属性之间的相互关系。假设各属性在类别之外最多只依赖于一个其他属性。<span class="math display">\[P(c|x) 正比于 P(c)\Pi_{i=1}^dP(x_i|c,pa_i)\]</span> 确定各个属性的父属性的方法：</p><ul><li>SPODE假定各属性都依赖于同一属性（超父），再通过模型选择确定最终超父属性</li><li>TAN通过两属性条件互信息反映相关性强弱，继而保留相关属性之间的依赖性</li><li>AODE 与SPODE相比 不进行模型选择，采用集成各种模型的思路</li></ul><p>TODO：计算</p><h4 id="贝叶斯网">贝叶斯网</h4><p>有向无环图模型，节点表示随机变量，箭头表示因果关系</p><h4 id="贝叶斯方法延展">贝叶斯方法延展</h4><p>贝叶斯决策和贝叶斯估计都基于贝叶斯定理，解决不同问题。贝叶斯决策的任务是学习一个分类器，贝叶斯估计的目的是学习概率模型中的参数。</p><p>过拟合的时候，可以考虑引入正则化。</p><p>最大似然估计：参数的值看作给定量，只是不知道</p><p>贝叶斯估计：参数也看成随机变量</p><h4 id="决策树没写完">决策树（没写完）</h4><p>定义：一种非参数监督的分类与回归方法，推理过程与人类进行分类的机制类似，根据对象属性分情况进行讨论</p><p>对于一个样本，决策树的推理过程对应于一条从根节点到某一叶节点的路径，也即一个判定测试序列。</p><p>if-then 序列，互斥且完备。将特征空间划分为矩形平行区域</p><ul><li>优点：易于理解，可以处理离散连续数据，层次反映信息量，需要数据少</li><li>缺点：不稳健，数据变化造成结构大幅改变，贪心算法不能保证返回最优决策树，用分段近似不平滑不连续</li></ul><p>决策树的生成是一个递归问题，终止条件：</p><ul><li></li></ul><h4 id="概率图模型">概率图模型</h4><p>概率图模型是一类用图来表示变量间因果或相关关系的概率模型，按图的结构可分有向图、无向图模型。</p><ul><li>隐马尔可夫模型：关于时序的生成式有向图模型，建模观测序列和状态序列的联合分布</li><li>马尔可夫随机场：生成式无向图模型，联合概率可分解为所有极大团势函数乘积</li><li>条件随机场：判别式无向图模型，看作给定观测值条件下的马尔可夫随机场，建模的是给定观测值下的条件分布</li><li>精确推理<ul><li>变量消去：无向图模型，两两向量当作极大团</li><li>信念传播：无环概率图模型，先从叶节点到根节点传递消息，再从根节点到叶节点传递消息</li></ul></li><li>近似推理<ul><li>采样法：比如拒绝采样</li><li>变分法：用简易分布近似复杂的真实分布</li></ul></li><li>话题模型：生成式有向图模型，主要处理离散数据的集合</li></ul><h2 id="chapter-6">Chapter 6</h2><p>集成学习</p><h4 id="bagging">Bagging</h4><p><span class="math display">\[H(x) = \frac{1}{T}\sum_{t=1}^Th_t(x)\]</span></p><p>自助采样法得到数据，训练，而后各个弱分类器投票得到结果 <spanclass="math display">\[H(x) = arg\max_{y\in \mathcal{Y}}\sum_{t=1}^T 1(h_t(x) = y)\]</span> <strong>包外估计</strong>：</p><p>用训练好的模型估计没有采样到的数据的结果，用结果来计算泛化误差的估计</p><p><strong>随机森林</strong></p><p>自助采样样本和属性，构建不同的决策树形成森林，最后决策为各个决策树投票的结果</p><p>回顾之前的：偏差大意味着欠拟合，方差大意味着过拟合。Bagging算法可以降低方差。</p><h4 id="boosting">Boosting</h4><p><span class="math display">\[H(x) = \sum_t \alpha_t h_t(x)\]</span></p><p>通过迭代更新训练样本的权重分布。对弱分类器重新加权得到强分类器，串行生成。</p><p>重点介绍<strong>Adaboost算法</strong></p><p>训练集：$D = {(x_1, y_1), , (x_N, y_N) } $ 其中<spanclass="math inline">\(y_i \in \mathcal{Y} =\{-1, +1 \}\)</span></p><p>初始化训练数据权重分布<span class="math inline">\(D_1 = (w_{11},\cdots, w_{1i}, \cdots, w_{1n})\)</span>，其中<spanclass="math inline">\(w_{1i} = \frac{1}{n}\)</span></p><p>对于<span class="math inline">\(t = 1, 2, \cdots, T\)</span>:</p><ul><li>选择一个弱分类器<span class="math inline">\(h_t(x)\)</span></li></ul><p><span class="math display">\[h_t(x) = arg\min_{h_j\in H}[\varepsilon_j = \sum_{i=1}^nw_{t,i} \cdot1[y_i \ne h_j(x_i)]]\]</span></p><p>​ 也就是说从弱分类器中选错误率最小的一个作为初始的分类器</p><ul><li><span class="math inline">\(\alpha_t =\frac{1}{2}\log{(\frac{1-\varepsilon_t}{\varepsilon_t})}\)</span>也就是说错的约多<span class="math inline">\(\alpha\)</span>越小</li><li><span class="math inline">\(w_{t+1,i} = \frac{w_{t,i}\exp{(-\alpha_ty_ih_t(x_i))}}{Z_t}\)</span>, <spanclass="math inline">\(Z_t =\sum_{i=1}^Nw_{t,i}\exp{(-\alpha_ty_ih_t(x_i))}\)</span>也就是说，某个样本已经被分得越好，权重越小</li></ul><p>最后输出 <span class="math display">\[H(x) = \sum_t \alpha_t h_t(x)\]</span> 此算法可以降低偏差。</p><p>TODO：算算例题</p><h4 id="stacking">Stacking</h4><p>常见的结合策略有：简单平均、加权平均、绝对多数投票法、相对多数投票法、加权投票法</p><p>接下来介绍学习法。</p><p><img src="C:\Users\12484\AppData\Roaming\Typora\typora-user-images\image-20230612153201543.png" alt="image-20230612153201543" style="zoom:67%;" /></p><p>应用实例：</p><p>TODO</p><h2 id="chapter-7">Chapter 7</h2><p>无监督学习</p><ul><li>聚类问题</li><li>降维问题</li><li>生成式问题</li></ul><h4 id="无监督聚类">无监督聚类</h4><h5 id="k均值聚类算法">k均值聚类算法</h5><p>思想：首先随机初始化聚类中心，而后不断更新聚类中心、更新聚类样本直到收敛</p><p>有N个样本，K个类别。定义准则函数 <span class="math display">\[J = \sum_{i=1}^K \sum_{x\in C_i} ||x-\mu_i||^2 = \sum_{i = 1}^N\sum_{k=1}^K r_{ik}||x-\mu_i||^2\]</span> 其中<span class="math inline">\(r_{ik}\)</span>表示<spanclass="math inline">\(x_i\)</span>属于第<spanclass="math inline">\(k\)</span>个聚类。</p><ul><li>第一步，初始化<span class="math inline">\(\mu_k\)</span>,按照最优化准则产生<span class="math inline">\(r_{ik}\)</span></li><li>第二步，根据<spanclass="math inline">\(r_{ik}\)</span>按照最优化准则产生<spanclass="math inline">\(\mu_k = \frac{\sum_ir_{ik}x_i}{\sum_{i}r_{ik}}\)</span></li><li>第三步，根据<span class="math inline">\(\mu_k\)</span>产生新的<spanclass="math inline">\(r_{ik}\)</span></li></ul><p>k值选择：找拐点</p><p>优点：高效，迭代收敛</p><p>缺点：k要指定，无法处理噪声和离群数据，不利于非凸数据</p><p>TODO：做例题</p><h5 id="学习向量化方法">学习向量化方法</h5><p>有标签，但是不一定准确</p><h6 id="lvq-算法">LVQ 算法</h6><p>样本<span class="math inline">\(D = \{(x_1, y_1), \cdots, (x_N, y_N)\}\)</span>, 每个样本由n个属性描述：<span class="math inline">\((x_{j1};x_{j2}; \cdots;x_{jn})\)</span></p><p>输出：学习一组n维原型向量<span class="math inline">\(p_1, p_2,\cdots, p_q\)</span>每个原型向量代表一个聚类簇，簇标记为<spanclass="math inline">\(t_i\)</span></p><p>准则函数： <span class="math display">\[J = arg\min_{0\le i\le q} ||x-p_i||^2\]</span> 关键步骤：</p><p>对于每个样本<spanclass="math inline">\(x_j\)</span>找到最接近的原型<spanclass="math inline">\(p_i\)</span></p><ul><li>若两点相同，把<span class="math inline">\(p_i\)</span>拉向<spanclass="math inline">\(x_j\)</span> : <spanclass="math inline">\(p_i&#39; = p_i +\eta (x_j - p_i)\)</span></li><li>两点不同，相反：<span class="math inline">\(p_i&#39; = p_i -\eta(x_j - p_i)\)</span></li></ul><p>TODO: 做例题</p><h5 id="高斯混合聚类算法不会">高斯混合聚类算法（不会）</h5><p>（？？？）</p><h5 id="层次化聚类算法">层次化聚类算法</h5><p>有两种方法</p><ul><li>自顶向下：从单个集群的所有数据开始，考虑将集群一分为二的所有可能方法选择最好的划分，对两边递归操作。</li><li>自底向上：从样本自己的集群中开始，找到最好的一对以合并到一个新的集群之中。重复直到所有的簇融合在一起</li></ul><p>以自底向上为例：</p><ul><li>初始化：每个样本为一簇</li><li>合并：计算任意两簇之间距离，最小的合并，记录</li><li>重复</li></ul><p>簇划分：可以手动选择阈值</p><h5 id="密度聚类">密度聚类</h5><p>聚类结构不是球状时，基于聚类的算法聚类效果不好</p><p>密度聚类：基于密度的聚类，假设聚类结构能通过样本分布的紧密程度来确定</p><p>典型算法：DBSCAN</p><ul><li>若一个点的<spanclass="math inline">\(\varepsilon\)</span>邻域包含至少<spanclass="math inline">\(MinPts\)</span>个样本，则称其为一个核心点</li><li>密度直达（不对称）</li><li>密度可达（不对称）</li><li>密度相连（对称）</li></ul><p>DBSCAN算法: 从核心对象出发，找到所有密度可达的构成簇。重复。</p><p>优点：对噪声、离群点鲁棒，可以构成非凸簇</p><p>缺点：多超参敏感，不适用一个簇内多密度情况</p><h4 id="降维学习">降维学习</h4><p>维度灾难：增加维度，所需训练样本数量呈指数级增加，否则过拟合</p><ul><li>线性方法<ul><li>PCA</li><li>LDA</li></ul></li><li>非线性方法<ul><li>保留局部特征<ul><li>重建权值LLE</li><li>邻接图LE</li><li>切空间Hession LLE</li></ul></li><li>保留全部特征<ul><li>基于距离<ul><li>基于欧氏距离MDS</li><li>基于测地线距离ISOMAP</li></ul></li><li>基于核<ul><li>核PCA</li></ul></li></ul></li></ul></li></ul><h5 id="主成分分析pca">主成分分析（PCA）</h5><p>是一种无监督的数据降维技术，是数据从高维空间到低维子空间的正交投影变换，使得投影数据的方差最大化频率。</p><p>通过线性变化，使得一组正交向量来表示原特征，新的特征向量是原特征向量的线性组合，转换后这组向量叫做主成分。</p><p>核心思想：</p><ul><li>降维后方差尽量大</li><li>降维后数据均方误差尽可能小</li></ul><p>在相关性最强的方向建立坐标。</p><h5 id="线性判别分析">线性判别分析</h5><p>Fisher判别</p><p>核Fisher判别：将样本通过核函数映射到新的特征空间中</p><h5 id="流形学习">流形学习</h5><p>低维流形嵌入到了高维空间，在局部上仍然具有欧氏空间的性质</p><p>多维尺度变换MDS：找到一个低维空间使得样本间的距离和在高维空间中的基本一致</p><p>等距映射ISOMAP：引入测地距离。保持测地距离</p><p>局部线性嵌入LLE：给定数据集，通过最近邻等方式构造一个数据图，然后在每一个局部区域，高维空间中样本线性重构关系在低维空间中均得以保持</p><p>拉普拉斯特征映射LE：在高维空间中距离近的点在低维空间中也近，LE将问题转化为求解图拉普拉斯算子的广义特征值问题</p><h2 id="chapter-8">Chapter 8</h2><p>半监督</p><p>有一些有标注的数据和大量无标注的数据</p><p>半监督学习：让学习器不依赖外界交互、自动地利用未标记样本来提升模型的学习性能</p><p>三大假设：</p><ul><li>平滑假设</li><li>聚类假设</li><li>流形假设</li></ul><p>归纳式半监督学习：学习一个函数来适用于训练过程中未观察到的数据</p><p>直推式半监督学习：学习一个函数来预测训练集中未标记样本的数据</p><h4 id="自训练方法">自训练方法</h4><ul><li><p>得到有标记数据<span class="math inline">\(\mathcal{L} = \{(x_i,y_i)\}\)</span>, 无标记数据<span class="math inline">\(\mathcal{U} =\{y_i\}_{j=l+1}^{l+u}\)</span>，训练出一个模型</p></li><li><p>用模型预测无标记数据，得到伪标签</p></li><li><p>选择伪标记中置信度较高的，认定为有标记</p></li><li><p>循环</p></li></ul><p>评价：构成分离的簇，置信度较高的往往正确；正确标记大多样本才能提升精度，否则可能错上加错</p><h4 id="半监督生成方法">半监督生成方法</h4><ul><li><p>假设所有数据都是一个潜在的模型生成的，比如高斯混合模型</p></li><li><p>根据最大化后验概率来预测每个样本的标签</p></li><li><p>通过期望最大化(EM)迭代更新高斯混合模型的参数</p></li><li><p>不断迭代到收敛</p></li></ul><p>评价：要求对数据分布做出假设，可以被视为一种软标签的自训练方法，所有无标记数据都被用来更新模型</p><h4 id="半监督支持向量机">半监督支持向量机</h4><p>试图找到将两类标记分开且穿过数据低密度区域的超平面</p><p>著名方法：TSVM</p><ul><li>利用有标记样本学得一个SVM</li><li>对未标记数据进行标记指派，将预测结果作为伪标记</li><li>求出新的划分超平面和松弛向量</li><li>找两个标记不同且很有可能出错的，交换</li><li>重新求解更新后划分超平面和松弛向量</li><li>增大未标记样本的重要性来提高影响</li><li>重复3-6直到重要程度一致</li></ul><h4 id="基于图的半监督学习">基于图的半监督学习</h4><p>构建连接相似样本的图结构，根据边的权重，将标签赋予图中未标记的节点</p><p>亲和矩阵，权重大表示两个标签相同</p><p>可以定义能量函数，最后实现标记传播。</p><p>基于图的半监督学习属于直推学习，但是新增测试样本无法直接预测标签，需要归纳式半监督学习。可以通过引入正则化，允许偏离真实标签，惩罚偏离程度。(流形正则化问题)</p><ul><li>概念上很清晰</li><li>计算开销大</li></ul><h4 id="基于分歧的方法">基于分歧的方法</h4><p>借助学习器之间的“分歧”来利用未标记数据</p><p>协同学习：多个学习器互相学习共同进步</p><p>基本假设：</p><ul><li>数据拥有两个充分且条件独立的视图(充分：每个视图包含足以产生最优学习器的信息；条件独立：给定类别标记下两个视图独立)</li><li>不同视图有相容性(相容性：不同视图包含的关于输出空间的信息是一致的)</li></ul><p>例如：网页中，文字和图片的关系</p><p>一起训练，都觉得之心度高，就从伪标注样本中去除</p><p>评价：简单有效，适用范围广。但是要使用这个方法，需要显著分歧的学习器，在有标记样本少、数据没有多视图时难以做到</p><h2 id="chapter-9">Chapter 9</h2><p>特征提取与选择</p><h4 id="lbp特征">LBP特征</h4><p>看一个3x3，以中间为基准，不考虑强度绝对值，只考虑相对大小。写成0，1，以左上角为起始位置，每个3x3可以写成8位二进制编码。</p><p>拓展：可以把3x3推广到任意邻域</p><p>解决旋转不变性：循环将每个位置作为起始点编码，选取编码最小的作为最终结果</p><p>考虑多尺度：取一块块的区域，区域内像素取均值，以块为单位来比较</p><p>减少编码空间：用跳变数区分，大大减少编码空间</p><h4 id="sift特征">SIFT特征</h4><p>尺度不变特征。检测图像的关键点用以表征和匹配成对的图像。应当与位置、光照、噪声等无关。</p><ul><li>检测关键点位置——通过尺度金字塔DoG</li><li>精细化关键点位置——过滤低对比度点、边缘点</li><li>计算关键点方向——统计得到区域主方向</li><li>计算关键点描述子——堆叠多个元胞方向直方图</li></ul><h4 id="特征选择">特征选择</h4><ul><li>避免维度灾难</li><li>降低计算代价</li><li>排除无关干扰</li><li>降低学习难度</li></ul><p>提升选择特征的效率：</p><ul><li>更高效的特征子集评价方式，避免完整交叉验证</li><li>更高效特征子集搜索方式，避免枚举遍历全部子集</li></ul><p>评价指标</p><ul><li>基于距离的：类内散度小，类间距离大</li><li>基于信息熵：信息熵越小，越容易区分</li></ul><p>特征搜索战略</p><p>迭代搜索（不断寻找更好的特征子集划分，直到达到要求或限制）</p><ul><li>贪心<ul><li>前向搜索：每次都从剩下的特征中搜索选择一个最佳的</li><li>后向搜索：每次从已选特征中排除一个最差的，直到满足条件</li><li>双向搜索：每次加入若干特征同时剔除若干特征</li></ul></li></ul><p>贪心搜索不一定是全局最优解</p><ul><li>随机<ul><li>遗传算法：以染色体适应度为概率选择个体，进行交叉、变异，直到满足条件</li></ul></li></ul><p><strong>综合方法</strong></p><p>将评价指标与搜索策略相结合</p><ul><li><p>过滤式 特征选择与学习器学习独立 如Relief选取指定个数的最大相关统计量的特征或选取相关度高于一定阈值的特征与猜中近邻的距离小则相关统计量大，与猜错近邻的距离大则相关统计量大优点：复杂度正比于采样数、样本数、特征维度，效率高不足：只能处理两类问题拓展到多分类：猜中近邻为相同样本最近邻，猜错近邻为每一不同类别样本最近邻</p></li><li><p>包裹式特征选择与学习器关联，直接利用学习器的性能作为特征子集评价准则 如LVW。随机产生特征子集，交叉验证评估特征子集质量，选择是否接受（性能更好或者性能一样但是维度更低）优点：直接用目标分类器评估，量身定做 缺点：代价比较高</p></li><li><p>嵌入式</p><p>特征选择与学习器融为一体，可以同时优化——自动舍弃不对的。还可以用正则项来约束经典方法：LASSO L1范数更容易获得稀疏解(画图可以理解)</p></li></ul><h2 id="老师说的重点">老师说的重点</h2><p>C1</p><p>基本概念，概念间的关系</p><p>重要的人和事，华人</p><p>理论流派。</p><p>几个技术的背景知识，跟设计题有关。</p><p>C2</p><p>系统层面的模式识别和机器学习的定义</p><p>五个环节要考，数据获取 预处理 特征提取选择 学习器设计 部署</p><p>模型的评估与选择：经验误差泛化误差的意义</p><p>知道为什么要做模型评估与选择，构建验证集，3个方法</p><p>性能度量：常见任务，混淆矩阵，PR，ROC</p><p>泛化误差的方差偏差的分解</p><p>小结中的结论要掌握</p><p>C3</p><p><strong>计算题：SVM，聚类</strong></p><p>线性模型的评价策略，概念</p><blockquote><p>优势：简单，可解释性强，是通往非线性模型的基础。</p><p>模型的评价策略：最小平方误差准则，Fisher准则，感知机准则，其他准则（logistic，softmax）</p></blockquote><p>机器学习三要素，优势劣势</p><blockquote><p>三要素：模型，策略，算法。模型：定义函数集合。策略：函数拟合评价。算法：最优函数选择。</p></blockquote><p>评价策略：三个策略</p><p>logistic，softmax，线性判别：概念要知道。</p><p>线性判别分析的核心思想优化目标，如何扩展到多分类和非线性</p><blockquote><p>核心思想：投影到一条直线上，同类尽可能接近，不同尽可能原理</p><p>类内散度要小，类间距离要大</p><p>拓展到多分类:不只是投影到直线，而是一个超平面<strong>(???)</strong></p></blockquote><p>思想和优势要掌握</p><p>线性不可分什么的不考了</p><p>C4</p><p>历史还是在第一章）</p><p>感知机是重点，能干什么不能干什么，优缺点，万有逼近</p><blockquote><p>可以与或非不能异或</p><p>万有逼近：单一隐层，任意宽度，用S型函数作为激活函数的前馈神经网络可以用任意精度近似任何一个从有限维空间到另一个有限维空间的Borel可测函数（有阶梯函数、连续函数、分段连续函数等等）</p></blockquote><p>BP了解下就好</p><p>常见神经网络需要掌握。</p><blockquote><p>MP，Hebb(突触增强)</p><p>前馈：RBF网络(径向基函数作为激活函数)。LeNet:卷积神经网络</p><p>反馈：Hopfield网络(记忆，可以将输入图片和记忆图片关联)波尔兹曼机(基于前者，能量，随机递归神经网络)</p><p>循环：LSTM，Elman网络(局部记忆和局部循环)</p><p>其他：SOM(竞争学习，无监督，旅行商)，级联相关网络</p></blockquote><p>给你一个模型一段描述是不是合理。前馈反馈什么的对应什么模型，不同的网络解决了什么问题有什么结构</p><p>支持向量机要计算，例题中选一个。如果是对偶问题会给参考公式。不会考核</p><p>核函数的思想要知道，但是不会考计算</p><blockquote><p>思想：将低维样本映射到高维空间。而且映射只以内积形式出现，所以可以直接设计核函数</p><p>核函数：将两个d维样本映射为一个实数值的对称连续函数</p><p>多分类：成对，一对多</p></blockquote><p>C5</p><p>5.1不考</p><p>最小错误率和最小风险掌握啥掌握怎么算</p><p>5.2.2概率密度函数估计只需要了解</p><p>5.2.3了解一下</p><p>169页的总结要看</p><blockquote><p>一开始是概率模型，结合了先验知识和预测后，贝叶斯决策。如果难以获得先验概率和类条件概率，则用概率密度估计。</p><p>认为不同属性条件概率独立：朴素贝叶斯。至多允许和一个属性相关：半朴素贝叶斯。考虑更多属性的因果关系：贝叶斯网络。</p><p><strong>判别模型：线性模型，回归，神经网络，svm，决策树，boosting</strong></p><p><strong>生成模型：高斯模型，贝叶斯学习，直方图，Parzen窗，knn，隐马尔科夫，混合高斯</strong></p></blockquote><p><strong>决策树</strong>要考，和随机森林的关系要知道</p><p>​ 信息论什么的了解</p><p>概率图模型大题里面不考</p><p>C6</p><p>为什么要做集成学习？</p><blockquote><p>集成学习：通过构建并结合多个个体学习器(弱分类器)完成学习任务。流程：按照特定策略训练一组弱分类器，再通过某种策略将其结合形成强分类器</p></blockquote><blockquote><p>集成简单分类器获得更好效果。单一模型的偏差较大</p><p>学习器结合的三大好处：</p><ul><li>统计上：假设空间大，减少单一学习器误选导致的泛化性能不佳</li><li>计算上：降低进入局部极小点的风险</li><li>表示上：结合多个学习器扩大假设空间，进行更好的近似</li></ul></blockquote><p>三个方法，异同</p><blockquote><p>bagging：个体学习器之间不存在强依赖关系，可同时生成的并行化方法。自助采样，投票。可以用包外估计。可以降低方差。每个学习器都相对较强。</p><p>boosting：个体学习器之间存在强依赖关系，需要串行生成的序列化方法。每个学习器可以很弱。学习一个弱分类器、对其加权、根据错误率调整样本分布，循环，最终根据加权获得强分类器。可以降低偏差。</p><p>stacking：个体学习器的输出作为特征输入到次级分类器</p><p>要求：个体学习器要好而不同。准确性，多样性</p></blockquote><blockquote><p>bagging 和 boosting的差别：</p><p>样本选择：前者独立后者依赖于上一次结果</p><p>样本权重：前者均匀，后者调整</p><p>预测函数：前者权重相等，后者不等</p><p>并行：前者是，后者否</p><p>偏差方差：前者降低方差后者降低偏差</p></blockquote><p>弱分类器和强分类器的概念要掌握</p><blockquote><p>弱分类器：识别错误率小于1/2，就是说只比随机猜测强一点</p><p>强分类器：识别准确率很高而且能在多项式时间内完成的分类器</p></blockquote><p>随机森林要考，和决策树的区别</p><blockquote><p>随机森林：属于bagging。随机自助采样样本和属性，构造很多决策树基分类器。用多棵决策树来生成最后的输出结果</p></blockquote><p>adaboost了解思想</p><blockquote><p>误差率越小的分类器最终占比越大，误分类样本将在下一轮中起更大作用</p></blockquote><p>bagging和boosting的区别什么的总结</p><p>stacking的策略</p><blockquote><p>训练初级学习器，输出作为样例输入特征形成新的数据集，训练次级学习器</p></blockquote><p>C7</p><p><strong>K-means 或 层次化 计算</strong></p><p>19页无监督学习和监督学习对比</p><blockquote><p>监督学习：目标明确，需要带标签的训练数据，基于标签计算准确率</p><p>无监督学习：目标不明确，不需要带标签的训练数据，基于任务设计特定估计指标</p></blockquote><p>聚类的目标：分别朝两个方向最优化</p><blockquote><p>将数据集D中样本划分成几个通常不相交子集，簇内相似度高，簇间相似度低</p></blockquote><p>聚类的性能好坏的度量：掌握</p><blockquote><p>定义度量距离。簇内平均距离，簇内最远距离，簇间最近距离，簇间中心距离</p><p>DB：簇内方差和/每个簇质心之间的距离</p><p>Dunn：不同集群个例最小距离/集群内最大距离</p><p>外部指标：Jaccard，FM，Rand</p></blockquote><p>常用聚类方法要了解，列举，知道思想</p><p>高斯混合聚类不考计算题！</p><p>给点，推断一下</p><p>DBSCAN 概念 有缺点</p><blockquote><ul><li>优点：噪声、离群点鲁棒，形成非凸簇</li><li>缺点：多超参数敏感，不适合一簇内多种密度情况</li></ul></blockquote><p>降维：<strong>PCA，LDA</strong></p><p>维度灾难的定义，核心思想</p><blockquote><p>数据量一定的情况下，特征维度超过一定值的时候，分类器的效果反而下降</p><p>另一方面，增加特征维度，为了覆盖训练样本同样的特征范围、防止过拟合，所需训练样本数量呈指数增长</p><p>方式：特征选择，降维</p></blockquote><p>PCA要掌握</p><blockquote><p>PCA是一种线性方法。无监督数据降维，从高维投影到低维使得1.方差尽可能大 2. 均方误差尽可能小</p><p>为了这样，要选择最大的特征值</p><p>步骤：</p><ul><li>对数据集中心化并求出协方差矩阵S</li><li>求S的特征值 特征向量</li><li>特征值从大往小排，前M个对应的特征向量构成投影矩阵W</li><li>输出这个投影矩阵</li></ul><p>总结：</p><p>优点：基于特征向量，不需要参数训练，不用迭代，不会陷入局部最优</p><p>缺点：仅限二阶统计量，仅限线性投影</p></blockquote><p>LDA</p><blockquote><p>类内散度小，类间离散度大</p><p>核：将样本通过核函数映射到新的特征空间中，再对样本做fisher判别分析。</p></blockquote><p>流形学习：了解基本概念</p><blockquote><p>通过非线性投影将高维数据降低到低维非线性结构。</p><p>流形具有在局部与欧氏空间同胚的空间，能用欧式距离来计算。因此低维流形嵌入到了高维空间在局部上仍然有欧氏空间的性质</p><p>MDS：找到一个低维空间使得样本间的距离在高维和低维基本一致</p><p>优点：计算容易不用先验知识，保持数据距离</p><p>缺点：数据量大时计算慢，无法区分各个维度重要性</p></blockquote><p>127页总结要重点关注</p><p>C8</p><p>半监督学习</p><p>和无监督学习的区别。</p><blockquote><p>有标记无标记。有标记远少于无标记</p></blockquote><p>三大假设是什么。</p><blockquote><p>平滑，聚类，流形</p></blockquote><p>几个学习方法了解一下就好。重点看86页总结</p><blockquote><p>半监督学习背景：极少标注</p><p>归纳式直推式对比</p><p>三大假设</p><p>学习方法：自训练，生成，svm，图，分歧(多个学习器，利用数据不同视图)</p></blockquote><p>C9</p><p>重点。</p><p>LBP，SIFT 特点，优缺点</p><blockquote><ul><li>LBP</li></ul><p>利用相邻像素提取稳定特征 可以进行8位编码</p><p>进阶：扩展到任意邻域，旋转不变性(循环选择起始点，选编码最小的)，多尺度(整块的平均，当作一个大像素)，等价模式(跳变次数来编码)</p><p>总结：光照不变形——仅考虑相对强度；尺度、旋转不变性——多尺度、循环编码</p><p>优点：计算速度快效率高 缺点：太简单，鲁棒性差</p><ul><li>SIFT：手工设计的关键点检测特征</li></ul><p>检测图像的关键点，用以表征和匹配成对图像</p><p>要点：显著点、位置无关、光照噪声鲁棒</p><ol type="1"><li>检测关键点位置：提取尺度金字塔，通过极大极小值检测比较DoG值(高斯核卷积出来的)判断是否是局部极大/极小值</li><li>精细化关键点位置：为去除低对比度关键点，二阶泰勒展开，更新。为去除边上关键点，用hessian矩阵找边缘点</li><li>计算关键点方向：计算点的梯度，做直方图统计，得到区域主方向。</li><li>计算关键点描述子：特征点附近2x2元胞窗口，统计每个的梯度直方图，旋转到主关键点方向。归一处理得到结果</li></ol><p>优点：鲁棒，旋转、光照、尺度不变性</p><p>缺点：复杂，调参极多，难以穷尽各种变换的差异</p></blockquote><blockquote><p>我们希望的目标：鲁棒，变换不变，区分性强，计算复杂度低</p></blockquote><p>特征选择的重要性：掌握。</p><p>对于特征好坏的评价标准要了解</p><p>搜索策略了解一下</p><p>综合方法要考，概念和思想</p><blockquote><p>在上面</p></blockquote><p>C10</p><p>10.2网络优化的方法要掌握</p>]]></content>
    
    
    
    <tags>
      
      <tag>notes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[Record] 在Ubuntu22.04上配置qt5.12.3</title>
    <link href="/2023/05/25/%5BRecord%5D%20%E5%9C%A8Ubuntu22.04%E4%B8%8A%E9%85%8D%E7%BD%AEqt5.12.3/"/>
    <url>/2023/05/25/%5BRecord%5D%20%E5%9C%A8Ubuntu22.04%E4%B8%8A%E9%85%8D%E7%BD%AEqt5.12.3/</url>
    
    <content type="html"><![CDATA[<h1 id="record-在ubuntu22.04上配置qt5.13.2">[Record]在Ubuntu22.04上配置qt5.13.2</h1><p>因为要用qt制作ui界面完成数据结构大作业，因此在这里记录一下环境配置过程</p><p>环境：</p><ul><li>Linux(Ubuntu 22.04 LTS)</li></ul><h2 id="安装步骤">安装步骤</h2><ul><li>首先安装必要的工具</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo apt-get install build-essential libgl1-mesa-dev<br></code></pre></td></tr></table></figure><ul><li><p>进入<ahref="https://download.qt.io/archive/qt/5.13/5.13.2/">此链接</a></p></li><li><p>安装<ahref="https://download.qt.io/archive/qt/5.13/5.13.2/qt-opensource-linux-x64-5.13.2.run">qt-opensource-linux-x64-5.13.2.run</a></p></li><li><p>进入安装的路径，为安装程序赋予权限并运行</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">chmod +x qt-opensource-linux-x64-5.13.2.run <br>sudo ./qt-opensource-linux-x64-5.13.2.run<br></code></pre></td></tr></table></figure><ul><li>一直点下一步，选择要安装的东西</li><li><figure><img src="/typora-user-images/image-20230525141446695.png"alt="image-20230525141446695" /><figcaption aria-hidden="true">image-20230525141446695</figcaption></figure></li><li>设置环境变量</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo vim /etc/profile<br></code></pre></td></tr></table></figure><p>​ 增加以下内容：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-string">&quot;/opt/Qt5.13.2/Tools/QtCreator/bin:<span class="hljs-variable">$PATH</span>&quot;</span><br><span class="hljs-built_in">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-string">&quot;/opt/Qt5.13.2/5.13.2/gcc_64/bin:<span class="hljs-variable">$PATH</span>&quot;</span><br></code></pre></td></tr></table></figure><ul><li>wq保存退出，修改环境变量：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">source /etc/profile<br></code></pre></td></tr></table></figure><p>安装成功。其中，QtCreator默认在以下目录中</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/opt/</span>Qt5.<span class="hljs-number">13.2</span><span class="hljs-regexp">/Tools/</span>QtCreator/bin<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>record</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[Human Parsing] SCHP</title>
    <link href="/2023/05/05/%5BHuman%20Parsing%5D%20SCHP/"/>
    <url>/2023/05/05/%5BHuman%20Parsing%5D%20SCHP/</url>
    
    <content type="html"><![CDATA[<h2 id="schp">SCHP</h2><blockquote><p>出发点：ground-truth中可能存在错误，影响训练。使用迭代优化的过程，不断同时提升labels和models</p></blockquote><figure><img src="\typora-user-images\image-20230505100544843.png"alt="image-20230505100544843" /><figcaption aria-hidden="true">image-20230505100544843</figcaption></figure><p>方法详细介绍：</p><p>开始给一个训练集，有图片<spanclass="math inline">\(\mathcal{X}\)</span>和不怎么精确的掩码<spanclass="math inline">\(\mathcal{Y}\)</span></p><p>目标是训练一个parser，通过<strong>模型聚合</strong>(modelaggregation)、<strong>标签提升</strong>(labelrefinement)、<strong>模型交互</strong>(interaction)。首先根据不怎么精确的标注对模型进行warm-upinitialization，然后对模型和掩码标注进行交替优化。</p><p>循环地聚合学习到的模型，以移动平均的方式细化掩码注释<spanclass="math inline">\(\mathcal{Y}\)</span>。具体来说，在某一个循环中，首先让模型从上一个被优化过的掩码中进行学习，然后通过参数化移动平均的操作把当前周期中学习到的模型权重和上一周期的模型权重进行聚合，然后用聚合后的模型来推断掩码，再通过像素移动平均操作来校正标注中的噪声，提升后的掩码标注将作为下一轮的ground-truth。</p><p>对每一个部分进行详细介绍：</p><ul><li><p>Warm-up Initialization</p><p>挺重要的</p></li><li><p>Online Model Aggregation</p><p>把不同周期得到的模型记为<spanclass="math inline">\(\{\theta_i\}_{i=1}^N\)</span> 设<spanclass="math inline">\(\theta\)</span>是在第<spanclass="math inline">\(i\)</span>轮学到的模型，上一轮使用的模型是<spanclass="math inline">\(\theta_{i-1}\)</span>，那么我们可以用移动平均的方式得到第<spanclass="math inline">\(i\)</span>轮使用的模型： <spanclass="math display">\[\theta_i=\frac{i}{i+1}\theta_{i-1}+\frac{1}{i+1}\theta\]</span>然而实验发现直接这样可能让模型表现更差，那是因为BN层在模型聚合之后参数不准确了，为了弥补这一点</p><blockquote><p>we forward all the training samples for one epoch to exactlyre-estimate the BatchNorm statistics in all BN layers as follows:</p></blockquote><p><span class="math display">\[m = (t-1)/t \\\mu_t = m\mu_{t-1}+(1-m)E[x_B] \\\sigma_t^2 = m \sigma_{t-1}^2+(1-m)Var[x_B]\]</span></p></li></ul><p>​ 其中<span class="math inline">\(t\)</span>是循环的次数，<spanclass="math inline">\(x_B\)</span>是BN层的输入特征。</p><p>​ 看起来很复杂，实际上基本没有增加复杂度。</p><ul><li><p>Online Label Refinement</p><p>把不同训练周期中得到的预测标签集表示为<spanclass="math inline">\(\{\mathcal{Y_i}\}_{i=1}^n\)</span> 跟先前类似：<span class="math display">\[\mathcal{Y_i} = \frac{i}{i+1}\mathcal{Y_{i-1}}+\frac{1}{i+1}\mathcal{Y}\]</span></p></li><li><p>Cyclical Training Strategy</p><p>为了满足在结束的时候有较小的学习率，在每一周期开始时有较大的学习率来走出局部极值，使用循环重启的余弦退火学习速率调度算法，设<spanclass="math inline">\(T\)</span>是每一个循环有的epochs数目，<spanclass="math inline">\(\eta_{min},\eta_{max}\)</span>表示结束、开始时的学习率</p><p>则： <span class="math display">\[\eta =\eta_{min}+\frac{1}{2}(\eta_{max}-\eta_{min})(1+\cos(\frac{T_{cur}}{T}\pi))\]</span></p></li></ul><p>伪代码：</p><p><img src="\typora-user-images\image-20230505154408196.png" alt="image-20230505154408196" style="zoom: 50%;" /></p>]]></content>
    
    
    
    <tags>
      
      <tag>notes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[Human Parsing] Papers</title>
    <link href="/2023/05/04/%E6%95%B4%E7%90%86/"/>
    <url>/2023/05/04/%E6%95%B4%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="human-parsing">Human Parsing</h1><p>一些主要矛盾：</p><ul><li>scale不同</li><li>降采样导致像素损失</li></ul><p>常见的方法</p><ul><li>联合姿态估计（关节什么的）s</li></ul><p>想法：</p><ul><li>有文章说在某一相似背景下训练出来的模型可能遇到新的背景就会识别的不好。假设ground-truth标的很对，可不可以就直接根据ground-truth先把人的轮廓扣出来，然后只对这一片地方进行特征提取，而把背景忽视或者直接用一些噪音替代，然后再其他操作，有没有可以减小背景的影响</li></ul><h2 id="atr">ATR</h2><p>将人体图像分解为语义时尚/身体区域。作者将其表述为一种主动模板回归（ATR）问题，其中每个时尚/身体项的归一化掩码表示为学习的掩码模板的线性组合，然后通过包括每个语义区域的位置、比例和可见性的主动形状参数变形为更精确的掩码。掩码模板系数和主动形状参数共同可以生成人体解析结果，并因此称为人体解析的结构输出。深度卷积神经网络（CNN）用于构建输入人体图像和人体解析结构输出之间的端到端关系。更具体地说，结构输出由两个单独的网络预测。第一个CNN网络带有最大池化，并设计用于预测每个标签掩码的模板系数，而第二个网络没有最大池化以保留对标签掩码位置的敏感性并准确预测主动形状参数。对于新图像，两个网络的结构输出被融合以生成每个像素的每个标签的概率，最后使用超像素平滑来细化人体解析结果。在大型数据集上进行全面评估，充分证明了ATR框架相对于其他人体解析技术的显着优越性。特别是，我们的ATR框架通过F1分数达到64.38％，显着高于基于最先进算法[28]的44.76％</p><figure><img src="\typora-user-images\image-20230403154837858.png"alt="image-20230403154837858" /><figcaption aria-hidden="true">image-20230403154837858</figcaption></figure><h2 id="m-cnn">M-CNN</h2><p>Matching-CNN Meets KNN: Quasi-Parametric HumanParsing是一篇关于人体解析的论文，由Si Liu等人在2015年发表于IEEECVPR上。该论文提出了一种基于KNN的准参数人体解析框架，该框架利用MatchingConvolutional Neural Network(M-CNN)来预测测试图像中与KNN图像中特定语义区域最佳匹配区域的匹配置信度和位移。该框架的目标是将传统的非参数方法和参数方法相结合，既能从注释数据中获得监督，又能灵活地使用新注释的数据。该论文在大型数据集上进行了全面评估，并证明了准参数模型相对于现有技术的显著性能提升</p><figure><img src="\typora-user-images\image-20230403155021588.png"alt="image-20230403155021588" /><figcaption aria-hidden="true">image-20230403155021588</figcaption></figure><h2 id="co-cnn">Co-CNN</h2><p>该论文提出了一种新的Contextualized Convolutional Neural Network(Co-CNN)架构，该架构将跨层上下文、全局图像级上下文、超像素内部上下文和超像素交叉邻域上下文集成到一个统一的网络中，用于解决人体解析任务。Co-CNN能够在端到端的方式下对输入的人体图像进行像素级别的分类，并取得了优秀的性能</p><figure><img src="\typora-user-images\image-20230403155315028.png"alt="image-20230403155315028" /><figcaption aria-hidden="true">image-20230403155315028</figcaption></figure><h2 id="schp">SCHP</h2><p>该论文提出了一种名为Self-Correction for Human Parsing(SCHP)的噪声容忍方法，用于逐步提高监督标签和学习模型的可靠性和准确性。SCHP是一种模型无关的方法，可以应用于任何人体解析模型以进一步提高其性能</p><figure><img src="\typora-user-images\image-20230403155737493.png"alt="image-20230403155737493" /><figcaption aria-hidden="true">image-20230403155737493</figcaption></figure><h2 id="slrs">SLRS</h2><p>该论文提出了一种名为Self-Learning with Rectification(SLR)的方法，用于解决人体解析任务中样本不足的问题。SLR方法通过自学习策略生成伪标签来重新训练模型，但直接使用噪声伪标签会导致误差放大和积累。因此，SLR方法引入了一种循环学习调度程序来推断更可靠的伪标签，并设计了一种去噪学习和半监督学习相结合的策略，以进一步提高模型性能</p><figure><img src="\typora-user-images\image-20230403160712629.png"alt="image-20230403160712629" /><figcaption aria-hidden="true">image-20230403160712629</figcaption></figure><figure><img src="\typora-user-images\image-20230403160744026.png"alt="image-20230403160744026" /><figcaption aria-hidden="true">image-20230403160744026</figcaption></figure><h2 id="pcnet">PCNet</h2><p>PCNet方法主要由三个模块组成，包括部分类别模块、关系聚合模块和关系分散模块。其中，部分类别模块用于生成部分类别特定的特征图，关系聚合模块用于聚合全局和局部上下文信息，而关系分散模块则用于将全局和局部上下文信息分散到各个部位<imgsrc="\typora-user-images\image-20230403160954024.png"alt="image-20230403160954024" /></p><h2 id="aog">AOG</h2><p>该论文提出了一种名为Attribute And-Or Grammar(A-AOG)的模型，用于在带有属性的解析图中联合推断人体姿势和人体属性。与当前文献中其他流行方法训练单独的姿势和个体属性分类器不同，该模型将属性增强到分层表示中的节点中，从而实现了对人体姿势、部位和属性的联合解析</p><ul><li>Phrase structure grammar: 表示人体从整体到部分的层次分解</li><li>Dependency grammar: 通过身体姿态的运动图来建模几何关节</li><li>Attribute grammar: 考虑不同部分兼容性关系，使得他们遵循一致风格</li></ul><p><img src="\typora-user-images\image-20230327191937546.png" alt="image-20230327191937546" style="zoom:67%;" /></p><h2 id="pgn">PGN</h2><p>不要检测部分了</p><p>两个部分：1)语义部分分割，将每个像素分配为人类的一个部分（如脸部、手臂）；2)实例意识的边缘检测，将语义部分分成不同的人物实例</p><p>之后两者互相完善</p><figure><img src="\typora-user-images\image-20230403180113620.png"alt="image-20230403180113620" /><figcaption aria-hidden="true">image-20230403180113620</figcaption></figure><h2 id="mula">MuLA</h2><p>这篇论文提出了一种新的 Mutual Learning to Adapt 模型 (MuLA)用于联合人体解析和姿态估计。它有效地利用了两个任务之间的相互利益，同时提高了它们的性能。与现有的后处理或基于多任务学习的方法不同，MuLA通过反复利用其并行任务的指导信息来预测动态任务特定模型参数。因此，MuLA可以快速适应解析和姿态模型，通过将其对应部分的信息纳入更强大的表示中，提供更强大、更健壮和更准确的结果。MuLA是使用卷积神经网络实现的，并且是端到端可训练的。在基准 LIP 和扩展PASCAL-Person-Part 上进行了全面实验，证明了所提出的 MuLA模型具有优越的性能，优于现有的基准模型</p><figure><img src="\typora-user-images\image-20230403181119605.png"alt="image-20230403181119605" /><figcaption aria-hidden="true">image-20230403181119605</figcaption></figure><h2 id="hazn">HAZN</h2><p>适应不同局部规模的物体或部分</p><p>HAZN是两个“Auto-Zoom Nets”的组合，使用FCN完成两个任务：</p><ul><li>预测目标的位置和规模</li><li>为预测的目标区域估计部分分数</li><li><figure><img src="\typora-user-images\image-20230404144905334.png"alt="image-20230404144905334" /><figcaption aria-hidden="true">image-20230404144905334</figcaption></figure></li></ul><h2 id="lg-lstm">LG-LSTM</h2><figure><img src="\typora-user-images\image-20230404145934436.png"alt="image-20230404145934436" /><figcaption aria-hidden="true">image-20230404145934436</figcaption></figure><h2 id="mh-parser">MH-Parser</h2><p>MH-Parser使用新的Graph-GAN模型以自下而上的方式同时生成全局解析映射和人物实例掩码</p><figure><img src="\typora-user-images\image-20230404152445268.png"alt="image-20230404152445268" /><figcaption aria-hidden="true">image-20230404152445268</figcaption></figure><blockquote><p>不是很懂</p></blockquote><h2 id="ss-jppnetlipssl">SS-JPPNet(LIP)（SSL）</h2><blockquote><p>我们将关节结构损失作为分割损失的权重，分割损失成为我们的结构敏感损失。</p></blockquote><p><img src="\typora-user-images\image-20230405150631885.png"alt="image-20230405150631885" /> <span class="math display">\[L_{Joint} = \frac{1}{2N}\sum_{i=1}^n||c_i^p-c_i^{gt}||_2^2 \\L_{Structure} = L_{Joint}\cdot L_{Parsing}\]</span></p><blockquote><p>好奇怪，为什么损失函数是直接乘起来？</p></blockquote><h2 id="refinenet">RefineNet</h2><p>通用多路径优化网络，明确地利用下采样过程中的所有可用信息，使用远程残留连接实现高分辨率预测</p><p>引入了链式残差池化</p><p>比较图：</p><figure><img src="\typora-user-images\image-20230405152632909.png"alt="image-20230405152632909" /><figcaption aria-hidden="true">image-20230405152632909</figcaption></figure><p>RefineNet:</p><figure><img src="\typora-user-images\image-20230405152658463.png"alt="image-20230405152658463" /><figcaption aria-hidden="true">image-20230405152658463</figcaption></figure><h2 id="holistic-instance-level-human-parsing">Holistic, Instance-levelHuman Parsing</h2><p>关于多人的</p><figure><img src="\typora-user-images\image-20230405154613794.png"alt="image-20230405154613794" /><figcaption aria-hidden="true">image-20230405154613794</figcaption></figure><h2id="cross-domain-human-parsing-via-adversarial-feature-and-label-adaptation">Cross-DomainHuman Parsing via Adversarial Feature and Label Adaptation</h2><blockquote><p>我们提出了一种新的、高效的跨领域人类解析模型，以弥合跨领域在视觉外观和环境条件方面的差异，充分利用跨领域的共性。</p><p>To this end, we propose a novel and efficient cross-domain humanparsing model to bridge the cross-domain differences in terms of visualappearance and environment conditions and fully exploit commonalitiesacross domains.</p></blockquote><blockquote><p>什么是cross-domain? A:似乎就是应用于情况不大一样的场景比如背景很不一样什么的</p></blockquote><p>A discriminative feature adversarial network:一个特征判别对抗网络，对特征补偿进行监督，减小了两个域特征分布的差异</p><p>A structured label adversarial network:一个结构化标签对抗网络，引导目标域的分割结果遵循域间共享的结构化标签的高阶关系(???说人话)</p><figure><img src="\typora-user-images\image-20230405160643047.png"alt="image-20230405160643047" /><figcaption aria-hidden="true">image-20230405160643047</figcaption></figure><blockquote><p>呃呃，这篇文章完全不懂</p></blockquote><h2 id="jppnet">JPPNet</h2><figure><img src="\typora-user-images\image-20230405161909105.png"alt="image-20230405161909105" /><figcaption aria-hidden="true">image-20230405161909105</figcaption></figure><h2 id="mman">MMAN</h2><p>宏观-微观对抗网络</p><p>其中一个识别器MacroD作用于低分辨率的标签地图，并惩罚语义不一致，例如，错位的身体部位。另一个鉴别器MicroD侧重于高分辨率标签地图的多个补丁，以解决局部不一致性，如模糊和空洞</p><p>开源 https://github.com/RoyalVane/MMAN</p><figure><img src="\typora-user-images\image-20230405162714939.png"alt="image-20230405162714939" /><figcaption aria-hidden="true">image-20230405162714939</figcaption></figure><h2 id="nanmhp-dataset">NAN(MHP dataset)</h2><blockquote><p>NAN由三个类似生成对抗网络(Generative Adversarial Network,GAN)的子网组成，分别执行语义显著性预测、实例不可知解析和实例感知聚类。</p><p>NAN consists of three Generative Adversarial Network (GAN)-likesub-nets, respectively performing semantic saliency prediction,instance-agnostic parsing and instance-aware clustering.</p></blockquote><p>这些子网形成了一个嵌套的结构，并经过精心设计，以端到端方式共同学习</p><figure><img src="\typora-user-images\image-20230405164137662.png"alt="image-20230405164137662" /><figcaption aria-hidden="true">image-20230405164137662</figcaption></figure><p>(???)</p><h2 id="ce2p">CE2P</h2><p>在本文中，我们确定了几个有用的属性，包括特征解析、全局上下文信息和边缘细节，并进行严格的分析，以揭示如何利用它们来帮助人工解析任务</p><figure><img src="\typora-user-images\image-20230405164545542.png"alt="image-20230405164545542" /><figcaption aria-hidden="true">image-20230405164545542</figcaption></figure><blockquote><p>我对边缘检测还不是很了解，不知道为什么可以这样</p></blockquote><h2 id="spreid">SPReID</h2><figure><img src="\typora-user-images\image-20230405170918658.png"alt="image-20230405170918658" /><figcaption aria-hidden="true">image-20230405170918658</figcaption></figure><h2 id="jointmpe">JointMPE</h2><p>共同解决姿势估计和语义部分分割</p><p>训练了两个FCN：Pose FCN and PartFCN提供对姿势关节和语义的初始估计。然后用fully-connected conditionalrandomfield(FCRF)来融合它们，这里面一个全新的语义-关节平滑操作被实施来提升两者的consistency。</p><p>为了refine part segments，被上面refine过后的东西被第二个partFCN融合。为了减小FCRF的complexity，引入了人体检测框加速。</p><figure><img src="\typora-user-images\image-20230408155051444.png"alt="image-20230408155051444" /><figcaption aria-hidden="true">image-20230408155051444</figcaption></figure><h2 id="corrpm">CorrPM</h2><p>人体的语义边缘和关键点位置如何共同提升human parsing</p><p>相比特征拼接，揭示相关性似乎更好</p><p>提出CorrPM来揭示这种相关性，使用heterogeneous non-local block来从边缘、姿势和parsing的特征映射中揭示空间相关性</p><p><img src="\typora-user-images\image-20230408160751778.png" alt="image-20230408160751778" style="zoom:67%;" /></p><p><img src="\typora-user-images\image-20230408160816684.png" alt="image-20230408160816684" style="zoom:67%;" /></p><h2 id="cdcl">CDCL</h2><blockquote><p>我们提出的方法利用真实数据的丰富和真实变化，以及合成数据的易于获得的标签，在没有任何人类标注标签的情况下，学习对真实图像的多人部分分割。</p></blockquote><figure><img src="\typora-user-images\image-20230408163811916.png"alt="image-20230408163811916" /><figcaption aria-hidden="true">image-20230408163811916</figcaption></figure><p><img src="\typora-user-images\image-20230408163855577.png" alt="image-20230408163855577" style="zoom:67%;" /></p><h2 id="dpc">DPC</h2><p>用meta-learning(???)</p><p>跟deeplab有关系</p><h2 id="snt">SNT</h2><p>一种树结构，multiple semantic sub-regions in a hierarchical way</p><p>然后用semnatic aggregation module 来 combine multiple hierarchicalfeatures</p><p><img src="\typora-user-images\image-20230410103703390.png" alt="image-20230410103703390" style="zoom:67%;" /></p><figure><img src="\typora-user-images\image-20230410104024948.png"alt="image-20230410104024948" /><figcaption aria-hidden="true">image-20230410104024948</figcaption></figure><h2 id="nppnet">NPPNet</h2><p>结合human parsing and pose estimation</p><figure><img src="\typora-user-images\image-20230410104825965.png"alt="image-20230410104825965" /><figcaption aria-hidden="true">image-20230410104825965</figcaption></figure><p><img src="\typora-user-images\image-20230410105025348.png" alt="image-20230410105025348" style="zoom: 67%;" /></p><h2 id="cnif-prhp">CNIF-PRHP</h2><p>分析三种相关的过程：</p><ul><li>direct inference</li><li>bottom-up inference</li><li>top-down inference</li></ul><p>assimilating generic message-passing networks with their edge-typed,convolutional counterparts</p><p><img src="\typora-user-images\image-20230410110931174.png" alt="image-20230410110931174" style="zoom:80%;" /></p><p><img src="\typora-user-images\image-20230410111019453.png" alt="image-20230410111019453" style="zoom:50%;" /></p><p><img src="\typora-user-images\image-20230410111054943.png" alt="image-20230410111054943" style="zoom:50%;" /></p><figure><img src="\typora-user-images\image-20230410111124193.png"alt="image-20230410111124193" /><figcaption aria-hidden="true">image-20230410111124193</figcaption></figure><h2 id="bgnet">BGNet</h2><p>BGNet exploits the inherent hierarchical structure of a human bodyand the relationship of different human parts by means of grammar rulesin both cascaded and paralleled manner.</p><p>We also design a Part-aware Convolutional Recurrent Neural Network(PCRNN) to pass messages which are generated by grammar rules.</p><figure><img src="\typora-user-images\image-20230410112131772.png"alt="image-20230410112131772" /><figcaption aria-hidden="true">image-20230410112131772</figcaption></figure><h2 id="gwnet">GWNet</h2><p>在前面那个上加了一个Wavelet Prediction Module</p><figure><img src="\typora-user-images\image-20230410113546364.png"alt="image-20230410113546364" /><figcaption aria-hidden="true">image-20230410113546364</figcaption></figure><h2 id="wshp">WSHP</h2><p>pose 结构</p><p>transfer</p><figure><img src="\typora-user-images\image-20230411155830125.png"alt="image-20230411155830125" /><figcaption aria-hidden="true">image-20230411155830125</figcaption></figure><p><img src="\typora-user-images\image-20230411160205740.png" alt="image-20230411160205740" style="zoom:50%;" /></p><h2 id="braidnet">BraidNet</h2>]]></content>
    
    
    
    <tags>
      
      <tag>notes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[Record] 不怎么成功配置的docker</title>
    <link href="/2023/05/04/%5BRecord%5D%20%E4%BB%A4%E4%BA%BA%E6%8A%93%E7%8B%82%E7%9A%84docker/"/>
    <url>/2023/05/04/%5BRecord%5D%20%E4%BB%A4%E4%BA%BA%E6%8A%93%E7%8B%82%E7%9A%84docker/</url>
    
    <content type="html"><![CDATA[<p>搞了老半天docker，仍然无法把dockerdesktop和root下的docker联系起来。好在在终端的root权限下终于可以使用带gpu的镜像了。稍微记录一下，防止摆烂一段时间后就忘记了。</p><p>安装docker：</p><p>跟着官网文档<ahref="https://docs.docker.com/engine/install/ubuntu/">这里</a>配。</p><p>为了使用gpu，还要跟着<ahref="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#install-guide">这个链接</a>配置ContainerToolkit</p><p>因为我的整个配置过程非常混乱，不确定是否这两个就够了，等我哪天重装系统了再来看看有没有要加的（叹气</p><p>然后就是使用</p><p>首先拉一个带gpu的镜像下来，比如</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker pull nvidia/cuda:11.6.2-base-ubuntu20.04<br></code></pre></td></tr></table></figure><p>然后创造容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker run -it --gpus all --name myubuntu nvidia/cuda:11.6.2-base-ubuntu20.04<br></code></pre></td></tr></table></figure><p>参数：</p><ul><li><strong>-i</strong>: 交互式操作。</li><li><strong>-t</strong>: 终端。</li></ul><p>如果要退出，直接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">exit<br></code></pre></td></tr></table></figure><p>如果想让容器后台运行，用-d</p><p>想要开始、停止、重启一个容器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker start &lt;容器&gt;<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker stop &lt;容器&gt;<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker restart &lt;容器&gt;<br></code></pre></td></tr></table></figure><p>进入、离开（不关闭）容器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker attach &lt;容器&gt;<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker exec ...<br></code></pre></td></tr></table></figure><p>更多操作可以参考<ahref="https://www.runoob.com/docker/docker-container-usage.html">菜鸟教程</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>record</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[Information Theory] 复习时的一些心得理解</title>
    <link href="/2023/04/25/%5BInformation%20Theory%5D%20%E5%A4%8D%E4%B9%A0%E6%97%B6%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BF%83%E5%BE%97/"/>
    <url>/2023/04/25/%5BInformation%20Theory%5D%20%E5%A4%8D%E4%B9%A0%E6%97%B6%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BF%83%E5%BE%97/</url>
    
    <content type="html"><![CDATA[<h1 id="information-theory-复习时的一些心得">[Information Theory]复习时的一些心得</h1><blockquote><p>注：此篇笔记对应的教材是《信息论基础》，对应英文版是《Elements ofInformationTheory》。此篇笔记并不详细阐述内容，而是跳跃性地讲述一些我可能不怎么正确的感受。</p></blockquote><h2 id="chapter-2">Chapter 2</h2><h3 id="一些定义">一些定义</h3><h4 id="信息熵联合熵条件熵">信息熵、联合熵、条件熵</h4><p>对信息熵的定义的理解：</p><ul><li><p>为什么要用log？</p><ul><li><p>对于某个概率为p的东西，我们想找一个东西描述它的信息，要满足以下条件：</p></li><li><blockquote><p>非负</p><p>如果概率是1，信息量为0</p><p>函数连续(这样比较好)</p><p>如果两件事独立，一起发生的信息是单独发生的信息之和</p></blockquote><p>这样就可以得出<span class="math inline">\(I(p) \proptolog\frac{1}{p}\)</span></p></li></ul></li><li><p>知道这个之后，<spanclass="math inline">\(H\)</span>的定义是怎么出来的？</p><ul><li>刚才讨论的是对于<span class="math inline">\(p\)</span>的，对于<spanclass="math inline">\(H\)</span>，就是看一个随机变量，它具有一定的概率分布，我们要求它的信息。那么显然就可以理解为它对于刚在给定概率下信息的期望(相当于求个和或者说积个分，把每个<spanclass="math inline">\(p\)</span>下贡献的信息累加起来)</li><li>因此我们有了</li></ul><p><span class="math display">\[H(X) = \mathbb{E}_plog\frac{1}{p(x)}\]</span></p><hr /></li></ul><p>理解了上面所说，联合熵、条件熵也就不难理解了，不过是看联合概率、条件概率罢了</p><hr /><h4 id="信息熵的链式法则">信息熵的链式法则</h4><p><span class="math display">\[H(X_1, X_2, \cdots ,X_n) = \sum_{i=1}^nH(X_i|X_{i-1}, \cdots, X_1)\]</span></p><blockquote><p>顺便，互信息的链式法则：<span class="math inline">\(I(X_1, \cdots ,X_n;Y) = \sum_{i=1}^n I(X_i;Y|X_{i-1}, \cdots, X_1)\)</span></p></blockquote><p>对于Chain rule的理解：</p><ul><li>理解一下<span class="math inline">\(H(X,Y) = H(X) +H(Y|X)\)</span>即可</li><li>想看看<span class="math inline">\(X,Y\)</span>的信息。我们先知道了<spanclass="math inline">\(X\)</span>的信息，此时由于<spanclass="math inline">\(X\)</span>、<spanclass="math inline">\(Y\)</span>之间可能存在一定的相关性，我们对<spanclass="math inline">\(X\)</span>已经有了一点点的了解，剩下的关于<spanclass="math inline">\(X, Y\)</span>的信息即为<spanclass="math inline">\(H(X|Y)\)</span></li><li>跟同学交流时，他说有点像是线性代数中的Cramer法则？就好像分解成一些正交空间一样。有点感觉。</li></ul><p>根据上面的第二条，似乎互信息用<span class="math inline">\(I(X,Y) =H(X) - H(X|Y)\)</span>来表示是很显然的一件事了</p><hr /><h4 id="kl散度相对熵">KL散度（相对熵）</h4><p>首先我们看看公式 <span class="math display">\[D(p||q) = \sum_{x\in\mathcal{X}}p(x)log\frac{p(x)}{q(x)}  =\mathbb{E}_plog\frac{p(X)}{q(X)}\]</span> 为了延续上面的理解方式，可以写成 <span class="math display">\[D(p||q) = \mathbb{E}_p(log\frac{1}{q(X)}-log\frac{1}{p(X)})\]</span>E中的第一项似乎是在一个给定值下q的信息，第二项是p的信息，好像就是信息差。而这件事是对p的期望，似乎是从p的角度看待这件事的一般，从p的角度看两者的差距。</p><p>在网上找到了一种我觉得很妙的解释：</p><blockquote><p>假设P(X=0)=0.8，P(X=1)=0.2，那么我们可以用0来编码消息0，用10来编码消息1，这样平均每个消息的编码长度为：L(P) = 0.8 * 1 + 0.2 * 2 = 1.2 bits 这也等于X的熵H(P)。但是如果我们不知道X的真实概率分布P(X)，而只知道一个近似的概率分布Q(X)，那么我们可能会用不同的编码方案来传输消息。例如，假设Q(X=0)=0.5，Q(X=1)=0.5，那么我们可能会用01来编码消息0，用11来编码消息1，这样平均每个消息的编码长度为：L(Q) = 0.8 * 2 + 0.2 * 2 = 2 bits 这也等于X的交叉熵H(P,Q)。可以看出，由于我们使用了一个不准确的概率分布Q(X)来编码消息，导致了平均每个消息的编码长度增加了0.8bits，这就是用Q(X)来近似P(X)所造成的信息损失。这个信息损失就是KL散度D(P||Q)：D(P||Q) = L(Q) - L(P) = H(P,Q) - H(P) = 0.8 bits<strong>也就是说，KL散度衡量了如果我们使用Q(X)来代替P(X)，那么每个消息需要额外传输多少比特数。</strong></p></blockquote><p>后来我们知道，KL散度是肯定大于0的，但是显然每个点两者信息的大小关系有可能有大有小，而最后大于0，反映了一种最优性和必然性，回头再看看。</p><blockquote><p>学完了后面，回头看这个，感觉好显然了……</p></blockquote><hr /><h4 id="互信息和dl-散度的关系">互信息和DL-散度的关系</h4><p><span class="math display">\[I(X,Y) = D[p(X,Y)||p(X)p(Y)]\]</span></p><hr /><h4 id="条件互信息">条件互信息</h4><p><span class="math display">\[I(X;Y|Z) = H(X| Z) - H(X|Y, Z)\]</span></p><p>这可以从信息增益的角度理解，也可以写成<spanclass="math inline">\(\mathbb{E}\)</span>的形式从KL-D的角度理解</p><hr /><h4 id="互信息的链式法则">互信息的链式法则</h4><p><span class="math display">\[I(X_1,X_2\cdots X_n;Y) = \sum_{i=1}^{n}I(X_i;Y|X_{i-1},\cdots,X_1)\]</span></p><p>理解上，跟信息熵的链式法则一样，只不过之前的信息现在变成了和Y之间的互信息</p><hr /><h4 id="条件相对熵">条件相对熵</h4><p><span class="math display">\[D[p(y|x) || q(y|x)] = \mathbb{E}_{x,y}log\frac{p(Y|X)}{q(Y|X)}\]</span></p><hr /><h4 id="相对熵的链式法则">相对熵的链式法则</h4><p><span class="math display">\[D[p(x, y)||q(x,y)] = D[p(x)||q(x)] + D[p(y|x)||q(y|x)]\]</span></p><p>这个要怎么很直观地理解？</p><blockquote><p>相对熵可以看作是两个分布之间的差异或不相似度的度量，而链式法则可以看作是一种将这种差异分解为局部差异和条件差异的方法。</p></blockquote><h3 id="一些推论">一些推论</h3><p>首先，教材引入了Jensen不等式，这在几何上非常容易理解，不赘述</p><p>结论： <span class="math display">\[f(x): convex\;\Rightarrow\;\mathbb{E}f(X) \ge f(\mathbb{E}X)\]</span> 以此证明 <span class="math display">\[D[p||q]\ge 0\]</span></p><p>数学上很好证，讲讲理解 <span class="math display">\[H(X) = \sum_xp(x)log\frac{1}{p(x)}\]</span></p><p><span class="math display">\[D[p(x)||q(x)] = \sum_x p(x)[log\frac{1}{q(x)}-log\frac{1}{p(x)}]\]</span></p><p>我们盯着这两个式子，<spanclass="math inline">\(log\frac{1}{p(x)}\)</span>随着<spanclass="math inline">\(p(x)\)</span>的增大而减小，是否就是说，在概率大的时候我们要挑选一个小一点的东西对它进行表示，概率小的时候用大一点的东西对它进行表示，那么最后我们总共的消耗比较小（编码的角度）</p><p>而对于KL散度，我们已经知道了<spanclass="math inline">\(p(x)\)</span>，那么对于每个概率点，用<spanclass="math inline">\(log\frac{1}{p(x)}\)</span>来表示应该是最优的，但是如果我们偏偏要用<spanclass="math inline">\(log\frac{1}{q(x)}\)</span>来对每个概率点表示，比如我们在P中有很大概率的时候，用了Q中有很小概率的东西进行了表示，那就造成了很大的损失，导致KL散度增加。每个点的改变的程度就是[]中的式子相减。因为函数的性质，总体加起来肯定会变大，这更说明了原来的表示的最优性。</p><hr /><h4 id="相对熵的凸性">相对熵的凸性</h4><p><span class="math display">\[D(\lambda p_1+(1-\lambda)p_2||\lambda q_1+(1-\lambda)q_2)\le \lambdaD(p_1||q_1)+(1-\lambda)D(p_2||q_2)\]</span></p><p>怎么理解？？？</p><hr /><p>Q：单个映射是不是也可以看成一个MC？</p><h2 id="chapter-3-渐进均分性">Chapter 3 渐进均分性</h2><p>关于<strong>渐进均分性</strong></p><p>这一章提出了典型集的概念。我们用抛硬币举例，抛100000次均匀硬币，全部正面朝上概率几乎为0，而50000次左右正面朝上的概率似乎大一些。渐进均分就是说，某些特定的情况（比如刚才提到的大约一半次数朝上）的概率之和将会无限趋近于1，同时，这些特定的情况本身的概率是几乎相等的。</p><p>一些标记：</p><ul><li><p>一系列随机变量<span class="math inline">\(X_1, X_2, \cdots, X_n\;\;i.i.d \sim p(x)\)</span></p></li><li><p>典型集<spanclass="math inline">\(A_{\varepsilon}^{(n)}\)</span></p></li></ul><p>则对于典型集以及处于典型集里的<span class="math inline">\((X_1, X_2,\cdots, X_n)\)</span>：</p><ul><li><spanclass="math inline">\(Pr\{A_{\varepsilon}^{(n)}\}&gt;1-\varepsilon\)</span></li><li><span class="math inline">\(|A_{\varepsilon}^{(n)}|\rightarrow2^{nH(X)}\)</span></li><li><span class="math inline">\(p(x_1, x_2, \cdots, x_n)\rightarrow2^{-nH(X)}\)</span></li></ul><h2 id="chapter-4-平稳过程的熵率">Chapter 4 平稳过程的熵率</h2><p>关于<strong>平稳过程的熵率</strong></p><p>熵率的定义： <span class="math display">\[H(\mathcal{X}) = \lim_{n\rightarrow \infty}{\frac{1}{n}H(X_1, X_2,\cdots, X_n)}\]</span> 再定义 <span class="math display">\[H&#39;(\mathcal{X}) = \lim_{n\rightarrow \infty}{H(X_n|X_{n-1}, \cdots ,X_1)}\]</span> 对于一个平稳随机过程，有 <span class="math display">\[H(\mathcal{X}) = H&#39;(\mathcal{X})\]</span> 对于马尔科夫过程，有 <span class="math display">\[H(\mathcal{X}) = H&#39;(\mathcal{X}) = H(X_2|X_1) =-\sum_{ij}{\mu_iP_{ij}}\log{P_{ij}}\]</span></p><h2 id="chapter-5-数据压缩">Chapter 5 数据压缩</h2><p>关于<strong>数据压缩</strong></p><p>给信息编码。主要关注前缀码(即时码)。</p><p>Kraft不等式： <span class="math display">\[\sum_iD^{-l_i}\le 1\]</span>可以推广到无穷。这个的证明非常妙！想象成一个D进制数，然后把所有编码投影到区间[0,1]，同时由于是即时码，取一个点会导致某个区间不可用。</p><p>对于满足某一组离散分布的点，最优码长<span class="math inline">\(L\rightarrow H_D(X)\)</span></p><h2 id="chapter-6-信息论与统计学">Chapter 6 信息论与统计学</h2><p>现在我们有一个字母表<span class="math inline">\(\mathcal{X}\)</span>.里面的元素为<span class="math inline">\(\{a_1, a_2, \cdots,a_{|\mathcal{X} |}\}\)</span></p><p>我们有<spanclass="math inline">\(n\)</span>个随机变量，它们都取自这个字母表。我们现在不关注每个随机变量具体取了字母表中的哪个元素，而关注统计上的情况，也就是说几个元素取了哪个字母表中元素。我们把这种分布一样的情况称为一个<strong>型</strong>，称为<spanclass="math inline">\(P\)</span>，型一样的元素组成的集合称为<strong>型类</strong><spanclass="math inline">\(T(P)\)</span>.所有型组成的集合是<spanclass="math inline">\(\mathcal{P}\)</span></p><p>型总数的粗略估计： <span class="math display">\[|\mathcal{P_n}| \le (n+1)^{|\mathcal{X}|}\]</span> 型类大小估计： <span class="math display">\[\frac{1}{(n+1)^{|\mathcal{X}|}}2^{nH(P)}\le|T(P)|\le2^{nH(P)}\]</span> 在任意概率<spanclass="math inline">\(Q\)</span>下，属于型类<spanclass="math inline">\(T(P)\)</span>的概率： <spanclass="math display">\[Q^n(T(P)) = 2^{-nD(P||Q)}\]</span> 在任意概率<span class="math inline">\(Q\)</span>下，型类<spanclass="math inline">\(P\)</span>中某一元素<spanclass="math inline">\(x\)</span>的概率： <span class="math display">\[Q^n(x) = 2^{-n(D(P_x||Q)+H(P_x))}\]</span> 可以感受到均分的存在。</p>]]></content>
    
    
    
    <tags>
      
      <tag>notes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[Record] 配置PyQt5 (Linux系统) </title>
    <link href="/2023/04/16/%5BRecord%5D%E9%85%8D%E7%BD%AEPyQt5%20(Linux%20%E7%B3%BB%E7%BB%9F)/"/>
    <url>/2023/04/16/%5BRecord%5D%E9%85%8D%E7%BD%AEPyQt5%20(Linux%20%E7%B3%BB%E7%BB%9F)/</url>
    
    <content type="html"><![CDATA[<h1 id="record配置pyqt5-linux-系统">[Record]配置PyQt5 (Linux 系统)</h1><blockquote><p>在Windows环境忙活了一下午都找不到东西，于是尝试使用Linux，果然方便快捷</p></blockquote><ul><li><p>系统：Ubuntu22.04</p></li><li><p>First, create a new environment in anaconda</p></li></ul><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">conda create <span class="hljs-literal">-n</span> pyqt python=<span class="hljs-number">3.8</span><br></code></pre></td></tr></table></figure><ul><li>Go into the environment</li></ul><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">conda activate pyqt<br></code></pre></td></tr></table></figure><ul><li>get pyqt5</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install pyqt5<br></code></pre></td></tr></table></figure><ul><li>Install Qt Designer</li></ul><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs powershell">sudo apt<span class="hljs-literal">-get</span> install qttools5<span class="hljs-literal">-dev-tools</span><br>sudo apt<span class="hljs-literal">-get</span> install qttools5<span class="hljs-literal">-dev</span><br></code></pre></td></tr></table></figure><ul><li>Open the Qt Designer</li></ul><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs powershell"><span class="hljs-built_in">cd</span> /usr/lib/x86_64<span class="hljs-literal">-linux-gnu</span>/qt5/bin/ <br>./designer<br></code></pre></td></tr></table></figure><ul><li>Edit the file and save it</li><li>Transform the ui into python</li></ul><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">pyuic5 path/to/helloworld.ui <span class="hljs-literal">-o</span> path/to/helloworld.py<br></code></pre></td></tr></table></figure><p>上面的命令生成的代码只是一些定义的函数，并没有调用，执行之后并不会有任何界面显示，需要在另外的python文件中进行import之后调用；如果想要在单独一个文件中执行并显示图形结果，可以使用</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">pyuic5 <span class="hljs-literal">-x</span> path/to/helloworld.ui <span class="hljs-literal">-o</span> path/to/helloworld.py<br></code></pre></td></tr></table></figure><blockquote><p>感谢来自csdn的文章<ahref="https://blog.csdn.net/ayiya_Oese/article/details/116299610">链接</a></p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>record</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[Semantic Segmentation] Some Classic network for Semantic Segmentation</title>
    <link href="/2023/03/09/Semantic%20segmentation/"/>
    <url>/2023/03/09/Semantic%20segmentation/</url>
    
    <content type="html"><![CDATA[<h1 id="semantic-segmentation">Semantic Segmentation</h1><ul><li>FCN</li><li>U-Net</li><li>PSPNet</li><li>Deeplab v1-3(+)</li></ul><h2 id="fcn">FCN</h2><blockquote><p>Our key insight is to build <strong>“fullyconvolutional”networks</strong> that take input of <strong>arbitrarysize</strong> and produce correspondingly-sized output with efficientinference and learning.</p></blockquote><blockquote><p>We then define <strong>a skip architecture</strong> that<strong>combines semantic information from a deep, coarse layer</strong>with appearance information from a shallow, fine layer to produceaccurate and detailed segmentations.</p></blockquote><p>Structure:</p><figure><img src="/typora-user-images/image-20230313202318039.png"alt="Structure of FCN" /><figcaption aria-hidden="true">Structure of FCN</figcaption></figure><p>The FCN will choose a classify network as backbone (e.g. VGGNet,ResNet). It abandon FC and replace them with Conv.</p><h4 id="details">Details:</h4><h2 id="unet">UNet</h2><p><strong>Structure:</strong></p><figure><img src="/typora-user-images/image-20230325195356603.png"alt="Structure of UNet" /><figcaption aria-hidden="true">Structure of UNet</figcaption></figure><h2 id="pspnet">PSPNet</h2><p><strong>Structure:</strong></p><figure><img src="/typora-user-images/image-20230325195446997.png"alt="Structure of PSPNet" /><figcaption aria-hidden="true">Structure of PSPNet</figcaption></figure>]]></content>
    
    
    
    <tags>
      
      <tag>paper</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[DL] Basic-Net</title>
    <link href="/2023/02/19/%5BDL%5DBasic-Net/"/>
    <url>/2023/02/19/%5BDL%5DBasic-Net/</url>
    
    <content type="html"><![CDATA[<h1 id="nets">Nets</h1><h2 id="杂七杂八的介绍">杂七杂八的介绍</h2><ul><li>LeNet 1994</li></ul><blockquote><p>是早期（1994年）的神经网络之一，用于手写数字识别，由卷积层，池化层，全连接层组成，网络结构如下图所示</p><figure><img src="/typora-user-images/image-20230313201432146.png"alt="Structure of LeNet" /><figcaption aria-hidden="true">Structure of LeNet</figcaption></figure></blockquote><ul><li>AlexNet 2012</li></ul><blockquote><p>是首个实用性很强的卷积神经网络由卷积操，池化层，全连接层，<strong>softmax层</strong>以及<strong>ReLU、Dropout</strong>构成。首次提出在2012年的ILSVRC大规模视觉识别竞赛上。其网络结构如下图所示：</p><figure><img src="/typora-user-images/image-20230313201406795.png"alt="Structure of AlexNet" /><figcaption aria-hidden="true">Structure of AlexNet</figcaption></figure></blockquote><ul><li>VGGNet 2014</li></ul><blockquote><p>VGGNet出现在2014年ILSVRC上比赛上获得了分类项目的第二名和定位项目的第一名，VGGNet相对于AlexNet堆叠了更多基础模块导致网络深度达到近二十层，另外它将之前5x5，7x7的卷积核替换成3x3的小卷积核，用2x2池化代替3x3，去除了局部响应归一化</p><p>。在训练高级别的网络时，可以先训练低级别的网络，用前者获得的权重初始化高级别的网络，可以加速网络的收敛。网络参数如下表所示：</p><figure><img src="/typora-user-images/image-20230313201420238.png"alt="Structure of VGGNet" /><figcaption aria-hidden="true">Structure of VGGNet</figcaption></figure></blockquote><ul><li><p>LeNet 1994</p></li><li><p>AlexNet 2012</p></li><li><p>VGGNet 2014</p></li><li><p>GoogleNet 2014</p></li><li><p>ResNet 2016</p></li><li><p>DenseNet 2017</p></li><li><p>SqueezeNet 2017</p></li><li><p>MobileNet 2017</p></li><li><p>SEnet 2018</p></li></ul><h2 id="lenet">LeNet</h2><blockquote><p>是一系列网络，包括LeNet 1-5</p><p>Yann LeCun 等人在 1990 年</p></blockquote><p>7层神经网络，包括3个卷积层，2个池化层，2个全连接层。所有卷积核5x5，stride = 1. 池化为全局，激活函数为Sigmoid</p><p>用pytorch展现网络架构</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Reshape</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)<br>    <br>net = torch.nn.Sequential(<br>    Reshape(), <br>    nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>), nn.Sigmoid(),  <span class="hljs-comment"># padding表示在边缘加2，因为本来是32x32的</span><br>    nn.AvgPool2d(<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),<br>    nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">5</span>), nn.Sigmoid(),<br>    nn.AvgPool2d(<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>), <br>    nn.Flatten(),<br>    nn.Linear(<span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>, <span class="hljs-number">120</span>), nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>), nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>)<br>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.rand(size=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>), dtype=torch.float32)<br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    X = layer(X)<br>    <span class="hljs-built_in">print</span>(layer.__class__.__name__, <span class="hljs-string">&#x27;output shape: \t&#x27;</span>, X.shape)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 输出</span><br>Reshape output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>])<br>Conv2d output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>])<br>Sigmoid output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>])<br>AvgPool2d output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">14</span>, <span class="hljs-number">14</span>])<br>Conv2d output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>])<br>Sigmoid output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>])<br>AvgPool2d output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>])<br>Flatten output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">400</span>])<br>Linear output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">120</span>])<br>Sigmoid output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">120</span>])<br>Linear output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">84</span>])<br>Sigmoid output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">84</span>])<br>Linear output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">10</span>])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">batch_size = <span class="hljs-number">256</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_accuracy_gpu</span>(<span class="hljs-params">net, data_iter, device=<span class="hljs-literal">None</span></span>): <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;使用GPU计算模型在数据集上的精度&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.Module):<br>        net.<span class="hljs-built_in">eval</span>()  <span class="hljs-comment"># 设置为评估模式</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> device:<br>            device = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(net.parameters())).device<br>    <span class="hljs-comment"># 正确预测的数量，总预测的数量</span><br>    metric = d2l.Accumulator(<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> data_iter:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(X, <span class="hljs-built_in">list</span>):<br>                <span class="hljs-comment"># BERT微调所需的（之后将介绍）</span><br>                X = [x.to(device) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> X]<br>            <span class="hljs-keyword">else</span>:<br>                X = X.to(device)<br>            y = y.to(device)<br>            metric.add(d2l.accuracy(net(X), y), y.numel())<br>    <span class="hljs-keyword">return</span> metric[<span class="hljs-number">0</span>] / metric[<span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_ch6</span>(<span class="hljs-params">net, train_iter, test_iter, num_epochs, lr, device</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_weights</span>(<span class="hljs-params">m</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(m) == nn.Linear <span class="hljs-keyword">or</span> <span class="hljs-built_in">type</span>(m) == nn.Conv2d:<br>            nn.init.xavier_uniform_(m.weight)<br>    net.apply(init_weights)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;training on &quot;</span>, device)<br>    net.to(device)<br>    optimizer = torch.optim.SGD(net.parameters(), lr=lr)<br>    loss = nn.CrossEntropyLoss()<br>    animator = d2l.Animator(xlabel=<span class="hljs-string">&#x27;epoch&#x27;</span>, xlim=[<span class="hljs-number">1</span>, num_epochs],<br>                            legend=[<span class="hljs-string">&#x27;train loss&#x27;</span>, <span class="hljs-string">&#x27;train acc&#x27;</span>, <span class="hljs-string">&#x27;test acc&#x27;</span>])<br>    timer, num_batches = d2l.Timer(), <span class="hljs-built_in">len</span>(train_iter)<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        metric = d2l.Accumulator(<span class="hljs-number">3</span>)<br>        net.train()<br>        <span class="hljs-keyword">for</span> i, (X, y) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_iter):<br>            timer.start()<br>            optimizer.zero_grad()<br>            X, y = X.to(device), y.to(device)<br>            y_hat = net(X)<br>            l = loss(y_hat, y)<br>            l.backward()<br>            optimizer.step()<br>            <span class="hljs-keyword">with</span> torch.no_grad():<br>                metric.add(l * X.shape[<span class="hljs-number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="hljs-number">0</span>])<br>            timer.stop()<br>            train_l = metric[<span class="hljs-number">0</span>] / metric[<span class="hljs-number">2</span>]<br>            train_acc = metric[<span class="hljs-number">1</span>] / metric[<span class="hljs-number">2</span>]<br>            <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % (num_batches // <span class="hljs-number">5</span>) == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> i == num_batches - <span class="hljs-number">1</span>:<br>                animator.add(epoch + (i + <span class="hljs-number">1</span>) / num_batches,<br>                             (train_l, train_acc, <span class="hljs-literal">None</span>))<br>        test_acc = evaluate_accuracy_gpu(net, test_iter)<br>        animator.add(epoch + <span class="hljs-number">1</span>, (<span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, test_acc))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;loss <span class="hljs-subst">&#123;train_l:<span class="hljs-number">.3</span>f&#125;</span>, train acc <span class="hljs-subst">&#123;train_acc:<span class="hljs-number">.3</span>f&#125;</span>, &#x27;</span><br>          <span class="hljs-string">f&#x27;test acc <span class="hljs-subst">&#123;test_acc:<span class="hljs-number">.3</span>f&#125;</span>&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;metric[<span class="hljs-number">2</span>] * num_epochs / timer.<span class="hljs-built_in">sum</span>():<span class="hljs-number">.1</span>f&#125;</span> examples/sec &#x27;</span><br>          <span class="hljs-string">f&#x27;on <span class="hljs-subst">&#123;<span class="hljs-built_in">str</span>(device)&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">lr, num_epochs = <span class="hljs-number">0.9</span>, <span class="hljs-number">20</span><br>train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">loss <span class="hljs-number">0.356</span>, train acc <span class="hljs-number">0.868</span>, test acc <span class="hljs-number">0.849</span><br><span class="hljs-number">56757.7</span> examples/sec on cuda:<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><figure><img src="/typora-user-images/image-20230214101934210.png"alt="result" /><figcaption aria-hidden="true">result</figcaption></figure><h2 id="alexnet">AlexNet</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br>net = nn.Sequential(<br>    <span class="hljs-comment"># 这里使用一个11*11的更大窗口来捕捉对象。</span><br>    <span class="hljs-comment"># 同时，步幅为4，以减少输出的高度和宽度。</span><br>    <span class="hljs-comment"># 另外，输出通道的数目远大于LeNet</span><br>    nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">96</span>, kernel_size=<span class="hljs-number">11</span>, stride=<span class="hljs-number">4</span>, padding=<span class="hljs-number">1</span>), nn.ReLU(),<br>    nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>    <span class="hljs-comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span><br>    nn.Conv2d(<span class="hljs-number">96</span>, <span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>), nn.ReLU(),<br>    nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>    <span class="hljs-comment"># 使用三个连续的卷积层和较小的卷积窗口。</span><br>    <span class="hljs-comment"># 除了最后的卷积层，输出通道的数量进一步增加。</span><br>    <span class="hljs-comment"># 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度</span><br>    nn.Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">384</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>), nn.ReLU(),<br>    nn.Conv2d(<span class="hljs-number">384</span>, <span class="hljs-number">384</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>), nn.ReLU(),<br>    nn.Conv2d(<span class="hljs-number">384</span>, <span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>), nn.ReLU(),<br>    nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>    nn.Flatten(),<br>    <span class="hljs-comment"># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span><br>    nn.Linear(<span class="hljs-number">6400</span>, <span class="hljs-number">4096</span>), nn.ReLU(),<br>    nn.Dropout(p=<span class="hljs-number">0.5</span>),<br>    nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>), nn.ReLU(),<br>    nn.Dropout(p=<span class="hljs-number">0.5</span>),<br>    <span class="hljs-comment"># 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span><br>    nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>)<br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    X=layer(X)<br>    <span class="hljs-built_in">print</span>(layer.__class__.__name__,<span class="hljs-string">&#x27;output shape:\t&#x27;</span>,X.shape)<br></code></pre></td></tr></table></figure><pre><code class="hljs">Conv2d output shape:     torch.Size([1, 96, 54, 54])ReLU output shape:   torch.Size([1, 96, 54, 54])MaxPool2d output shape:  torch.Size([1, 96, 26, 26])Conv2d output shape:     torch.Size([1, 256, 26, 26])ReLU output shape:   torch.Size([1, 256, 26, 26])MaxPool2d output shape:  torch.Size([1, 256, 12, 12])Conv2d output shape:     torch.Size([1, 384, 12, 12])ReLU output shape:   torch.Size([1, 384, 12, 12])Conv2d output shape:     torch.Size([1, 384, 12, 12])ReLU output shape:   torch.Size([1, 384, 12, 12])Conv2d output shape:     torch.Size([1, 256, 12, 12])ReLU output shape:   torch.Size([1, 256, 12, 12])MaxPool2d output shape:  torch.Size([1, 256, 5, 5])Flatten output shape:    torch.Size([1, 6400])Linear output shape:     torch.Size([1, 4096])ReLU output shape:   torch.Size([1, 4096])Dropout output shape:    torch.Size([1, 4096])Linear output shape:     torch.Size([1, 4096])ReLU output shape:   torch.Size([1, 4096])Dropout output shape:    torch.Size([1, 4096])Linear output shape:     torch.Size([1, 10])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">batch_size = <span class="hljs-number">128</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="hljs-number">224</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">lr, num_epochs = <span class="hljs-number">0.01</span>, <span class="hljs-number">10</span><br>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 输出</span><br>loss <span class="hljs-number">0.331</span>, train acc <span class="hljs-number">0.878</span>, test acc <span class="hljs-number">0.882</span><br><span class="hljs-number">1635.7</span> examples/sec on cuda:<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><figure><img src="/typora-user-images/image-20230214102245051.png"alt="result" /><figcaption aria-hidden="true">result</figcaption></figure><h2 id="vggnet">VGGNet</h2><p>使用块，更大更深的AlexNet</p><ul><li><p>带填充以保持分辨率的卷积层；</p></li><li><p>非线性激活函数，如ReLU；</p></li><li><p>汇聚层，如最大汇聚层。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">vgg_block</span>(<span class="hljs-params">num_convs, in_channels, out_channels</span>):<br>    layers = []<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_convs):<br>        layers.append(nn.Conv2d(in_channels, out_channels,<br>                                kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>))<br>        layers.append(nn.ReLU())<br>        in_channels = out_channels<br>    layers.append(nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>))<br>    <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">conv_arch = ((<span class="hljs-number">1</span>, <span class="hljs-number">64</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">256</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">512</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">512</span>))<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">vgg</span>(<span class="hljs-params">conv_arch</span>):<br>    conv_blks = []<br>    in_channels = <span class="hljs-number">1</span><br>    <span class="hljs-keyword">for</span> (num_convs, out_channels) <span class="hljs-keyword">in</span> conv_arch:<br>        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))<br>        in_channels = out_channels<br>    <span class="hljs-keyword">return</span> nn.Sequential(<br>        *conv_blks, <br>        nn.Flatten(),<br>        nn.Linear(out_channels * <span class="hljs-number">7</span> * <span class="hljs-number">7</span>, <span class="hljs-number">4096</span>), nn.ReLU(),<br>        nn.Dropout(<span class="hljs-number">0.5</span>), nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>), nn.ReLU(),<br>        nn.Dropout(<span class="hljs-number">0.5</span>), nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">10</span>)<br>    )<br>net = vgg(conv_arch)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.randn(size=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>))<br><span class="hljs-keyword">for</span> blk <span class="hljs-keyword">in</span> net:<br>    X = blk(X)<br>    <span class="hljs-built_in">print</span>(blk.__class__.__name__, <span class="hljs-string">&#x27;output shape:\t&#x27;</span>, X.shape)<br>    <br></code></pre></td></tr></table></figure><pre><code class="hljs">Sequential output shape:     torch.Size([1, 64, 112, 112])Sequential output shape:     torch.Size([1, 128, 56, 56])Sequential output shape:     torch.Size([1, 256, 28, 28])Sequential output shape:     torch.Size([1, 512, 14, 14])Sequential output shape:     torch.Size([1, 512, 7, 7])Flatten output shape:    torch.Size([1, 25088])Linear output shape:     torch.Size([1, 4096])ReLU output shape:   torch.Size([1, 4096])Dropout output shape:    torch.Size([1, 4096])Linear output shape:     torch.Size([1, 4096])ReLU output shape:   torch.Size([1, 4096])Dropout output shape:    torch.Size([1, 4096])Linear output shape:     torch.Size([1, 10])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">ratio = <span class="hljs-number">4</span><br>small_conv_arch = [(pair[<span class="hljs-number">0</span>], pair[<span class="hljs-number">1</span>] // ratio) <span class="hljs-keyword">for</span> pair <span class="hljs-keyword">in</span> conv_arch]<br>net = vgg(small_conv_arch)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.randn(size=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>))<br><span class="hljs-keyword">for</span> blk <span class="hljs-keyword">in</span> net:<br>    X = blk(X)<br>    <span class="hljs-built_in">print</span>(blk.__class__.__name__, <span class="hljs-string">&#x27;output shape:\t&#x27;</span>, X.shape)<br></code></pre></td></tr></table></figure><pre><code class="hljs">Sequential output shape:     torch.Size([1, 16, 112, 112])Sequential output shape:     torch.Size([1, 32, 56, 56])Sequential output shape:     torch.Size([1, 64, 28, 28])Sequential output shape:     torch.Size([1, 128, 14, 14])Sequential output shape:     torch.Size([1, 128, 7, 7])Flatten output shape:    torch.Size([1, 6272])Linear output shape:     torch.Size([1, 4096])ReLU output shape:   torch.Size([1, 4096])Dropout output shape:    torch.Size([1, 4096])Linear output shape:     torch.Size([1, 4096])ReLU output shape:   torch.Size([1, 4096])Dropout output shape:    torch.Size([1, 4096])Linear output shape:     torch.Size([1, 10])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">lr, num_epochs, batch_size = <span class="hljs-number">0.05</span>, <span class="hljs-number">10</span>, <span class="hljs-number">128</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="hljs-number">224</span>)<br>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br></code></pre></td></tr></table></figure><pre><code class="hljs">loss 0.177, train acc 0.935, test acc 0.8941091.9 examples/sec on cuda:0</code></pre><figure><img src="/typora-user-images/image-20230214140415757.png"alt="result" /><figcaption aria-hidden="true">result</figcaption></figure><h2 id="googlenetinception-v3">GoogleNet/Inception V3</h2><p>Inception 块</p><p>输入被copy成四块</p><figure><img src="/typora-user-images/image-20230214141018466.png"alt="image-20230214141018466" /><figcaption aria-hidden="true">image-20230214141018466</figcaption></figure><figure><img src="/typora-user-images/image-20230214141928373.png"alt="image-20230214141928373" /><figcaption aria-hidden="true">image-20230214141928373</figcaption></figure><p>用了很多1x1卷积，降低通道数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Inception</span>(nn.Module):<br>    <span class="hljs-comment"># c1--c4是每条路径的输出通道数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(Inception, self).__init__(**kwargs)<br>        <span class="hljs-comment"># 线路1，单1x1卷积层</span><br>        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 线路2，1x1卷积层后接3x3卷积层</span><br>        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="hljs-number">0</span>], kernel_size=<span class="hljs-number">1</span>)<br>        self.p2_2 = nn.Conv2d(c2[<span class="hljs-number">0</span>], c2[<span class="hljs-number">1</span>], kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 线路3，1x1卷积层后接5x5卷积层</span><br>        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="hljs-number">0</span>], kernel_size=<span class="hljs-number">1</span>)<br>        self.p3_2 = nn.Conv2d(c3[<span class="hljs-number">0</span>], c3[<span class="hljs-number">1</span>], kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)<br>        <span class="hljs-comment"># 线路4，3x3最大汇聚层后接1x1卷积层</span><br>        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>)<br>        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        p1 = F.relu(self.p1_1(x))<br>        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))<br>        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))<br>        p4 = F.relu(self.p4_2(self.p4_1(x)))<br>        <span class="hljs-comment"># 在通道维度上连结输出</span><br>        <span class="hljs-keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">b1 = nn.Sequential(nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>),<br>                   nn.ReLU(),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">b2 = nn.Sequential(nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">1</span>),<br>                   nn.ReLU(),<br>                   nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>                   nn.ReLU(),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">b3 = nn.Sequential(Inception(<span class="hljs-number">192</span>, <span class="hljs-number">64</span>, (<span class="hljs-number">96</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">32</span>), <span class="hljs-number">32</span>),<br>                   Inception(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">192</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">96</span>), <span class="hljs-number">64</span>),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">b4 = nn.Sequential(Inception(<span class="hljs-number">480</span>, <span class="hljs-number">192</span>, (<span class="hljs-number">96</span>, <span class="hljs-number">208</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">48</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">160</span>, (<span class="hljs-number">112</span>, <span class="hljs-number">224</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">128</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">256</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">112</span>, (<span class="hljs-number">144</span>, <span class="hljs-number">288</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">528</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">b5 = nn.Sequential(Inception(<span class="hljs-number">832</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   Inception(<span class="hljs-number">832</span>, <span class="hljs-number">384</span>, (<span class="hljs-number">192</span>, <span class="hljs-number">384</span>), (<span class="hljs-number">48</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>                   nn.Flatten())<br><br>net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.rand(size=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">96</span>, <span class="hljs-number">96</span>))<br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    X = layer(X)<br>    <span class="hljs-built_in">print</span>(layer.__class__.__name__,<span class="hljs-string">&#x27;output shape:\t&#x27;</span>, X.shape)<br></code></pre></td></tr></table></figure><pre><code class="hljs">Sequential output shape:     torch.Size([1, 64, 24, 24])Sequential output shape:     torch.Size([1, 192, 12, 12])Sequential output shape:     torch.Size([1, 480, 6, 6])Sequential output shape:     torch.Size([1, 832, 3, 3])Sequential output shape:     torch.Size([1, 1024])Linear output shape:     torch.Size([1, 10])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">lr, num_epochs, batch_size = <span class="hljs-number">0.1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">128</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="hljs-number">96</span>)<br>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br></code></pre></td></tr></table></figure><pre><code class="hljs">loss 0.246, train acc 0.907, test acc 0.8931690.6 examples/sec on cuda:0</code></pre><figure><img src="/typora-user-images/image-20230214200912069.png"alt="image-20230214200912069" /><figcaption aria-hidden="true">image-20230214200912069</figcaption></figure><h2 id="resnet">ResNet</h2><figure><img src="/typora-user-images/image-20230214201006292.png"alt="image-20230214201006292" /><figcaption aria-hidden="true">image-20230214201006292</figcaption></figure><figure><img src="/typora-user-images/image-20230214201016846.png"alt="image-20230214201016846" /><figcaption aria-hidden="true">image-20230214201016846</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Residual</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_channels, num_channels,</span><br><span class="hljs-params">                 use_1x1conv=<span class="hljs-literal">False</span>, strides=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.conv1 = nn.Conv2d(input_channels, num_channels,<br>                               kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>, stride=strides)<br>        self.conv2 = nn.Conv2d(num_channels, num_channels,<br>                               kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">if</span> use_1x1conv:<br>            self.conv3 = nn.Conv2d(input_channels, num_channels,<br>                                   kernel_size=<span class="hljs-number">1</span>, stride=strides)<br>        <span class="hljs-keyword">else</span>:<br>            self.conv3 = <span class="hljs-literal">None</span><br>        self.bn1 = nn.BatchNorm2d(num_channels)<br>        self.bn2 = nn.BatchNorm2d(num_channels)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        Y = F.relu(self.bn1(self.conv1(X)))<br>        Y = self.bn2(self.conv2(Y))<br>        <span class="hljs-keyword">if</span> self.conv3:<br>            X = self.conv3(X)<br>        Y += X<br>        <span class="hljs-keyword">return</span> F.relu(Y)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">b1 = nn.Sequential(nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>),<br>                   nn.BatchNorm2d(<span class="hljs-number">64</span>), nn.ReLU(),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)<br>                  )<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">resnet_block</span>(<span class="hljs-params">input_channels, num_channels, num_residuals,</span><br><span class="hljs-params">                 first_block=<span class="hljs-literal">False</span></span>):<br>    blk = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_residuals):<br>        <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> first_block:<br>            blk.append(Residual(input_channels, num_channels,<br>                                use_1x1conv=<span class="hljs-literal">True</span>, strides=<span class="hljs-number">2</span>))<br>        <span class="hljs-keyword">else</span>:<br>            blk.append(Residual(num_channels, num_channels))<br>    <span class="hljs-keyword">return</span> blk<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">b2 = nn.Sequential(*resnet_block(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-number">2</span>, first_block=<span class="hljs-literal">True</span>))<br>b3 = nn.Sequential(*resnet_block(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">2</span>))<br>b4 = nn.Sequential(*resnet_block(<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>))<br>b5 = nn.Sequential(*resnet_block(<span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">2</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">net = nn.Sequential(b1, b2, b3, b4, b5,<br>                    nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>                    nn.Flatten(), nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.rand(size=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>))<br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    X = layer(X)<br>    <span class="hljs-built_in">print</span>(layer.__class__.__name__,<span class="hljs-string">&#x27;output shape:\t&#x27;</span>, X.shape)<br></code></pre></td></tr></table></figure><pre><code class="hljs">Sequential output shape:     torch.Size([1, 64, 56, 56])Sequential output shape:     torch.Size([1, 64, 56, 56])Sequential output shape:     torch.Size([1, 128, 28, 28])Sequential output shape:     torch.Size([1, 256, 14, 14])Sequential output shape:     torch.Size([1, 512, 7, 7])AdaptiveAvgPool2d output shape:  torch.Size([1, 512, 1, 1])Flatten output shape:    torch.Size([1, 512])Linear output shape:     torch.Size([1, 10])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">lr, num_epochs, batch_size = <span class="hljs-number">0.05</span>, <span class="hljs-number">10</span>, <span class="hljs-number">256</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="hljs-number">96</span>)<br>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br></code></pre></td></tr></table></figure><pre><code class="hljs">loss 0.015, train acc 0.996, test acc 0.8932613.9 examples/sec on cuda:0</code></pre><figure><img src="/typora-user-images/image-20230214201027140.png"alt="image-20230214201027140" /><figcaption aria-hidden="true">image-20230214201027140</figcaption></figure>]]></content>
    
    
    
    <tags>
      
      <tag>notes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[cs231n] Summary</title>
    <link href="/2023/02/18/%5Bcs231n%5DSummary/"/>
    <url>/2023/02/18/%5Bcs231n%5DSummary/</url>
    
    <content type="html"><![CDATA[<h1 id="cs231n-summary">cs231n: Summary</h1><h2 id="neural-networks">Neural Networks</h2><h3id="image-classification-data-driven-approach-k-nearest-neighbor-trainvaltest-splits"><ahref="https://cs231n.github.io/classification/">Image Classification:Data-driven Approach, k-Nearest Neighbor, train/val/test splits</a></h3><h4 id="image-classification">Image Classification</h4><p>Using a set of labeled images to predict categories of a set of testimages. Then we can measure the accuracy of the predictions.</p><h4 id="nearest-neighbor-classifier">Nearest Neighbor classifier</h4><ul><li>Choose a distance(L1, L2, etc.)</li><li>Calculate the sum of the distance between each text data and all thetrain data. Get the closest one. The label of this data is what theclassifier predict.</li></ul><h4 id="knn-classifier">kNN classifier</h4><p>Find the top k closest images and then have them vote on the label ofthe test image.</p><h4 id="validation-set-cross-validation">Validation set,cross-validation</h4><p><img src="/typora-user-images/image-20230203181619887.png" alt="image-20230203181619887" style="zoom:80%;" /></p><p>In this picture, fold 5 is the validation set. For cross-validation,we let fold 1-5 be validation set separately to help us choose somehyperparameters.</p><h3 id="linear-classification-support-vector-machine-softmax"><ahref="https://cs231n.github.io/linear-classify/">Linear classification:Support Vector Machine, Softmax</a></h3><p>What we want: a map from images to label scores. <spanclass="math inline">\(\Rightarrow\)</span> Score function, Lossfunction</p><h4 id="score-function">Score function</h4><p><span class="math inline">\(x_i\)</span> is a picture and <spanclass="math inline">\(W\)</span> is a matrix named weights. And <spanclass="math inline">\(b\)</span> is bias.<br /><span class="math display">\[f(x_i,W,b)=Wx_i+b\]</span> Sometimes we can extend <spanclass="math inline">\(W\)</span>:</p><p><img src="/typora-user-images/image-20230203184649909.png" alt="image-20230203184649909" style="zoom: 67%;" /></p><h5 id="preprocessing-center-the-data">Preprocessing: center thedata</h5><p>For photos, pixel value: [0~255]</p><p>Now: [0…255] <span class="math inline">\(\Rightarrow\)</span> [-127.. 127] <span class="math inline">\(\Rightarrow\)</span> [-1,1]</p><h4 id="loss-function">Loss function</h4><h5 id="multiclass-support-vector-machine-loss">Multiclass SupportVector Machine loss</h5><p>For image <span class="math inline">\(x_i\)</span> with label <spanclass="math inline">\(y_i\)</span>. Score function is <spanclass="math inline">\(f(x_i,W)\)</span>. Let <spanclass="math inline">\(s_j = f(x_i,W)_j\)</span>. Multiclass SVM loss:<span class="math display">\[L_i = \sum_{j\neq y_i}max(0,s_j-s_{y_i}+\Delta)\]</span></p><h6 id="regularization">Regularization</h6><p><span class="math display">\[R(W) = \sum_k\sum_l W_{k,l}^2\\L=\frac{1}{N}\sum_i L_i + \lambda R(W)\]</span></p><p>Or <span class="math display">\[L=\frac{1}{N}\sum_i\sum_{j\neqy_i}[max(0,f(x_i,W))_j-f(x_i,W)_{y_i}+\Delta]+\lambda\sum_k\sum_lW_{k,l}^2\]</span></p><h4 id="softmax-classifier">Softmax classifier</h4><p>With <span class="math inline">\(f(x_i,W)=Wx_i\)</span> unchanged,but for the loss: <span class="math display">\[L_i=-log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}})\;or\;L_i=-f_{y_i}+log\sum_je^{f_j}\]</span></p><h3 id="optimization-stochastic-gradient-descent"><ahref="https://cs231n.github.io/optimization-1/">Optimization: StochasticGradient Descent</a></h3><h4 id="visualizing-the-loss-function">Visualizing the lossfunction</h4><p><img src="/typora-user-images/image-20230203191407300.png" alt="image-20230203191407300" style="zoom:80%;" /></p><figure><img src="/typora-user-images/image-20230203191516168.png"alt="image-20230203191516168" /><figcaption aria-hidden="true">image-20230203191516168</figcaption></figure><h4 id="optimization">Optimization</h4><ul><li><p>Random search</p></li><li><p>Random local search</p></li><li><p>Following the gradient</p></li></ul><p><span class="math display">\[\frac{df(x)}{dx}=\lim_{h\rightarrow 0}\frac{f(x+h)-f(x)}{h}\]</span></p><p>Use this definition to calculate grad of all dims</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">eval_numerical_gradient</span>(<span class="hljs-params">f, x</span>):<br>  <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">  a naive implementation of numerical gradient of f at x</span><br><span class="hljs-string">  - f should be a function that takes a single argument</span><br><span class="hljs-string">  - x is the point (numpy array) to evaluate the gradient at</span><br><span class="hljs-string">  &quot;&quot;&quot;</span><br><br>  fx = f(x) <span class="hljs-comment"># evaluate function value at original point</span><br>  grad = np.zeros(x.shape)<br>  h = <span class="hljs-number">0.00001</span><br><br>  <span class="hljs-comment"># iterate over all indexes in x</span><br>  it = np.nditer(x, flags=[<span class="hljs-string">&#x27;multi_index&#x27;</span>], op_flags=[<span class="hljs-string">&#x27;readwrite&#x27;</span>])<br>  <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> it.finished:<br><br>    <span class="hljs-comment"># evaluate function at x+h</span><br>    ix = it.multi_index<br>    old_value = x[ix]<br>    x[ix] = old_value + h <span class="hljs-comment"># increment by h</span><br>    fxh = f(x) <span class="hljs-comment"># evalute f(x + h)</span><br>    x[ix] = old_value <span class="hljs-comment"># restore to previous value (very important!)</span><br><br>    <span class="hljs-comment"># compute the partial derivative</span><br>    grad[ix] = (fxh - fx) / h <span class="hljs-comment"># the slope</span><br>    it.iternext() <span class="hljs-comment"># step to next dimension</span><br><br>  <span class="hljs-keyword">return</span> grad<br></code></pre></td></tr></table></figure><p>In fact, we usually use <span class="math display">\[\frac{f(x+h)-f(x-h)}{2h}\]</span> But this method of calculation is expensive and not soaccurate. So maybe we can do it in a more “math” way.</p><p>Take loss function of SVM as an example: <spanclass="math display">\[L_i = \sum_{j\neq y_i}[max(0,w_j^Tx_i-w_{y_i}^Tx_i+\Delta)]\]</span> We can differentiate the function: <spanclass="math display">\[\Delta_{w_{y_i}}L_i = -(\sum_{j\neqy_i}1_{\{w_j^Tx_i-w_{y_i}^Tx_i+\Delta &gt;0\}}) \\\Delta_{w_{j}}L_i = 1_{\{w_j^Tx_i-w_{y_i}^Tx_i+\Delta &gt;0\}}x_i\]</span> With grad, we can do <strong>Gradient Descent</strong> bychoosing a suitable <strong>step size</strong>(or <strong>learningrate</strong>),</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>  weights_grad = evaluate_gradient(loss_fun, data, weights)<br>  weights += - step_size * weights_grad <span class="hljs-comment"># perform parameter update</span><br></code></pre></td></tr></table></figure><p>However, if the dataset is very big, this can be extremely expensive.So we introduce <strong>Mini-batch gradient descent</strong>. That meanswe can only evaluate on a small subset to get a gradient.</p><p>The extreme case of this method is the subset has only one image.This process is called <strong>Stochastic GradientDescent(SGD)</strong></p><h3 id="backpropagation-intuitions"><ahref="https://cs231n.github.io/optimization-2/">Backpropagation,Intuitions</a></h3><p>Make good use of chain rule</p><p><strong>Ex</strong></p><figure><img src="/typora-user-images/image-20230203202320171.png"alt="image-20230203202320171" /><figcaption aria-hidden="true">image-20230203202320171</figcaption></figure><ul><li>Add gate</li><li>Max gate</li><li>Multiply gate</li></ul><h3 id="neural-networks-part-1-setting-up-the-architecture"><ahref="https://cs231n.github.io/neural-networks-1/">Neural Networks Part1: Setting up the Architecture</a></h3><figure><img src="/typora-user-images/image-20230207121144890.png"alt="image-20230207121144890" /><figcaption aria-hidden="true">image-20230207121144890</figcaption></figure><p>input<span class="math inline">\(\rightarrow\)</span>input<spanclass="math inline">\(\cdot\)</span>weight<spanclass="math inline">\(\rightarrow\)</span>input<spanclass="math inline">\(\cdot\)</span>weight+bias<spanclass="math inline">\(\rightarrow\)</span>activate-f(input<spanclass="math inline">\(\cdot\)</span>weight+bias)</p><p><strong>Ex</strong> <span class="math display">\[s=W_2max(0,W_1x)\\s=W_3max(0,W_2max(0,W_1x))\]</span></p><h4 id="single-neuron-as-a-linear-classifier">Single neuron as a linearclassifier</h4><ul><li>Binary Softmax classifier</li><li>Binary SVM classifier</li><li>Regularization interpretation</li></ul><h4 id="commonly-used-activation-functions">Commonly used activationfunctions</h4><ul><li><strong>Sigmoid</strong></li></ul><p><span class="math display">\[\sigma(x)=\frac{1}{1+e^{-x}}\]</span></p><p>​ Shortcomings:</p><p>​ 1. kill gradients</p><p>​ 2. not zero-centered</p><p><img src="/typora-user-images/image-20230207123419875.png" alt="image-20230207123419875" style="zoom:50%;" /></p><ul><li><strong>Tanh</strong></li></ul><p><span class="math display">\[tanh(x)=2\sigma(2x)-1\]</span></p><p><img src="/typora-user-images/image-20230207123435272.png" alt="image-20230207123435272" style="zoom:50%;" /></p><p>​ Solve the problem of not zero-centered</p><ul><li><strong>ReLU</strong></li></ul><p><span class="math display">\[f(x)=max(0,x)\]</span></p><p>​ Advantages: greatly accelerate the convergence of stochasticdescent; not so expensive as tanh/sigmoid</p><p>​ Shortcoming: may die when a large gradient flow through a ReLUneuron. May solved by setting a proper learning rate</p><ul><li><strong>Leaky ReLU</strong></li></ul><p><span class="math display">\[f(x)=1_{(x&lt;0)}(\alpha x)+1_{(x&gt;=0)}(x)\]</span></p><p>​ in which <span class="math inline">\(\alpha\)</span> is very smalllike 0.01</p><ul><li><strong>Maxout</strong></li></ul><p><span class="math display">\[max(w_1^Tx+b_1,w_2^Tx+b_2)\]</span></p><blockquote><p><strong>Naming conventions</strong> When we talk about N-layer neuralnetwork, the input layer is not included in “N”.</p><p><strong>Output Layer</strong> Unlike other layers, the output layerneurons most commonly do not have an activation function. (while theoutput layer is used to present scores of every class, it’s easy tounderstand)</p><p><strong>Sizing neural networks</strong> Measure the size of neural:number of neurons/number of parameters</p></blockquote><h4 id="representational-power">Representational power</h4><p>Surveys has proven that given any continuous function f(x)and someϵ&gt;0, there exists a Neural Network g(x) with one hidden layer (with areasonable choice of non-linearity, e.g. sigmoid) such that∀x,∣f(x)−g(x)∣&lt;ϵ. In other words, the neural network can approximateany continuous function.</p><p>Practically, deeper networks can work better than asingle-hidden-layer network</p><p>For <strong>neural network</strong>, usually 3-layer networks will bebetter than 2-layer nets. But more deeper(4,5,6-layer) network rarelyhelps much more. But for <strong>convolutional network </strong>, it isdifferent. Depth is a very important factor.</p><p>Regularization is very important, which can elite overfitting.</p><h3 id="neural-networks-part-2-setting-up-the-data-and-the-loss"><ahref="https://cs231n.github.io/neural-networks-2/">Neural Networks Part2: Setting up the Data and the Loss</a></h3><h4 id="data-preprocessing">Data Preprocessing</h4><h5 id="mean-subtraction">Mean subtraction</h5><p>code: <code>X-=np.mean(X, axis = 0)</code></p><h5 id="normalization">Normalization</h5><p>code: <code>X/=np.std(X, axis = 0)</code></p><h5 id="pca-and-whitening">PCA and Whitening</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Assume input data matrix X of size [N x D]</span><br>X -= np.mean(X, axis = <span class="hljs-number">0</span>) <span class="hljs-comment"># zero-center the data (important)</span><br>cov = np.dot(X.T, X) / X.shape[<span class="hljs-number">0</span>] <span class="hljs-comment"># get the data covariance matrix</span><br></code></pre></td></tr></table></figure><p>compute the SVD factorization of the data covariance matrix:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">U,S,V = np.linalg.svd(cov)<br></code></pre></td></tr></table></figure><p>where the columns of<code class="language-plaintext highlighter-rouge">U</code> are theeigenvectors and<code class="language-plaintext highlighter-rouge">S</code> is a 1-Darray of the singular values. To decorrelate the data, we project theoriginal (but zero-centered) data into the eigenbasis:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">Xrot = np.dot(X, U)<br></code></pre></td></tr></table></figure><p>dimensionality reduction:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">Xrot_reduced = np.dot(X, U[:,:<span class="hljs-number">100</span>]) <span class="hljs-comment"># Xrot_reduced becomes [N x 100]</span><br></code></pre></td></tr></table></figure><p>For <strong>whitening</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># whiten the data:</span><br><span class="hljs-comment"># divide by the eigenvalues (which are square roots of the singular values)</span><br>Xwhite = Xrot / np.sqrt(S + <span class="hljs-number">1e-5</span>)<br></code></pre></td></tr></table></figure><figure><img src="/typora-user-images/image-20230207144339020.png"alt="image-20230207144339020" /><figcaption aria-hidden="true">image-20230207144339020</figcaption></figure><p><strong>Common pitfall</strong> The mean must be computed only overthe training data and then subtracted equally from all splits(train/val/test).</p><h4 id="weight-initialization">Weight Initialization</h4><p><strong>Pitfall: all zero initialization</strong> That will makeevery neuron do same thing.</p><p><strong>Small random numbers</strong> aim: <em>symmetrybreaking</em>. <code>W=0.01*np.random.randn(D, H)</code></p><p><strong>Calibrating the variances with 1/sqrt(n)</strong><code>W=np.random.randn(n) / sqrt(n)</code> where n is the number of itsinputs</p><p>Other: <strong>Sparse initialization</strong>, <strong>Initializingthe biases(0)</strong>, <strong>Batch Normalization</strong></p><h4 id="regularization-1">Regularization</h4><p><strong>L2 regularization</strong> May the most common. for all <spanclass="math inline">\(w\)</span>, add <spanclass="math inline">\(\frac{1}{2}\lambda w^2\)</span> to theobjective.</p><p><strong>L1 regularization</strong> for each weight <spanclass="math inline">\(w\)</span>, we add the term <spanclass="math inline">\(\lambda |w|\)</span> to the objective.</p><p>We can also combine the L1 regularization with the L2 regularization:<span class="math inline">\(\lambda _1|w|+\lambda_2w^2\)</span>, whichis called Elastic net regularization.</p><p><strong>Max norm constraints</strong> Enforce an absolute upper boundon the magnitude of the weight vector.</p><p><strong>Dropout</strong> keep a neuron active with some probability<span class="math inline">\(p\)</span> or setting it to zerootherwise</p><p>And there are many other methods about regularization. <strong>Biasregularization, per-layer regularization</strong>…</p><h4 id="loss-functions">Loss functions</h4><p>Let <span class="math inline">\(f=f(x_i;W)\)</span> to be theactivations of the output layer in a Neural Network.</p><p><strong>Classification</strong></p><p>SVM: <span class="math display">\[L_i=\sum_{j\neq y_i}max(0,f_j-f_{y_i}+1)\]</span></p><blockquote><p>sometimes use <spanclass="math inline">\(max(0,(f_j-f_{y_i}+1)^2)\)</span></p></blockquote><p>Softmax: <span class="math display">\[L_i=-log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}})\]</span> <strong>Problem: Large number of classes</strong> When the setof labels is very large, Softmax becomes very expensive. It may behelpful to use <em>Hierarchical Softmax</em>.</p><p><strong>Attribute classification</strong></p><p>Both losses above assume that there is a single correct answer <spanclass="math inline">\(y_i\)</span>. But what if <spanclass="math inline">\(y_i\)</span> is a binary vector where everyexample may or may not have a certain attribute <spanclass="math display">\[L_i=\sum_jmax(0,1-y_{ij}f_j)\]</span> where <span class="math inline">\(y_{ij}\)</span> is either +1or -1</p><h3 id="neural-networks-part-3-learning-and-evaluation"><ahref="https://cs231n.github.io/neural-networks-3/">Neural Networks Part3: Learning and Evaluation</a></h3><p>Talking about learning process.</p><h4 id="gradient-checks">Gradient Checks</h4><p><span class="math display">\[\frac{df(x)}{dx}=\frac{f(x+h)-f(x-h)}{2h}\]</span></p><p>In order to compare the numerical gradient <spanclass="math inline">\(f_n&#39;\)</span> and analytic gradient <spanclass="math inline">\(f_a&#39;\)</span>, we can use relative error:<span class="math display">\[\frac{|f_a&#39;-f_b&#39;|}{max(|f_a&#39;|,|f_n&#39;|)}\]</span> ​ In practive:</p><ul><li>relative error &gt;<span class="math inline">\(1e-2\)</span>:probably wrong</li><li>1e-2&gt;relative error&gt;1e-4: uncomfortable…</li><li>1e-4&gt;relative error: OK…but without kinks(e.g. tanh and softmax),to high.</li><li>1e-7&gt;relative error: happy</li></ul><blockquote><p>should use double precision</p></blockquote><blockquote><p>if jump kinks, may not be exact</p></blockquote><p>In order to avoid above problems:</p><ul><li>Use only few datapoints</li><li>be careful with h</li></ul><figure><img src="/typora-user-images/image-20230207205258779.png"alt="image-20230207205258779" /><figcaption aria-hidden="true">image-20230207205258779</figcaption></figure><ul><li><p>Don’t let the regularization overwhelm the data</p></li><li><p>Remember to turn off dropout/augmentations</p></li></ul><h4 id="before-learning-sanity-checks-tipstricks">Before learning:sanity checks Tips/Tricks</h4><ul><li>Trace loss function</li></ul><p><img src="/typora-user-images/image-20230207210235489.png" alt="image-20230207210235489" style="zoom:80%;" /></p><p>​ The right picture may mean the data is so small</p><ul><li>Trace train/val accuracy</li></ul><p>​<img src="/typora-user-images/image-20230207210422908.png" alt="image-20230207210422908" style="zoom:80%;" /></p><ul><li>Ratio of weights: updates</li></ul><p>​ <strong>Ex</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># assume parameter vector W and its gradient vector dW</span><br>param_scale = np.linalg.norm(W.ravel())<br>update = -learning_rate*dW <span class="hljs-comment"># simple SGD update</span><br>update_scale = np.linalg.norm(update.ravel())<br>W += update <span class="hljs-comment"># the actual update</span><br><span class="hljs-built_in">print</span> update_scale / param_scale <span class="hljs-comment"># want ~1e-3</span><br></code></pre></td></tr></table></figure><ul><li><p>Activation / Gradient distributions per layer</p></li><li><p>First-layer Visualizations</p></li></ul><p>​ <strong>Ex</strong></p><p><img src="/typora-user-images/image-20230207211414465.png" alt="image-20230207211414465" style="zoom:80%;" /></p><p>​ Left: many noise. may in trouble. Right: Nice, smooth</p><h4 id="parameter-updates">Parameter updates</h4><h5 id="first-ordersgd-momentum-nesterov-momentum">First-order(SGD),momentum, Nesterov momentum</h5><ul><li><strong>Vanilla</strong> update</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x += - learning_rate * dx<br></code></pre></td></tr></table></figure><ul><li><strong>Momentum</strong> update</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Momentum update</span><br>v = mu * v - learning_rate * dx <span class="hljs-comment"># integrate velocity</span><br>x += v <span class="hljs-comment"># integrate position</span><br></code></pre></td></tr></table></figure><p>​ mu can be seen as the coefficient of friction in physics. (typical0.9)</p><ul><li><strong>Nesterov</strong> Momentum</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">x_ahead = x + mu * v<br><span class="hljs-comment"># evaluate dx_ahead (the gradient at x_ahead instead of at x)</span><br>v = mu * v - learning_rate * dx_ahead<br>x += v<br></code></pre></td></tr></table></figure><figure><img src="/typora-user-images/image-20230207213105160.png"alt="image-20230207213105160" /><figcaption aria-hidden="true">image-20230207213105160</figcaption></figure><p>​ In practice, people like to rename <spanclass="math inline">\(x\_head\)</span> as <spanclass="math inline">\(x\)</span> :</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">v_prev = v <span class="hljs-comment"># back this up</span><br>v = mu * v - learning_rate * dx <span class="hljs-comment"># velocity update stays the same</span><br>x += -mu * v_prev + (<span class="hljs-number">1</span> + mu) * v <span class="hljs-comment"># position update changes form</span><br></code></pre></td></tr></table></figure><h5 id="annealing-the-learning-rate">Annealing the learning rate</h5><p>If the learning rate is too high, the system contains too muchkinetic energy, unable to settle down into deeper, but narrower parts ofthe loss function.</p><p>Normally, there are three methods to decay the learning rate:</p><ul><li><strong>Step decay</strong> Reduce the learning rate every fewepochs.</li><li><strong>Exponential decay</strong> In math: <spanclass="math inline">\(\alpha = \alpha_0e^{-kt}\)</span> in which <spanclass="math inline">\(\alpha_0,k\)</span> are hyperparameters and <spanclass="math inline">\(t\)</span> is the iteration number.</li><li><strong>1/t decay</strong> In math: <spanclass="math inline">\(\alpha = \alpha _0/(1+kt)\)</span> in which <spanclass="math inline">\(\alpha_0,k\)</span> are hyperparameters and <spanclass="math inline">\(t\)</span> is the iteration number</li></ul><p>​ In practice, we find that the step decay is slightly preferablebecause the hyperparameters it involves</p><h5 id="second-order-methods">Second order methods</h5><p>Basing on Newton’s method: <span class="math display">\[x\leftarrow x-[Hf(x)]^{-1}\nabla f(x)\]</span> Multiplying by the inverse Hessian leads the optimization totake more aggressive steps in directions of shallow curvature andshorter steps in directions of steep curvature</p><p>However, because of the expensive cost of calculating the Hessianmatrix, this method is impractical.</p><h5 id="per-parameter-adaptive-learning-rate-methods">Per-parameteradaptive learning rate methods</h5><p><strong>Adagrad</strong> is an adaptive learning rate methodoriginally proposed by <ahref="http://jmlr.org/papers/v12/duchi11a.html">Duchi et al.</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Assume the gradient dx and parameter vector x</span><br>cache += dx**<span class="hljs-number">2</span><br>x += - learning_rate * dx / (np.sqrt(cache) + eps)<br></code></pre></td></tr></table></figure><p><code>eps</code>: 1e-4~1e-8</p><p>shortcoming: the monotonic learning rate usually proves tooaggressive and stops learning too early</p><p><strong>RMSprop</strong> <ahref="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">slide29 of Lecture 6</a> of Geoff Hinton’s Coursera class: reduce Adagrad’saggressive</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">cache = decay_rate * cache + (<span class="hljs-number">1</span> - decay_rate) * dx**<span class="hljs-number">2</span><br>x += - learning_rate * dx / (np.sqrt(cache) + eps)<br></code></pre></td></tr></table></figure><p>in which <code>decay_rate</code> is a hyperparameter: 0.9, 0.99,0.999</p><p><strong>Adam</strong> a recently proposed update looks a bit likeRMSprop with momentum</p><p>simplified:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">m = beta1*m + (<span class="hljs-number">1</span>-beta1)*dx<br>v = beta2*v + (<span class="hljs-number">1</span>-beta2)*(dx**<span class="hljs-number">2</span>)<br>x += - learning_rate * m / (np.sqrt(v) + eps)<br></code></pre></td></tr></table></figure><p>recommend: <code>eps = 1e-8</code>, <code>beta1 = 0.9</code>,<code>beta2 = 0.999</code></p><p>With the <em>bias correction</em> mechanism, the update looks asfollows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># t is your iteration counter going from 1 to infinity</span><br>m = beta1*m + (<span class="hljs-number">1</span>-beta1)*dx<br>mt = m / (<span class="hljs-number">1</span>-beta1**t)<br>v = beta2*v + (<span class="hljs-number">1</span>-beta2)*(dx**<span class="hljs-number">2</span>)<br>vt = v / (<span class="hljs-number">1</span>-beta2**t)<br>x += - learning_rate * mt / (np.sqrt(vt) + eps)<br></code></pre></td></tr></table></figure><h4 id="hyperparameter-optimization">Hyperparameter optimization</h4><p>The most common hyperparameters in context of Neural Networksinclude:</p><ul><li>the initial learning rate</li><li>learning rate decay schedule(such as the decay constant)</li><li>regularization strength(L2 penalty, dropout strength)</li></ul><p><strong>Implementation</strong> make a worker and master</p><p><strong>Prefer one validation fold to cross-validation</strong> asingle validation set of respectable size substantially simplifies thecode base, without the need for cross-validation with multiple folds</p><p><strong>Hyperparameter ranges</strong><code>learning_rate = 10 ** uniform(-6, 1)</code></p><p><strong>Prefer random search to grid search</strong> can be easilyunderstand through following image:</p><figure><img src="/typora-user-images/image-20230208141840955.png"alt="image-20230208141840955" /><figcaption aria-hidden="true">image-20230208141840955</figcaption></figure><p><strong>Careful with best values on border</strong> if we find thatthe results is on the border, we may set a bad range and miss the truebest result.</p><p><strong>Stage your search from coarse to fine</strong></p><p><strong>Bayesian Hyperparameter Optimization</strong> <ahref="https://github.com/JasperSnoek/spearmint">Spearmint</a>, <ahref="http://www.cs.ubc.ca/labs/beta/Projects/SMAC/">SMAC</a>, and <ahref="http://jaberg.github.io/hyperopt/">Hyperopt</a>. However, inpractical settings with ConvNets it is still relatively difficult tobeat random search in a carefully-chosen intervals.</p><h4 id="evaluation">Evaluation</h4><h5 id="model-ensembles">Model Ensembles</h5><p>A reliable way to improve the performance of Neural Networks by a fewpercent: train multiple independent models, and at test time averagetheir predictions.</p><p>The number of models <span class="math inline">\(\uparrow\)</span>performance <span class="math inline">\(\uparrow\)</span> the variety ofmodels <span class="math inline">\(\uparrow\)</span> performance <spanclass="math inline">\(\uparrow\)</span></p><p>Some approaches to forming an ensemble</p><ul><li><strong>Same model, different initializations</strong></li></ul><p>​ Use cross-validation to determine the best hyperparameters, thentrain multiple models with the best set of hyperparameters but withdifferent random initialization.</p><p>​ shortcoming: variety is only due to iitialization</p><ul><li><p><strong>Top models discovered duringcross-validation</strong></p><p>Use cross-validation to determine the best hyperparameters, then pickthe top few (e.g. 10) models to form the ensemble.</p></li></ul><p>​ shortcoming: may include suboptimal models</p><ul><li><strong>Different checkpoints of a single model</strong></li></ul><p>​ Taking different checkpoints of a single network over time istraining is very expensive</p><p>​ shortcoming: lack of variety</p><p>​ advantage: very cheap</p><ul><li><strong>Running average of parameters during training</strong></li></ul><p>​</p><p>Shortcoming of model ensembles: take longer to evaluate on testexample.</p><p>A good idea: “distill” a good ensemble back to a single model byincorporating the ensemble log likelihoods into a modifiedobjective.</p><h2 id="convolutional-neural-networks">Convolutional NeuralNetworks</h2><h3id="convolutional-neural-networks-architectures-convolution-pooling-layers"><ahref="https://cs231n.github.io/convolutional-networks/">ConvolutionalNeural Networks: Architectures, Convolution / Pooling Layers</a></h3><p>CNN base on an assumption that the inputs are images.</p><h4 id="layers-used-to-build-convnets">Layers used to buildConvNets</h4><p><strong>Convolutional Layer, Pooling Layer, Fully-ConnectedLayer</strong></p><figure><img src="/typora-user-images/image-20230208144748173.png"alt="image-20230208144748173" /><figcaption aria-hidden="true">image-20230208144748173</figcaption></figure><h5 id="convolutional-layer">Convolutional Layer</h5><p><strong>filter</strong> with size like <spanclass="math inline">\(5\times 5\times 3\)</span></p><p>During the forward pass, we slide (more precisely, convolve) eachfilter across the width and height of the input volume and compute dotproducts between the entries of the filter and the input at anyposition</p><p><span class="math inline">\(\rightarrow\)</span> produce a separate2-dimensional activation map</p><p>The spatial extent of this connectivity: a hyperparameter called the<strong>receptive field</strong></p><p><strong>Depth, Stride, Zero-padding</strong></p><p><strong>Depth</strong>: depend on the number of filter. Called “deepcolumn” or “fiber”</p><p><strong>Stride</strong>: the number of pixel the filter will movewhen we slide it</p><p><strong>Zero-padding</strong>: pad the input volume with zeros aroundthe border</p><p>In math. the input volume size :<spanclass="math inline">\(W\)</span>, the receptive size of the Conv Layerneurons: <span class="math inline">\(F\)</span>, the stride with whichthey are applied: <span class="math inline">\(S\)</span>, the amount ofzero padding used: <span class="math inline">\(P\)</span></p><p>then the output: <spanclass="math inline">\((W-F+2P)/S+1\)</span></p><p><strong>Summary</strong></p><ul><li><p>Accept size: <span class="math inline">\(W_1\times H_1\timesD_1\)</span></p></li><li><p>Require:</p><ul><li>number of filters <span class="math inline">\(K\)</span></li><li>spatial extent <span class="math inline">\(F\)</span></li><li>stride <span class="math inline">\(S\)</span></li><li>the amount of zero padding <spanclass="math inline">\(P\)</span></li></ul></li><li><p>Produce size: <span class="math inline">\(W_2 \times H_2 \timesD_2\)</span></p><ul><li><span class="math inline">\(W_2=(W_1-F+2P)/S+1\)</span></li><li><span class="math inline">\(H_2=(H_1-F+2P)/S+1\)</span></li><li><span class="math inline">\(D_2=K\)</span></li></ul></li></ul><p>common set: <span class="math inline">\(F=3, S=1, P=1\)</span></p><p><strong>Backpropagation</strong> The backward pass for a convolutionoperation (for both the data and the weights) is also a convolution (butwith spatially-flipped filters).</p><p><strong><span class="math inline">\(1\times 1\)</span>convolution</strong> note that we have 3 channels. so it’s notmeaningless</p><p><strong>Dilated convolutions</strong> have filters that have spacesbetween each cell, called dilation.</p><p>##### Pooling layer</p><p>Common: Max</p><figure><img src="/typora-user-images/image-20230208152427080.png"alt="image-20230208152427080" /><figcaption aria-hidden="true">image-20230208152427080</figcaption></figure><ul><li><p>Accept size <span class="math inline">\(W_1\times H_1\timesD_1\)</span></p></li><li><p>Hyperparameters</p><ul><li>spatial extent <span class="math inline">\(F\)</span></li><li>stride <span class="math inline">\(S\)</span></li></ul></li><li><p>Produce size <span class="math inline">\(W_2\times H_2\timesD_2\)</span></p><ul><li><span class="math inline">\(W_2=(W_1-F)/S+1\)</span></li><li><span class="math inline">\(H_2=(H_1-F)/S+1\)</span></li><li><span class="math inline">\(D_2=D_1\)</span></li></ul></li></ul><p>Common: <span class="math inline">\(F=3, S=2\)</span> more commonly<span class="math inline">\(F=2, S=2\)</span></p><h5 id="normalization-layer">Normalization Layer</h5><p>These layers have since fallen out of favor because in practice theircontribution has been shown to be minimal, if any. For various types ofnormalizations, see the discussion in Alex Krizhevsky’s <ahref="http://code.google.com/p/cuda-convnet/wiki/LayerParams#Local_response_normalization_layer_(same_map)">cuda-convnetlibrary API</a>.</p><h5 id="fully-connected-layer">Fully-connected layer</h5><p>Just like Neural Network section</p><h5 id="converting-fully-connected-layers-to-conv-layers">ConvertingFully Connected layers to CONV layers</h5><p>Each of these conversions could in practice involve manipulating(e.g. reshaping) the weight matrix <spanclass="math inline">\(W\)</span> in each FC layer into CONV layerfilters. It turns out that this conversion allows us to “slide” theoriginal ConvNet very efficiently across many spatial positions in alarger image, in a single forward pass.</p><h4 id="convnet-architectures">ConvNet Architectures</h4><p>The most common pattern:</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clean">INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt; [FC -&gt; RELU]*K -&gt; FC<br></code></pre></td></tr></table></figure><p><code>N &gt;= 0</code> (and usually <code>N &lt;= 3</code>),<code>M &gt;= 0</code>, <code>K &gt;= 0</code> (and usually<code>K &lt; 3</code>).</p><ul><li><p><code>INPUT -&gt; FC</code>, implements a linearclassifier.</p></li><li><p><code>INPUT -&gt; CONV -&gt; RELU -&gt; FC</code></p></li><li><p><code>INPUT -&gt; [CONV -&gt; RELU -&gt; POOL]*2 -&gt; FC -&gt; RELU -&gt; FC</code>.Here we see that there is a single CONV layer between every POOLlayer.</p></li><li><p><code>INPUT -&gt; [CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; POOL]*3 -&gt; [FC -&gt; RELU]*2 -&gt; FC</code>Here we see two CONV layers stacked before every POOL layer.</p></li></ul><p><em>Prefer a stack of small filter CONV to one large receptive fieldCONV layer</em>.</p><h4 id="layer-sizing-patterns">Layer Sizing Patterns</h4><ul><li><strong>Input Layer</strong></li></ul><p>​ Should be divisible by 2 many times.</p><p>​ Ex. 32 (e.g. CIFAR-10), 64, 96 (e.g. STL-10), or 224 (e.g. commonImageNet ConvNets), 384, and 512.</p><ul><li><strong>Conv Layer</strong></li></ul><p>​ 3x3 or 5x5 Step <span class="math inline">\(S=1\)</span>, zero</p><ul><li><strong>Pool Layer</strong></li></ul><p>​ 2x2 , stride = 2(sometimes 3x3, stride = 2)</p><p><strong>Use stride of 1 in CONV</strong>:</p><ol type="1"><li>Smaller strides work better in practice.</li><li>Allows us to leave all spatial down-sampling to the POOL layers,with the CONV layers only transforming the input volume depth-wise.</li></ol><p><strong>Use padding</strong>:</p><p>If the CONV layers were to not zero-pad the inputs and only performvalid convolutions, then the size of the volumes would reduce by a smallamount after each CONV, and the <strong>information at theborders</strong> would be “washed away” too quickly.</p><p><strong>Compromising based on memory constrains</strong>:</p><p>people prefer to make the compromise at only the first CONV layer ofthe network. For example, one compromise might be to use a first CONVlayer with filter sizes of 7x7 and stride of 2 (as seen in a ZF net). Asanother example, an AlexNet uses filter sizes of 11x11 and stride of4.</p><h4 id="case-studies">Case Studies</h4><ul><li><strong>LeNet</strong></li><li><strong>AlexNet</strong></li><li><strong>ZF Net</strong></li><li><strong>GoogleNet</strong></li><li><strong>VGGNet</strong></li><li><strong>ResNet</strong></li></ul><p>VGG:</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs routeros">INPUT: [224x224x3]        memory:  224<span class="hljs-number">*224</span><span class="hljs-number">*3</span>=150K   weights: 0<br>CONV3-64: [224x224x64]  memory:  224<span class="hljs-number">*224</span><span class="hljs-number">*64</span>=3.2M   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*3</span>)<span class="hljs-number">*64</span> = 1,728<br>CONV3-64: [224x224x64]  memory:  224<span class="hljs-number">*224</span><span class="hljs-number">*64</span>=3.2M   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*64</span>)<span class="hljs-number">*64</span> = 36,864<br>POOL2: [112x112x64]  memory:  112<span class="hljs-number">*112</span><span class="hljs-number">*64</span>=800K   weights: 0<br>CONV3-128: [112x112x128]  memory:  112<span class="hljs-number">*112</span><span class="hljs-number">*128</span>=1.6M   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*64</span>)<span class="hljs-number">*128</span> = 73,728<br>CONV3-128: [112x112x128]  memory:  112<span class="hljs-number">*112</span><span class="hljs-number">*128</span>=1.6M   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*128</span>)<span class="hljs-number">*128</span> = 147,456<br>POOL2: [56x56x128]  memory:  56<span class="hljs-number">*56</span><span class="hljs-number">*128</span>=400K   weights: 0<br>CONV3-256: [56x56x256]  memory:  56<span class="hljs-number">*56</span><span class="hljs-number">*256</span>=800K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*128</span>)<span class="hljs-number">*256</span> = 294,912<br>CONV3-256: [56x56x256]  memory:  56<span class="hljs-number">*56</span><span class="hljs-number">*256</span>=800K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*256</span>)<span class="hljs-number">*256</span> = 589,824<br>CONV3-256: [56x56x256]  memory:  56<span class="hljs-number">*56</span><span class="hljs-number">*256</span>=800K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*256</span>)<span class="hljs-number">*256</span> = 589,824<br>POOL2: [28x28x256]  memory:  28<span class="hljs-number">*28</span><span class="hljs-number">*256</span>=200K   weights: 0<br>CONV3-512: [28x28x512]  memory:  28<span class="hljs-number">*28</span><span class="hljs-number">*512</span>=400K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*256</span>)<span class="hljs-number">*512</span> = 1,179,648<br>CONV3-512: [28x28x512]  memory:  28<span class="hljs-number">*28</span><span class="hljs-number">*512</span>=400K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*512</span>)<span class="hljs-number">*512</span> = 2,359,296<br>CONV3-512: [28x28x512]  memory:  28<span class="hljs-number">*28</span><span class="hljs-number">*512</span>=400K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*512</span>)<span class="hljs-number">*512</span> = 2,359,296<br>POOL2: [14x14x512]  memory:  14<span class="hljs-number">*14</span><span class="hljs-number">*512</span>=100K   weights: 0<br>CONV3-512: [14x14x512]  memory:  14<span class="hljs-number">*14</span><span class="hljs-number">*512</span>=100K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*512</span>)<span class="hljs-number">*512</span> = 2,359,296<br>CONV3-512: [14x14x512]  memory:  14<span class="hljs-number">*14</span><span class="hljs-number">*512</span>=100K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*512</span>)<span class="hljs-number">*512</span> = 2,359,296<br>CONV3-512: [14x14x512]  memory:  14<span class="hljs-number">*14</span><span class="hljs-number">*512</span>=100K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*512</span>)<span class="hljs-number">*512</span> = 2,359,296<br>POOL2: [7x7x512]  memory:  7<span class="hljs-number">*7</span><span class="hljs-number">*512</span>=25K  weights: 0<br>FC: [1x1x4096]  memory:  4096  weights: 7<span class="hljs-number">*7</span><span class="hljs-number">*512</span><span class="hljs-number">*4096</span> = 102,760,448<br>FC: [1x1x4096]  memory:  4096  weights: 4096<span class="hljs-number">*4096</span> = 16,777,216<br>FC: [1x1x1000]  memory:  1000 weights: 4096<span class="hljs-number">*1000</span> = 4,096,000<br><br>TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~<span class="hljs-number">*2</span> <span class="hljs-keyword">for</span> bwd)<br>TOTAL params: 138M parameters<br></code></pre></td></tr></table></figure><p><strong>Computational Considerations</strong></p><p>There are three major sources of memory to keep track of:</p><ul><li><p>From the intermediate volume sizes</p></li><li><p>From the parameter sizes</p></li><li><p>Every ConvNet implementation has to maintain<strong>miscellaneous</strong> memory, such as the image data batches,perhaps their augmented versions, etc.</p></li></ul><h3id="transfer-learning-and-fine-tuning-convolutional-neural-networks"><ahref="https://cs231n.github.io/transfer-learning/">Transfer Learning andFine-tuning Convolutional Neural Networks</a></h3><h4 id="transfer-learning">Transfer Learning</h4><figure><img src="/typora-user-images/image-20230210153134566.png"alt="image-20230210153134566" /><figcaption aria-hidden="true">image-20230210153134566</figcaption></figure>]]></content>
    
    
    
    <tags>
      
      <tag>notes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[iGEM] Modelling Group: introduction about Markdown and LaTeX</title>
    <link href="/2023/01/25/Modelling-Group-introduction-about-Markdown-and-LaTeX/"/>
    <url>/2023/01/25/Modelling-Group-introduction-about-Markdown-and-LaTeX/</url>
    
    <content type="html"><![CDATA[<h1 id="markdown">Markdown</h1><p>What is markdown?</p><ul><li>language(light)</li><li>easy to control</li></ul><h2 id="titles">Titles</h2><h1 id="im-1">I'm 1</h1><h2 id="im-2">I'm 2</h2><h3 id="im-3">I'm 3</h3><h4 id="im-4">I'm 4</h4><h5 id="im-5">I'm 5</h5><h6 id="im-6">I'm 6</h6><p class="heading" id="im-7">I'm 7</p><p>for title: 1-6, no 7</p><h2 id="bold-italic-bolditalic">bold, italic, bold&amp;italic</h2><p>I am a <em>sentence</em>. I like eat <strong>beef</strong>.</p><p>I am a <strong><em>student</em></strong>.</p><h2 id="links-photos">links, photos</h2><p><a href="https://www.baidu.com">Baidu</a></p><figure><img src="/img/me.jpg" alt="me" /><figcaption aria-hidden="true">me</figcaption></figure><h2 id="quote">quote</h2><blockquote><p>Lu Xun: people should eat.</p></blockquote><h2 id="list-2-types">list: 2 types</h2><ul><li>one<ul><li>two</li></ul></li><li>three<ul><li>four<ul><li>five</li></ul></li></ul></li></ul><ol type="1"><li>1</li><li>3</li></ol><p>if no " ":</p><p>1.123</p><h2 id="code">code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">sum</span>=<span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>  <span class="hljs-built_in">sum</span> += i<br></code></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs C"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span><br>&#123;<br>  balabala;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="table">table</h2><table><thead><tr class="header"><th style="text-align: left;">Header One</th><th style="text-align: left;">Header Two</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">Item One</td><td style="text-align: left;">Item Two</td></tr></tbody></table><h2 id="others">others</h2><p><del>I eat rubbish yesterday</del></p><p><u>this sentence is important</u></p><p>I must dosomething.<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="guess">[1]</span></a></sup></p><h2 id="math">math</h2><p>We all know that 2+3=5</p><p>we all know that <spanclass="math inline">\(1+\frac{1}{2}=\frac{3}{2}\)</span>。<spanclass="math inline">\(2^2=4\)</span>, <spanclass="math inline">\(CO_2\)</span>, <spanclass="math inline">\(x_{11}\)</span> <span class="math display">\[1+\frac{1}{2}\]</span></p><p><span class="math display">\[\begin{pmatrix}x_{11} &amp; x_{12} \\x_{21} &amp; x_{22}\end{pmatrix}\]</span></p><p><span class="math display">\[X=\sum^{n}_{i=1} \frac{\Pi_{j=1}^{m}x_{ij}}{\sigma_{i}k_i^2}\]</span></p><p><span class="math display">\[\begin{equation}x=\frac{\partial T}{\partial V}c_{Boltzman}\alpha_x\end{equation}\]</span></p><p><span class="math display">\[\begin{equation*}y=\frac{\partial T}{\partial M}c_{Boltzman}\alpha_y\end{equation*}\]</span></p><p><span class="math display">\[\begin{equation}z=\frac{\partial T}{\partial K}c_{Boltzman}\alpha_z\end{equation}\]</span></p><p><span class="math display">\[\begin{align}1&amp;=0+1\\2&amp;=1+1-0-1+1\\444&amp;=400+40+4\end{align}\]</span></p><p><span class="math display">\[\begin{align*}a+b+c&amp;=x\\x&amp;=a+b+c\end{align*}\]</span></p><p><span class="math display">\[A+B\rightarrow C\\\]</span> (of course not end…)</p><hr /><h1 id="latex">LaTex</h1><p><span class="math inline">\(\LaTeX\)</span></p><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>guess<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    
    <tags>
      
      <tag>teach</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello world</title>
    <link href="/2023/01/23/Hello-world/"/>
    <url>/2023/01/23/Hello-world/</url>
    
    <content type="html"><![CDATA[<h1 id="hello-world">HELLO WORLD</h1><h2 id="my-first-try-of-blog">My First Try of Blog</h2><p>Thanks to the help of Wang Zhili and Hu shukai, I managed to build myfirst blog today.</p><p>My blog will all in English, recording some courses notes and dailyfeelings.</p>]]></content>
    
    
    
    <tags>
      
      <tag>intro</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
