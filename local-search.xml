<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>[Record] 在Ubuntu22.04上配置qt5.12.3</title>
    <link href="/2023/05/25/%5BRecord%5D%20%E5%9C%A8Ubuntu22.04%E4%B8%8A%E9%85%8D%E7%BD%AEqt5.12.3/"/>
    <url>/2023/05/25/%5BRecord%5D%20%E5%9C%A8Ubuntu22.04%E4%B8%8A%E9%85%8D%E7%BD%AEqt5.12.3/</url>
    
    <content type="html"><![CDATA[<h1 id="record-在ubuntu22.04上配置qt5.13.2">[Record]在Ubuntu22.04上配置qt5.13.2</h1><p>因为要用qt制作ui界面完成数据结构大作业，因此在这里记录一下环境配置过程</p><p>环境：</p><ul><li>Linux(Ubuntu 22.04 LTS)</li></ul><h2 id="安装步骤">安装步骤</h2><ul><li>首先安装必要的工具</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo apt-get install build-essential libgl1-mesa-dev<br></code></pre></td></tr></table></figure><ul><li><p>进入<ahref="https://download.qt.io/archive/qt/5.13/5.13.2/">此链接</a></p></li><li><p>安装<ahref="https://download.qt.io/archive/qt/5.13/5.13.2/qt-opensource-linux-x64-5.13.2.run">qt-opensource-linux-x64-5.13.2.run</a></p></li><li><p>进入安装的路径，为安装程序赋予权限并运行</p></li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">chmod +x qt-opensource-linux-x64-5.13.2.run <br>sudo ./qt-opensource-linux-x64-5.13.2.run<br></code></pre></td></tr></table></figure><ul><li>一直点下一步，选择要安装的东西</li><li><figure><img src="/typora-user-images/image-20230525141446695.png"alt="image-20230525141446695" /><figcaption aria-hidden="true">image-20230525141446695</figcaption></figure></li><li>设置环境变量</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo vim /etc/profile<br></code></pre></td></tr></table></figure><p>​ 增加以下内容：</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs routeros"><span class="hljs-built_in">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-string">&quot;/opt/Qt5.13.2/Tools/QtCreator/bin:<span class="hljs-variable">$PATH</span>&quot;</span><br><span class="hljs-built_in">export</span> <span class="hljs-attribute">PATH</span>=<span class="hljs-string">&quot;/opt/Qt5.13.2/5.13.2/gcc_64/bin:<span class="hljs-variable">$PATH</span>&quot;</span><br></code></pre></td></tr></table></figure><ul><li>wq保存退出，修改环境变量：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">source /etc/profile<br></code></pre></td></tr></table></figure><p>安装成功。其中，QtCreator默认在以下目录中</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs awk"><span class="hljs-regexp">/opt/</span>Qt5.<span class="hljs-number">13.2</span><span class="hljs-regexp">/Tools/</span>QtCreator/bin<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>record</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[Human Parsing] SCHP</title>
    <link href="/2023/05/05/%5BHuman%20Parsing%5D%20SCHP/"/>
    <url>/2023/05/05/%5BHuman%20Parsing%5D%20SCHP/</url>
    
    <content type="html"><![CDATA[<h2 id="schp">SCHP</h2><blockquote><p>出发点：ground-truth中可能存在错误，影响训练。使用迭代优化的过程，不断同时提升labels和models</p></blockquote><figure><img src="\typora-user-images\image-20230505100544843.png"alt="image-20230505100544843" /><figcaption aria-hidden="true">image-20230505100544843</figcaption></figure><p>方法详细介绍：</p><p>开始给一个训练集，有图片<spanclass="math inline">\(\mathcal{X}\)</span>和不怎么精确的掩码<spanclass="math inline">\(\mathcal{Y}\)</span></p><p>目标是训练一个parser，通过<strong>模型聚合</strong>(modelaggregation)、<strong>标签提升</strong>(labelrefinement)、<strong>模型交互</strong>(interaction)。首先根据不怎么精确的标注对模型进行warm-upinitialization，然后对模型和掩码标注进行交替优化。</p><p>循环地聚合学习到的模型，以移动平均的方式细化掩码注释<spanclass="math inline">\(\mathcal{Y}\)</span>。具体来说，在某一个循环中，首先让模型从上一个被优化过的掩码中进行学习，然后通过参数化移动平均的操作把当前周期中学习到的模型权重和上一周期的模型权重进行聚合，然后用聚合后的模型来推断掩码，再通过像素移动平均操作来校正标注中的噪声，提升后的掩码标注将作为下一轮的ground-truth。</p><p>对每一个部分进行详细介绍：</p><ul><li><p>Warm-up Initialization</p><p>挺重要的</p></li><li><p>Online Model Aggregation</p><p>把不同周期得到的模型记为<spanclass="math inline">\(\{\theta_i\}_{i=1}^N\)</span> 设<spanclass="math inline">\(\theta\)</span>是在第<spanclass="math inline">\(i\)</span>轮学到的模型，上一轮使用的模型是<spanclass="math inline">\(\theta_{i-1}\)</span>，那么我们可以用移动平均的方式得到第<spanclass="math inline">\(i\)</span>轮使用的模型： <spanclass="math display">\[\theta_i=\frac{i}{i+1}\theta_{i-1}+\frac{1}{i+1}\theta\]</span>然而实验发现直接这样可能让模型表现更差，那是因为BN层在模型聚合之后参数不准确了，为了弥补这一点</p><blockquote><p>we forward all the training samples for one epoch to exactlyre-estimate the BatchNorm statistics in all BN layers as follows:</p></blockquote><p><span class="math display">\[m = (t-1)/t \\\mu_t = m\mu_{t-1}+(1-m)E[x_B] \\\sigma_t^2 = m \sigma_{t-1}^2+(1-m)Var[x_B]\]</span></p></li></ul><p>​ 其中<span class="math inline">\(t\)</span>是循环的次数，<spanclass="math inline">\(x_B\)</span>是BN层的输入特征。</p><p>​ 看起来很复杂，实际上基本没有增加复杂度。</p><ul><li><p>Online Label Refinement</p><p>把不同训练周期中得到的预测标签集表示为<spanclass="math inline">\(\{\mathcal{Y_i}\}_{i=1}^n\)</span> 跟先前类似：<span class="math display">\[\mathcal{Y_i} = \frac{i}{i+1}\mathcal{Y_{i-1}}+\frac{1}{i+1}\mathcal{Y}\]</span></p></li><li><p>Cyclical Training Strategy</p><p>为了满足在结束的时候有较小的学习率，在每一周期开始时有较大的学习率来走出局部极值，使用循环重启的余弦退火学习速率调度算法，设<spanclass="math inline">\(T\)</span>是每一个循环有的epochs数目，<spanclass="math inline">\(\eta_{min},\eta_{max}\)</span>表示结束、开始时的学习率</p><p>则： <span class="math display">\[\eta =\eta_{min}+\frac{1}{2}(\eta_{max}-\eta_{min})(1+\cos(\frac{T_{cur}}{T}\pi))\]</span></p></li></ul><p>伪代码：</p><p><img src="\typora-user-images\image-20230505154408196.png" alt="image-20230505154408196" style="zoom: 50%;" /></p>]]></content>
    
    
    
    <tags>
      
      <tag>notes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[Human Parsing] Papers</title>
    <link href="/2023/05/04/%E6%95%B4%E7%90%86/"/>
    <url>/2023/05/04/%E6%95%B4%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="human-parsing">Human Parsing</h1><p>一些主要矛盾：</p><ul><li>scale不同</li><li>降采样导致像素损失</li></ul><p>常见的方法</p><ul><li>联合姿态估计（关节什么的）s</li></ul><p>想法：</p><ul><li>有文章说在某一相似背景下训练出来的模型可能遇到新的背景就会识别的不好。假设ground-truth标的很对，可不可以就直接根据ground-truth先把人的轮廓扣出来，然后只对这一片地方进行特征提取，而把背景忽视或者直接用一些噪音替代，然后再其他操作，有没有可以减小背景的影响</li></ul><h2 id="atr">ATR</h2><p>将人体图像分解为语义时尚/身体区域。作者将其表述为一种主动模板回归（ATR）问题，其中每个时尚/身体项的归一化掩码表示为学习的掩码模板的线性组合，然后通过包括每个语义区域的位置、比例和可见性的主动形状参数变形为更精确的掩码。掩码模板系数和主动形状参数共同可以生成人体解析结果，并因此称为人体解析的结构输出。深度卷积神经网络（CNN）用于构建输入人体图像和人体解析结构输出之间的端到端关系。更具体地说，结构输出由两个单独的网络预测。第一个CNN网络带有最大池化，并设计用于预测每个标签掩码的模板系数，而第二个网络没有最大池化以保留对标签掩码位置的敏感性并准确预测主动形状参数。对于新图像，两个网络的结构输出被融合以生成每个像素的每个标签的概率，最后使用超像素平滑来细化人体解析结果。在大型数据集上进行全面评估，充分证明了ATR框架相对于其他人体解析技术的显着优越性。特别是，我们的ATR框架通过F1分数达到64.38％，显着高于基于最先进算法[28]的44.76％</p><figure><img src="\typora-user-images\image-20230403154837858.png"alt="image-20230403154837858" /><figcaption aria-hidden="true">image-20230403154837858</figcaption></figure><h2 id="m-cnn">M-CNN</h2><p>Matching-CNN Meets KNN: Quasi-Parametric HumanParsing是一篇关于人体解析的论文，由Si Liu等人在2015年发表于IEEECVPR上。该论文提出了一种基于KNN的准参数人体解析框架，该框架利用MatchingConvolutional Neural Network(M-CNN)来预测测试图像中与KNN图像中特定语义区域最佳匹配区域的匹配置信度和位移。该框架的目标是将传统的非参数方法和参数方法相结合，既能从注释数据中获得监督，又能灵活地使用新注释的数据。该论文在大型数据集上进行了全面评估，并证明了准参数模型相对于现有技术的显著性能提升</p><figure><img src="\typora-user-images\image-20230403155021588.png"alt="image-20230403155021588" /><figcaption aria-hidden="true">image-20230403155021588</figcaption></figure><h2 id="co-cnn">Co-CNN</h2><p>该论文提出了一种新的Contextualized Convolutional Neural Network(Co-CNN)架构，该架构将跨层上下文、全局图像级上下文、超像素内部上下文和超像素交叉邻域上下文集成到一个统一的网络中，用于解决人体解析任务。Co-CNN能够在端到端的方式下对输入的人体图像进行像素级别的分类，并取得了优秀的性能</p><figure><img src="\typora-user-images\image-20230403155315028.png"alt="image-20230403155315028" /><figcaption aria-hidden="true">image-20230403155315028</figcaption></figure><h2 id="schp">SCHP</h2><p>该论文提出了一种名为Self-Correction for Human Parsing(SCHP)的噪声容忍方法，用于逐步提高监督标签和学习模型的可靠性和准确性。SCHP是一种模型无关的方法，可以应用于任何人体解析模型以进一步提高其性能</p><figure><img src="\typora-user-images\image-20230403155737493.png"alt="image-20230403155737493" /><figcaption aria-hidden="true">image-20230403155737493</figcaption></figure><h2 id="slrs">SLRS</h2><p>该论文提出了一种名为Self-Learning with Rectification(SLR)的方法，用于解决人体解析任务中样本不足的问题。SLR方法通过自学习策略生成伪标签来重新训练模型，但直接使用噪声伪标签会导致误差放大和积累。因此，SLR方法引入了一种循环学习调度程序来推断更可靠的伪标签，并设计了一种去噪学习和半监督学习相结合的策略，以进一步提高模型性能</p><figure><img src="\typora-user-images\image-20230403160712629.png"alt="image-20230403160712629" /><figcaption aria-hidden="true">image-20230403160712629</figcaption></figure><figure><img src="\typora-user-images\image-20230403160744026.png"alt="image-20230403160744026" /><figcaption aria-hidden="true">image-20230403160744026</figcaption></figure><h2 id="pcnet">PCNet</h2><p>PCNet方法主要由三个模块组成，包括部分类别模块、关系聚合模块和关系分散模块。其中，部分类别模块用于生成部分类别特定的特征图，关系聚合模块用于聚合全局和局部上下文信息，而关系分散模块则用于将全局和局部上下文信息分散到各个部位<imgsrc="\typora-user-images\image-20230403160954024.png"alt="image-20230403160954024" /></p><h2 id="aog">AOG</h2><p>该论文提出了一种名为Attribute And-Or Grammar(A-AOG)的模型，用于在带有属性的解析图中联合推断人体姿势和人体属性。与当前文献中其他流行方法训练单独的姿势和个体属性分类器不同，该模型将属性增强到分层表示中的节点中，从而实现了对人体姿势、部位和属性的联合解析</p><ul><li>Phrase structure grammar: 表示人体从整体到部分的层次分解</li><li>Dependency grammar: 通过身体姿态的运动图来建模几何关节</li><li>Attribute grammar: 考虑不同部分兼容性关系，使得他们遵循一致风格</li></ul><p><img src="\typora-user-images\image-20230327191937546.png" alt="image-20230327191937546" style="zoom:67%;" /></p><h2 id="pgn">PGN</h2><p>不要检测部分了</p><p>两个部分：1)语义部分分割，将每个像素分配为人类的一个部分（如脸部、手臂）；2)实例意识的边缘检测，将语义部分分成不同的人物实例</p><p>之后两者互相完善</p><figure><img src="\typora-user-images\image-20230403180113620.png"alt="image-20230403180113620" /><figcaption aria-hidden="true">image-20230403180113620</figcaption></figure><h2 id="mula">MuLA</h2><p>这篇论文提出了一种新的 Mutual Learning to Adapt 模型 (MuLA)用于联合人体解析和姿态估计。它有效地利用了两个任务之间的相互利益，同时提高了它们的性能。与现有的后处理或基于多任务学习的方法不同，MuLA通过反复利用其并行任务的指导信息来预测动态任务特定模型参数。因此，MuLA可以快速适应解析和姿态模型，通过将其对应部分的信息纳入更强大的表示中，提供更强大、更健壮和更准确的结果。MuLA是使用卷积神经网络实现的，并且是端到端可训练的。在基准 LIP 和扩展PASCAL-Person-Part 上进行了全面实验，证明了所提出的 MuLA模型具有优越的性能，优于现有的基准模型</p><figure><img src="\typora-user-images\image-20230403181119605.png"alt="image-20230403181119605" /><figcaption aria-hidden="true">image-20230403181119605</figcaption></figure><h2 id="hazn">HAZN</h2><p>适应不同局部规模的物体或部分</p><p>HAZN是两个“Auto-Zoom Nets”的组合，使用FCN完成两个任务：</p><ul><li>预测目标的位置和规模</li><li>为预测的目标区域估计部分分数</li><li><figure><img src="\typora-user-images\image-20230404144905334.png"alt="image-20230404144905334" /><figcaption aria-hidden="true">image-20230404144905334</figcaption></figure></li></ul><h2 id="lg-lstm">LG-LSTM</h2><figure><img src="\typora-user-images\image-20230404145934436.png"alt="image-20230404145934436" /><figcaption aria-hidden="true">image-20230404145934436</figcaption></figure><h2 id="mh-parser">MH-Parser</h2><p>MH-Parser使用新的Graph-GAN模型以自下而上的方式同时生成全局解析映射和人物实例掩码</p><figure><img src="\typora-user-images\image-20230404152445268.png"alt="image-20230404152445268" /><figcaption aria-hidden="true">image-20230404152445268</figcaption></figure><blockquote><p>不是很懂</p></blockquote><h2 id="ss-jppnetlipssl">SS-JPPNet(LIP)（SSL）</h2><blockquote><p>我们将关节结构损失作为分割损失的权重，分割损失成为我们的结构敏感损失。</p></blockquote><p><img src="\typora-user-images\image-20230405150631885.png"alt="image-20230405150631885" /> <span class="math display">\[L_{Joint} = \frac{1}{2N}\sum_{i=1}^n||c_i^p-c_i^{gt}||_2^2 \\L_{Structure} = L_{Joint}\cdot L_{Parsing}\]</span></p><blockquote><p>好奇怪，为什么损失函数是直接乘起来？</p></blockquote><h2 id="refinenet">RefineNet</h2><p>通用多路径优化网络，明确地利用下采样过程中的所有可用信息，使用远程残留连接实现高分辨率预测</p><p>引入了链式残差池化</p><p>比较图：</p><figure><img src="\typora-user-images\image-20230405152632909.png"alt="image-20230405152632909" /><figcaption aria-hidden="true">image-20230405152632909</figcaption></figure><p>RefineNet:</p><figure><img src="\typora-user-images\image-20230405152658463.png"alt="image-20230405152658463" /><figcaption aria-hidden="true">image-20230405152658463</figcaption></figure><h2 id="holistic-instance-level-human-parsing">Holistic, Instance-levelHuman Parsing</h2><p>关于多人的</p><figure><img src="\typora-user-images\image-20230405154613794.png"alt="image-20230405154613794" /><figcaption aria-hidden="true">image-20230405154613794</figcaption></figure><h2id="cross-domain-human-parsing-via-adversarial-feature-and-label-adaptation">Cross-DomainHuman Parsing via Adversarial Feature and Label Adaptation</h2><blockquote><p>我们提出了一种新的、高效的跨领域人类解析模型，以弥合跨领域在视觉外观和环境条件方面的差异，充分利用跨领域的共性。</p><p>To this end, we propose a novel and efficient cross-domain humanparsing model to bridge the cross-domain differences in terms of visualappearance and environment conditions and fully exploit commonalitiesacross domains.</p></blockquote><blockquote><p>什么是cross-domain? A:似乎就是应用于情况不大一样的场景比如背景很不一样什么的</p></blockquote><p>A discriminative feature adversarial network:一个特征判别对抗网络，对特征补偿进行监督，减小了两个域特征分布的差异</p><p>A structured label adversarial network:一个结构化标签对抗网络，引导目标域的分割结果遵循域间共享的结构化标签的高阶关系(???说人话)</p><figure><img src="\typora-user-images\image-20230405160643047.png"alt="image-20230405160643047" /><figcaption aria-hidden="true">image-20230405160643047</figcaption></figure><blockquote><p>呃呃，这篇文章完全不懂</p></blockquote><h2 id="jppnet">JPPNet</h2><figure><img src="\typora-user-images\image-20230405161909105.png"alt="image-20230405161909105" /><figcaption aria-hidden="true">image-20230405161909105</figcaption></figure><h2 id="mman">MMAN</h2><p>宏观-微观对抗网络</p><p>其中一个识别器MacroD作用于低分辨率的标签地图，并惩罚语义不一致，例如，错位的身体部位。另一个鉴别器MicroD侧重于高分辨率标签地图的多个补丁，以解决局部不一致性，如模糊和空洞</p><p>开源 https://github.com/RoyalVane/MMAN</p><figure><img src="\typora-user-images\image-20230405162714939.png"alt="image-20230405162714939" /><figcaption aria-hidden="true">image-20230405162714939</figcaption></figure><h2 id="nanmhp-dataset">NAN(MHP dataset)</h2><blockquote><p>NAN由三个类似生成对抗网络(Generative Adversarial Network,GAN)的子网组成，分别执行语义显著性预测、实例不可知解析和实例感知聚类。</p><p>NAN consists of three Generative Adversarial Network (GAN)-likesub-nets, respectively performing semantic saliency prediction,instance-agnostic parsing and instance-aware clustering.</p></blockquote><p>这些子网形成了一个嵌套的结构，并经过精心设计，以端到端方式共同学习</p><figure><img src="\typora-user-images\image-20230405164137662.png"alt="image-20230405164137662" /><figcaption aria-hidden="true">image-20230405164137662</figcaption></figure><p>(???)</p><h2 id="ce2p">CE2P</h2><p>在本文中，我们确定了几个有用的属性，包括特征解析、全局上下文信息和边缘细节，并进行严格的分析，以揭示如何利用它们来帮助人工解析任务</p><figure><img src="\typora-user-images\image-20230405164545542.png"alt="image-20230405164545542" /><figcaption aria-hidden="true">image-20230405164545542</figcaption></figure><blockquote><p>我对边缘检测还不是很了解，不知道为什么可以这样</p></blockquote><h2 id="spreid">SPReID</h2><figure><img src="\typora-user-images\image-20230405170918658.png"alt="image-20230405170918658" /><figcaption aria-hidden="true">image-20230405170918658</figcaption></figure><h2 id="jointmpe">JointMPE</h2><p>共同解决姿势估计和语义部分分割</p><p>训练了两个FCN：Pose FCN and PartFCN提供对姿势关节和语义的初始估计。然后用fully-connected conditionalrandomfield(FCRF)来融合它们，这里面一个全新的语义-关节平滑操作被实施来提升两者的consistency。</p><p>为了refine part segments，被上面refine过后的东西被第二个partFCN融合。为了减小FCRF的complexity，引入了人体检测框加速。</p><figure><img src="\typora-user-images\image-20230408155051444.png"alt="image-20230408155051444" /><figcaption aria-hidden="true">image-20230408155051444</figcaption></figure><h2 id="corrpm">CorrPM</h2><p>人体的语义边缘和关键点位置如何共同提升human parsing</p><p>相比特征拼接，揭示相关性似乎更好</p><p>提出CorrPM来揭示这种相关性，使用heterogeneous non-local block来从边缘、姿势和parsing的特征映射中揭示空间相关性</p><p><img src="\typora-user-images\image-20230408160751778.png" alt="image-20230408160751778" style="zoom:67%;" /></p><p><img src="\typora-user-images\image-20230408160816684.png" alt="image-20230408160816684" style="zoom:67%;" /></p><h2 id="cdcl">CDCL</h2><blockquote><p>我们提出的方法利用真实数据的丰富和真实变化，以及合成数据的易于获得的标签，在没有任何人类标注标签的情况下，学习对真实图像的多人部分分割。</p></blockquote><figure><img src="\typora-user-images\image-20230408163811916.png"alt="image-20230408163811916" /><figcaption aria-hidden="true">image-20230408163811916</figcaption></figure><p><img src="\typora-user-images\image-20230408163855577.png" alt="image-20230408163855577" style="zoom:67%;" /></p><h2 id="dpc">DPC</h2><p>用meta-learning(???)</p><p>跟deeplab有关系</p><h2 id="snt">SNT</h2><p>一种树结构，multiple semantic sub-regions in a hierarchical way</p><p>然后用semnatic aggregation module 来 combine multiple hierarchicalfeatures</p><p><img src="\typora-user-images\image-20230410103703390.png" alt="image-20230410103703390" style="zoom:67%;" /></p><figure><img src="\typora-user-images\image-20230410104024948.png"alt="image-20230410104024948" /><figcaption aria-hidden="true">image-20230410104024948</figcaption></figure><h2 id="nppnet">NPPNet</h2><p>结合human parsing and pose estimation</p><figure><img src="\typora-user-images\image-20230410104825965.png"alt="image-20230410104825965" /><figcaption aria-hidden="true">image-20230410104825965</figcaption></figure><p><img src="\typora-user-images\image-20230410105025348.png" alt="image-20230410105025348" style="zoom: 67%;" /></p><h2 id="cnif-prhp">CNIF-PRHP</h2><p>分析三种相关的过程：</p><ul><li>direct inference</li><li>bottom-up inference</li><li>top-down inference</li></ul><p>assimilating generic message-passing networks with their edge-typed,convolutional counterparts</p><p><img src="\typora-user-images\image-20230410110931174.png" alt="image-20230410110931174" style="zoom:80%;" /></p><p><img src="\typora-user-images\image-20230410111019453.png" alt="image-20230410111019453" style="zoom:50%;" /></p><p><img src="\typora-user-images\image-20230410111054943.png" alt="image-20230410111054943" style="zoom:50%;" /></p><figure><img src="\typora-user-images\image-20230410111124193.png"alt="image-20230410111124193" /><figcaption aria-hidden="true">image-20230410111124193</figcaption></figure><h2 id="bgnet">BGNet</h2><p>BGNet exploits the inherent hierarchical structure of a human bodyand the relationship of different human parts by means of grammar rulesin both cascaded and paralleled manner.</p><p>We also design a Part-aware Convolutional Recurrent Neural Network(PCRNN) to pass messages which are generated by grammar rules.</p><figure><img src="\typora-user-images\image-20230410112131772.png"alt="image-20230410112131772" /><figcaption aria-hidden="true">image-20230410112131772</figcaption></figure><h2 id="gwnet">GWNet</h2><p>在前面那个上加了一个Wavelet Prediction Module</p><figure><img src="\typora-user-images\image-20230410113546364.png"alt="image-20230410113546364" /><figcaption aria-hidden="true">image-20230410113546364</figcaption></figure><h2 id="wshp">WSHP</h2><p>pose 结构</p><p>transfer</p><figure><img src="\typora-user-images\image-20230411155830125.png"alt="image-20230411155830125" /><figcaption aria-hidden="true">image-20230411155830125</figcaption></figure><p><img src="\typora-user-images\image-20230411160205740.png" alt="image-20230411160205740" style="zoom:50%;" /></p><h2 id="braidnet">BraidNet</h2>]]></content>
    
    
    
    <tags>
      
      <tag>notes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[Record] 令人抓狂的docker</title>
    <link href="/2023/05/04/%5BRecord%5D%20%E4%BB%A4%E4%BA%BA%E6%8A%93%E7%8B%82%E7%9A%84docker/"/>
    <url>/2023/05/04/%5BRecord%5D%20%E4%BB%A4%E4%BA%BA%E6%8A%93%E7%8B%82%E7%9A%84docker/</url>
    
    <content type="html"><![CDATA[<p>搞了老半天docker，仍然无法把dockerdesktop和root下的docker联系起来。好在在终端的root权限下终于可以使用带gpu的镜像了。稍微记录一下，防止摆烂一段时间后就忘记了。</p><p>安装docker：</p><p>跟着官网文档<ahref="https://docs.docker.com/engine/install/ubuntu/">这里</a>配。</p><p>为了使用gpu，还要跟着<ahref="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#install-guide">这个链接</a>配置ContainerToolkit</p><p>因为我的整个配置过程非常混乱，不确定是否这两个就够了，等我哪天重装系统了再来看看有没有要加的（叹气</p><p>然后就是使用</p><p>首先拉一个带gpu的镜像下来，比如</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker pull nvidia/cuda:11.6.2-base-ubuntu20.04<br></code></pre></td></tr></table></figure><p>然后创造容器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker run -it --gpus all --name myubuntu nvidia/cuda:11.6.2-base-ubuntu20.04<br></code></pre></td></tr></table></figure><p>参数：</p><ul><li><strong>-i</strong>: 交互式操作。</li><li><strong>-t</strong>: 终端。</li></ul><p>如果要退出，直接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">exit<br></code></pre></td></tr></table></figure><p>如果想让容器后台运行，用-d</p><p>想要开始、停止、重启一个容器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker start &lt;容器&gt;<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker stop &lt;容器&gt;<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker restart &lt;容器&gt;<br></code></pre></td></tr></table></figure><p>进入、离开（不关闭）容器：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker attach &lt;容器&gt;<br></code></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">docker exec ...<br></code></pre></td></tr></table></figure><p>更多操作可以参考<ahref="https://www.runoob.com/docker/docker-container-usage.html">菜鸟教程</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>record</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[Information Theory] 复习时的一些心得理解</title>
    <link href="/2023/04/25/%5BInformation%20Theory%5D%20%E5%A4%8D%E4%B9%A0%E6%97%B6%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BF%83%E5%BE%97/"/>
    <url>/2023/04/25/%5BInformation%20Theory%5D%20%E5%A4%8D%E4%B9%A0%E6%97%B6%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BF%83%E5%BE%97/</url>
    
    <content type="html"><![CDATA[<h1 id="information-theory-复习时的一些心得">[Information Theory]复习时的一些心得</h1><h2 id="chapter-2">Chapter 2</h2><h3 id="一些定义">一些定义</h3><h4 id="信息熵联合熵条件熵">信息熵、联合熵、条件熵</h4><p>对信息熵的定义的理解：</p><ul><li><p>为什么要用log？</p><ul><li><p>对于某个概率为p的东西，我们想找一个东西描述它的信息，要满足以下条件：</p></li><li><blockquote><p>非负</p><p>如果概率是1，信息量为0</p><p>函数连续(这样比较好)</p><p>如果两件事独立，一起发生的信息是单独发生的信息之和</p></blockquote><p>这样就可以得出<span class="math inline">\(I(p) \proptolog\frac{1}{p}\)</span></p></li></ul></li><li><p>知道这个之后，<spanclass="math inline">\(H\)</span>的定义是怎么出来的？</p><ul><li>刚才讨论的是对于<span class="math inline">\(p\)</span>的，对于<spanclass="math inline">\(H\)</span>，就是看一个随机变量，它具有一定的概率分布，我们要求它的信息。那么显然就可以理解为它对于刚在给定概率下信息的期望(相当于求个和或者说积个分，把每个<spanclass="math inline">\(p\)</span>下贡献的信息累加起来)</li><li>因此我们有了</li></ul><p><span class="math display">\[H(X) = \mathbb{E}_plog\frac{1}{p(x)}\]</span></p><hr /></li></ul><p>理解了上面所说，联合熵、条件熵也就不难理解了，不过是看联合概率、条件概率罢了</p><hr /><h4 id="信息熵的链式法则">信息熵的链式法则</h4><p>对于Chain rule的理解：</p><ul><li>理解一下<span class="math inline">\(H(X,Y) = H(X) +H(X|Y)\)</span>即可</li><li>想看看X，Y的信息，我们先知道了X的信息，此时由于X、Y之间可能存在一定的相关性，我们对Y已经有了一点点的了解，剩下的关于X，Y的信息即为H(X|Y)</li><li>跟同学交流时，他说有点像是线性代数中的Cramer法则？就好像分解成一些正交空间一样。有点感觉。</li></ul><p>根据上面的第二条，似乎互信息用<span class="math inline">\(I(X,Y) =H(X) - H(X|Y)\)</span>来表示是很显然的一件事了</p><hr /><h4 id="kl散度相对熵">KL散度（相对熵）</h4><p>首先我们看看公式 <span class="math display">\[D(p||q) = \sum_{x\in\mathcal{X}}p(x)log\frac{p(x)}{q(x)}  =\mathbb{E}_plog\frac{p(X)}{q(X)}\]</span> 为了延续上面的理解方式，可以写成 <span class="math display">\[D(p||q) = \mathbb{E}_p(log\frac{1}{q(X)}-log\frac{1}{p(X)})\]</span>E中的第一项似乎是在一个给定值下q的信息，第二项是p的信息，好像就是信息差。而这件事是对p的期望，似乎是从p的角度看待这件事的一般，从p的角度看两者的差距。</p><p>在网上找到了一种我觉得很妙的解释：</p><blockquote><p>假设P(X=0)=0.8，P(X=1)=0.2，那么我们可以用0来编码消息0，用10来编码消息1，这样平均每个消息的编码长度为：L(P) = 0.8 * 1 + 0.2 * 2 = 1.2 bits 这也等于X的熵H(P)。但是如果我们不知道X的真实概率分布P(X)，而只知道一个近似的概率分布Q(X)，那么我们可能会用不同的编码方案来传输消息。例如，假设Q(X=0)=0.5，Q(X=1)=0.5，那么我们可能会用01来编码消息0，用11来编码消息1，这样平均每个消息的编码长度为：L(Q) = 0.8 * 2 + 0.2 * 2 = 2 bits 这也等于X的交叉熵H(P,Q)。可以看出，由于我们使用了一个不准确的概率分布Q(X)来编码消息，导致了平均每个消息的编码长度增加了0.8bits，这就是用Q(X)来近似P(X)所造成的信息损失。这个信息损失就是KL散度D(P||Q)：D(P||Q) = L(Q) - L(P) = H(P,Q) - H(P) = 0.8 bits<strong>也就是说，KL散度衡量了如果我们使用Q(X)来代替P(X)，那么每个消息需要额外传输多少比特数。</strong></p></blockquote><blockquote><p>回头再验证一下</p></blockquote><p>后来我们知道，KL散度是肯定大于0的，但是显然每个点两者信息的大小关系有可能有大有小，而最后大于0，反映了一种最优性和必然性，回头再看看。</p><hr /><h4 id="互信息和dl-散度的关系">互信息和DL-散度的关系</h4><p><span class="math display">\[I(X,Y) = D[p(X,Y)||p(X)p(Y)]\]</span></p><p>这又很神奇，后面证明正的时候再看看</p><hr /><h4 id="条件互信息">条件互信息</h4><p><span class="math display">\[I(X;Y|Z) = H(X| Z) - H(X|Y, Z)\]</span></p><p>这可以从信息增益的角度理解，也可以写成<spanclass="math inline">\(\mathbb{E}\)</span>的形式从KL-D的角度理解</p><hr /><h4 id="互信息的链式法则">互信息的链式法则</h4><p><span class="math display">\[I(X_1,X_2\cdots X_n;Y) = \sum_{i=1}^{n}I(X_i;Y|X_{i-1},\cdots,X_1)\]</span></p><p>理解上，跟信息熵的链式法则一样，只不过之前的信息现在变成了和Y之间的互信息</p><hr /><h4 id="条件相对熵">条件相对熵</h4><p><span class="math display">\[D[p(y|x) || q(y|x)] = \mathbb{E}_{x,y}log\frac{p(Y|X)}{q(Y|X)}\]</span></p><hr /><h4 id="相对熵的链式法则">相对熵的链式法则</h4><p><span class="math display">\[D[p(x, y)||q(x,y)] = D[p(x)||q(x)] + D[p(y|x)||q(y|x)]\]</span></p><p>这个要怎么很直观地理解？</p><blockquote><p>相对熵可以看作是两个分布之间的差异或不相似度的度量，而链式法则可以看作是一种将这种差异分解为局部差异和条件差异的方法。</p></blockquote><h3 id="一些推论">一些推论</h3><p>首先，教材引入了Jensen不等式，这在几何上非常容易理解，不赘述</p><p>结论： <span class="math display">\[f(x): convex\;\Rightarrow\;\mathbb{E}f(X) \ge f(\mathbb{E}X)\]</span> 以此证明 <span class="math display">\[D[p||q]\ge 0\]</span></p><p>数学上很好证，讲讲理解 <span class="math display">\[H(X) = \sum_xp(x)log\frac{1}{p(x)}\]</span></p><p><span class="math display">\[D[p(x)||q(x)] = \sum_x p(x)[log\frac{1}{q(x)}-log\frac{1}{p(x)}]\]</span></p><p>我们盯着这两个式子，<spanclass="math inline">\(log\frac{1}{p(x)}\)</span>随着<spanclass="math inline">\(p(x)\)</span>的增大而减小，是否就是说，在概率大的时候我们要挑选一个小一点的东西对它进行表示，概率小的时候用大一点的东西对它进行表示，那么最后我们总共的消耗比较小（编码的角度）</p><p>而对于KL散度，我们已经知道了<spanclass="math inline">\(p(x)\)</span>，那么对于每个概率点，用<spanclass="math inline">\(log\frac{1}{p(x)}\)</span>来表示应该是最优的，但是如果我们偏偏要用<spanclass="math inline">\(log\frac{1}{q(x)}\)</span>来对每个概率点表示，比如我们在P中有很大概率的时候，用了Q中有很小概率的东西进行了表示，那就造成了很大的损失，导致KL散度增加。每个点的改变的程度就是[]中的式子相减。因为函数的性质，总体加起来肯定会变大，这更说明了原来的表示的最优性。</p><hr /><h4 id="相对熵的凸性">相对熵的凸性</h4><p><span class="math display">\[D(\lambda p_1+(1-\lambda)p_2||\lambda q_1+(1-\lambda)q_2)\le \lambdaD(p_1||q_1)+(1-\lambda)D(p_2||q_2)\]</span></p><p>怎么理解？？？</p><hr /><p>Q：单个映射是不是也可以看成一个MC？</p>]]></content>
    
    
    
    <tags>
      
      <tag>notes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[Record] 配置PyQt5 (Linux系统) </title>
    <link href="/2023/04/16/%5BRecord%5D%E9%85%8D%E7%BD%AEPyQt5%20(Linux%20%E7%B3%BB%E7%BB%9F)/"/>
    <url>/2023/04/16/%5BRecord%5D%E9%85%8D%E7%BD%AEPyQt5%20(Linux%20%E7%B3%BB%E7%BB%9F)/</url>
    
    <content type="html"><![CDATA[<h1 id="record配置pyqt5-linux-系统">[Record]配置PyQt5 (Linux 系统)</h1><blockquote><p>在Windows环境忙活了一下午都找不到东西，于是尝试使用Linux，果然方便快捷</p></blockquote><ul><li><p>系统：Ubuntu22.04</p></li><li><p>First, create a new environment in anaconda</p></li></ul><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">conda create <span class="hljs-literal">-n</span> pyqt python=<span class="hljs-number">3.8</span><br></code></pre></td></tr></table></figure><ul><li>Go into the environment</li></ul><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">conda activate pyqt<br></code></pre></td></tr></table></figure><ul><li>get pyqt5</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install pyqt5<br></code></pre></td></tr></table></figure><ul><li>Install Qt Designer</li></ul><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs powershell">sudo apt<span class="hljs-literal">-get</span> install qttools5<span class="hljs-literal">-dev-tools</span><br>sudo apt<span class="hljs-literal">-get</span> install qttools5<span class="hljs-literal">-dev</span><br></code></pre></td></tr></table></figure><ul><li>Open the Qt Designer</li></ul><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs powershell"><span class="hljs-built_in">cd</span> /usr/lib/x86_64<span class="hljs-literal">-linux-gnu</span>/qt5/bin/ <br>./designer<br></code></pre></td></tr></table></figure><ul><li>Edit the file and save it</li><li>Transform the ui into python</li></ul><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">pyuic5 path/to/helloworld.ui <span class="hljs-literal">-o</span> path/to/helloworld.py<br></code></pre></td></tr></table></figure><p>上面的命令生成的代码只是一些定义的函数，并没有调用，执行之后并不会有任何界面显示，需要在另外的python文件中进行import之后调用；如果想要在单独一个文件中执行并显示图形结果，可以使用</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs powershell">pyuic5 <span class="hljs-literal">-x</span> path/to/helloworld.ui <span class="hljs-literal">-o</span> path/to/helloworld.py<br></code></pre></td></tr></table></figure><blockquote><p>感谢来自csdn的文章<ahref="https://blog.csdn.net/ayiya_Oese/article/details/116299610">链接</a></p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>record</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[Semantic Segmentation] Some Classic network for Semantic Segmentation</title>
    <link href="/2023/03/09/Semantic%20segmentation/"/>
    <url>/2023/03/09/Semantic%20segmentation/</url>
    
    <content type="html"><![CDATA[<h1 id="semantic-segmentation">Semantic Segmentation</h1><ul><li>FCN</li><li>U-Net</li><li>PSPNet</li><li>Deeplab v1-3(+)</li></ul><h2 id="fcn">FCN</h2><blockquote><p>Our key insight is to build <strong>“fullyconvolutional”networks</strong> that take input of <strong>arbitrarysize</strong> and produce correspondingly-sized output with efficientinference and learning.</p></blockquote><blockquote><p>We then define <strong>a skip architecture</strong> that<strong>combines semantic information from a deep, coarse layer</strong>with appearance information from a shallow, fine layer to produceaccurate and detailed segmentations.</p></blockquote><p>Structure:</p><figure><img src="/typora-user-images/image-20230313202318039.png"alt="Structure of FCN" /><figcaption aria-hidden="true">Structure of FCN</figcaption></figure><p>The FCN will choose a classify network as backbone (e.g. VGGNet,ResNet). It abandon FC and replace them with Conv.</p><h4 id="details">Details:</h4><h2 id="unet">UNet</h2><p><strong>Structure:</strong></p><figure><img src="/typora-user-images/image-20230325195356603.png"alt="Structure of UNet" /><figcaption aria-hidden="true">Structure of UNet</figcaption></figure><h2 id="pspnet">PSPNet</h2><p><strong>Structure:</strong></p><figure><img src="/typora-user-images/image-20230325195446997.png"alt="Structure of PSPNet" /><figcaption aria-hidden="true">Structure of PSPNet</figcaption></figure>]]></content>
    
    
    
    <tags>
      
      <tag>paper</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[DL] Basic-Net</title>
    <link href="/2023/02/19/%5BDL%5DBasic-Net/"/>
    <url>/2023/02/19/%5BDL%5DBasic-Net/</url>
    
    <content type="html"><![CDATA[<h1 id="nets">Nets</h1><h2 id="杂七杂八的介绍">杂七杂八的介绍</h2><ul><li>LeNet 1994</li></ul><blockquote><p>是早期（1994年）的神经网络之一，用于手写数字识别，由卷积层，池化层，全连接层组成，网络结构如下图所示</p><figure><img src="/typora-user-images/image-20230313201432146.png"alt="Structure of LeNet" /><figcaption aria-hidden="true">Structure of LeNet</figcaption></figure></blockquote><ul><li>AlexNet 2012</li></ul><blockquote><p>是首个实用性很强的卷积神经网络由卷积操，池化层，全连接层，<strong>softmax层</strong>以及<strong>ReLU、Dropout</strong>构成。首次提出在2012年的ILSVRC大规模视觉识别竞赛上。其网络结构如下图所示：</p><figure><img src="/typora-user-images/image-20230313201406795.png"alt="Structure of AlexNet" /><figcaption aria-hidden="true">Structure of AlexNet</figcaption></figure></blockquote><ul><li>VGGNet 2014</li></ul><blockquote><p>VGGNet出现在2014年ILSVRC上比赛上获得了分类项目的第二名和定位项目的第一名，VGGNet相对于AlexNet堆叠了更多基础模块导致网络深度达到近二十层，另外它将之前5x5，7x7的卷积核替换成3x3的小卷积核，用2x2池化代替3x3，去除了局部响应归一化</p><p>。在训练高级别的网络时，可以先训练低级别的网络，用前者获得的权重初始化高级别的网络，可以加速网络的收敛。网络参数如下表所示：</p><figure><img src="/typora-user-images/image-20230313201420238.png"alt="Structure of VGGNet" /><figcaption aria-hidden="true">Structure of VGGNet</figcaption></figure></blockquote><ul><li><p>LeNet 1994</p></li><li><p>AlexNet 2012</p></li><li><p>VGGNet 2014</p></li><li><p>GoogleNet 2014</p></li><li><p>ResNet 2016</p></li><li><p>DenseNet 2017</p></li><li><p>SqueezeNet 2017</p></li><li><p>MobileNet 2017</p></li><li><p>SEnet 2018</p></li></ul><h2 id="lenet">LeNet</h2><blockquote><p>是一系列网络，包括LeNet 1-5</p><p>Yann LeCun 等人在 1990 年</p></blockquote><p>7层神经网络，包括3个卷积层，2个池化层，2个全连接层。所有卷积核5x5，stride = 1. 池化为全局，激活函数为Sigmoid</p><p>用pytorch展现网络架构</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Reshape</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>)<br>    <br>net = torch.nn.Sequential(<br>    Reshape(), <br>    nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>), nn.Sigmoid(),  <span class="hljs-comment"># padding表示在边缘加2，因为本来是32x32的</span><br>    nn.AvgPool2d(<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),<br>    nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">5</span>), nn.Sigmoid(),<br>    nn.AvgPool2d(<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>), <br>    nn.Flatten(),<br>    nn.Linear(<span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>, <span class="hljs-number">120</span>), nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>), nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>)<br>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.rand(size=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>), dtype=torch.float32)<br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    X = layer(X)<br>    <span class="hljs-built_in">print</span>(layer.__class__.__name__, <span class="hljs-string">&#x27;output shape: \t&#x27;</span>, X.shape)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 输出</span><br>Reshape output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>])<br>Conv2d output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>])<br>Sigmoid output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>])<br>AvgPool2d output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, <span class="hljs-number">14</span>, <span class="hljs-number">14</span>])<br>Conv2d output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>])<br>Sigmoid output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">10</span>, <span class="hljs-number">10</span>])<br>AvgPool2d output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>])<br>Flatten output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">400</span>])<br>Linear output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">120</span>])<br>Sigmoid output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">120</span>])<br>Linear output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">84</span>])<br>Sigmoid output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">84</span>])<br>Linear output shape:  torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">10</span>])<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">batch_size = <span class="hljs-number">256</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">evaluate_accuracy_gpu</span>(<span class="hljs-params">net, data_iter, device=<span class="hljs-literal">None</span></span>): <span class="hljs-comment">#@save</span><br>    <span class="hljs-string">&quot;&quot;&quot;使用GPU计算模型在数据集上的精度&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(net, nn.Module):<br>        net.<span class="hljs-built_in">eval</span>()  <span class="hljs-comment"># 设置为评估模式</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> device:<br>            device = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(net.parameters())).device<br>    <span class="hljs-comment"># 正确预测的数量，总预测的数量</span><br>    metric = d2l.Accumulator(<span class="hljs-number">2</span>)<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> data_iter:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(X, <span class="hljs-built_in">list</span>):<br>                <span class="hljs-comment"># BERT微调所需的（之后将介绍）</span><br>                X = [x.to(device) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> X]<br>            <span class="hljs-keyword">else</span>:<br>                X = X.to(device)<br>            y = y.to(device)<br>            metric.add(d2l.accuracy(net(X), y), y.numel())<br>    <span class="hljs-keyword">return</span> metric[<span class="hljs-number">0</span>] / metric[<span class="hljs-number">1</span>]<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_ch6</span>(<span class="hljs-params">net, train_iter, test_iter, num_epochs, lr, device</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">init_weights</span>(<span class="hljs-params">m</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">type</span>(m) == nn.Linear <span class="hljs-keyword">or</span> <span class="hljs-built_in">type</span>(m) == nn.Conv2d:<br>            nn.init.xavier_uniform_(m.weight)<br>    net.apply(init_weights)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;training on &quot;</span>, device)<br>    net.to(device)<br>    optimizer = torch.optim.SGD(net.parameters(), lr=lr)<br>    loss = nn.CrossEntropyLoss()<br>    animator = d2l.Animator(xlabel=<span class="hljs-string">&#x27;epoch&#x27;</span>, xlim=[<span class="hljs-number">1</span>, num_epochs],<br>                            legend=[<span class="hljs-string">&#x27;train loss&#x27;</span>, <span class="hljs-string">&#x27;train acc&#x27;</span>, <span class="hljs-string">&#x27;test acc&#x27;</span>])<br>    timer, num_batches = d2l.Timer(), <span class="hljs-built_in">len</span>(train_iter)<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>        metric = d2l.Accumulator(<span class="hljs-number">3</span>)<br>        net.train()<br>        <span class="hljs-keyword">for</span> i, (X, y) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_iter):<br>            timer.start()<br>            optimizer.zero_grad()<br>            X, y = X.to(device), y.to(device)<br>            y_hat = net(X)<br>            l = loss(y_hat, y)<br>            l.backward()<br>            optimizer.step()<br>            <span class="hljs-keyword">with</span> torch.no_grad():<br>                metric.add(l * X.shape[<span class="hljs-number">0</span>], d2l.accuracy(y_hat, y), X.shape[<span class="hljs-number">0</span>])<br>            timer.stop()<br>            train_l = metric[<span class="hljs-number">0</span>] / metric[<span class="hljs-number">2</span>]<br>            train_acc = metric[<span class="hljs-number">1</span>] / metric[<span class="hljs-number">2</span>]<br>            <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % (num_batches // <span class="hljs-number">5</span>) == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> i == num_batches - <span class="hljs-number">1</span>:<br>                animator.add(epoch + (i + <span class="hljs-number">1</span>) / num_batches,<br>                             (train_l, train_acc, <span class="hljs-literal">None</span>))<br>        test_acc = evaluate_accuracy_gpu(net, test_iter)<br>        animator.add(epoch + <span class="hljs-number">1</span>, (<span class="hljs-literal">None</span>, <span class="hljs-literal">None</span>, test_acc))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;loss <span class="hljs-subst">&#123;train_l:<span class="hljs-number">.3</span>f&#125;</span>, train acc <span class="hljs-subst">&#123;train_acc:<span class="hljs-number">.3</span>f&#125;</span>, &#x27;</span><br>          <span class="hljs-string">f&#x27;test acc <span class="hljs-subst">&#123;test_acc:<span class="hljs-number">.3</span>f&#125;</span>&#x27;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&#x27;<span class="hljs-subst">&#123;metric[<span class="hljs-number">2</span>] * num_epochs / timer.<span class="hljs-built_in">sum</span>():<span class="hljs-number">.1</span>f&#125;</span> examples/sec &#x27;</span><br>          <span class="hljs-string">f&#x27;on <span class="hljs-subst">&#123;<span class="hljs-built_in">str</span>(device)&#125;</span>&#x27;</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">lr, num_epochs = <span class="hljs-number">0.9</span>, <span class="hljs-number">20</span><br>train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">loss <span class="hljs-number">0.356</span>, train acc <span class="hljs-number">0.868</span>, test acc <span class="hljs-number">0.849</span><br><span class="hljs-number">56757.7</span> examples/sec on cuda:<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><figure><img src="/typora-user-images/image-20230214101934210.png"alt="result" /><figcaption aria-hidden="true">result</figcaption></figure><h2 id="alexnet">AlexNet</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br>net = nn.Sequential(<br>    <span class="hljs-comment"># 这里使用一个11*11的更大窗口来捕捉对象。</span><br>    <span class="hljs-comment"># 同时，步幅为4，以减少输出的高度和宽度。</span><br>    <span class="hljs-comment"># 另外，输出通道的数目远大于LeNet</span><br>    nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">96</span>, kernel_size=<span class="hljs-number">11</span>, stride=<span class="hljs-number">4</span>, padding=<span class="hljs-number">1</span>), nn.ReLU(),<br>    nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>    <span class="hljs-comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span><br>    nn.Conv2d(<span class="hljs-number">96</span>, <span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>), nn.ReLU(),<br>    nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>    <span class="hljs-comment"># 使用三个连续的卷积层和较小的卷积窗口。</span><br>    <span class="hljs-comment"># 除了最后的卷积层，输出通道的数量进一步增加。</span><br>    <span class="hljs-comment"># 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度</span><br>    nn.Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">384</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>), nn.ReLU(),<br>    nn.Conv2d(<span class="hljs-number">384</span>, <span class="hljs-number">384</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>), nn.ReLU(),<br>    nn.Conv2d(<span class="hljs-number">384</span>, <span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>), nn.ReLU(),<br>    nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>    nn.Flatten(),<br>    <span class="hljs-comment"># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span><br>    nn.Linear(<span class="hljs-number">6400</span>, <span class="hljs-number">4096</span>), nn.ReLU(),<br>    nn.Dropout(p=<span class="hljs-number">0.5</span>),<br>    nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>), nn.ReLU(),<br>    nn.Dropout(p=<span class="hljs-number">0.5</span>),<br>    <span class="hljs-comment"># 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span><br>    nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>)<br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    X=layer(X)<br>    <span class="hljs-built_in">print</span>(layer.__class__.__name__,<span class="hljs-string">&#x27;output shape:\t&#x27;</span>,X.shape)<br></code></pre></td></tr></table></figure><pre><code class="hljs">Conv2d output shape:     torch.Size([1, 96, 54, 54])ReLU output shape:   torch.Size([1, 96, 54, 54])MaxPool2d output shape:  torch.Size([1, 96, 26, 26])Conv2d output shape:     torch.Size([1, 256, 26, 26])ReLU output shape:   torch.Size([1, 256, 26, 26])MaxPool2d output shape:  torch.Size([1, 256, 12, 12])Conv2d output shape:     torch.Size([1, 384, 12, 12])ReLU output shape:   torch.Size([1, 384, 12, 12])Conv2d output shape:     torch.Size([1, 384, 12, 12])ReLU output shape:   torch.Size([1, 384, 12, 12])Conv2d output shape:     torch.Size([1, 256, 12, 12])ReLU output shape:   torch.Size([1, 256, 12, 12])MaxPool2d output shape:  torch.Size([1, 256, 5, 5])Flatten output shape:    torch.Size([1, 6400])Linear output shape:     torch.Size([1, 4096])ReLU output shape:   torch.Size([1, 4096])Dropout output shape:    torch.Size([1, 4096])Linear output shape:     torch.Size([1, 4096])ReLU output shape:   torch.Size([1, 4096])Dropout output shape:    torch.Size([1, 4096])Linear output shape:     torch.Size([1, 10])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">batch_size = <span class="hljs-number">128</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="hljs-number">224</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">lr, num_epochs = <span class="hljs-number">0.01</span>, <span class="hljs-number">10</span><br>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 输出</span><br>loss <span class="hljs-number">0.331</span>, train acc <span class="hljs-number">0.878</span>, test acc <span class="hljs-number">0.882</span><br><span class="hljs-number">1635.7</span> examples/sec on cuda:<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure><figure><img src="/typora-user-images/image-20230214102245051.png"alt="result" /><figcaption aria-hidden="true">result</figcaption></figure><h2 id="vggnet">VGGNet</h2><p>使用块，更大更深的AlexNet</p><ul><li><p>带填充以保持分辨率的卷积层；</p></li><li><p>非线性激活函数，如ReLU；</p></li><li><p>汇聚层，如最大汇聚层。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">vgg_block</span>(<span class="hljs-params">num_convs, in_channels, out_channels</span>):<br>    layers = []<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_convs):<br>        layers.append(nn.Conv2d(in_channels, out_channels,<br>                                kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>))<br>        layers.append(nn.ReLU())<br>        in_channels = out_channels<br>    layers.append(nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>))<br>    <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">conv_arch = ((<span class="hljs-number">1</span>, <span class="hljs-number">64</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">256</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">512</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">512</span>))<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">vgg</span>(<span class="hljs-params">conv_arch</span>):<br>    conv_blks = []<br>    in_channels = <span class="hljs-number">1</span><br>    <span class="hljs-keyword">for</span> (num_convs, out_channels) <span class="hljs-keyword">in</span> conv_arch:<br>        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))<br>        in_channels = out_channels<br>    <span class="hljs-keyword">return</span> nn.Sequential(<br>        *conv_blks, <br>        nn.Flatten(),<br>        nn.Linear(out_channels * <span class="hljs-number">7</span> * <span class="hljs-number">7</span>, <span class="hljs-number">4096</span>), nn.ReLU(),<br>        nn.Dropout(<span class="hljs-number">0.5</span>), nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>), nn.ReLU(),<br>        nn.Dropout(<span class="hljs-number">0.5</span>), nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">10</span>)<br>    )<br>net = vgg(conv_arch)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.randn(size=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>))<br><span class="hljs-keyword">for</span> blk <span class="hljs-keyword">in</span> net:<br>    X = blk(X)<br>    <span class="hljs-built_in">print</span>(blk.__class__.__name__, <span class="hljs-string">&#x27;output shape:\t&#x27;</span>, X.shape)<br>    <br></code></pre></td></tr></table></figure><pre><code class="hljs">Sequential output shape:     torch.Size([1, 64, 112, 112])Sequential output shape:     torch.Size([1, 128, 56, 56])Sequential output shape:     torch.Size([1, 256, 28, 28])Sequential output shape:     torch.Size([1, 512, 14, 14])Sequential output shape:     torch.Size([1, 512, 7, 7])Flatten output shape:    torch.Size([1, 25088])Linear output shape:     torch.Size([1, 4096])ReLU output shape:   torch.Size([1, 4096])Dropout output shape:    torch.Size([1, 4096])Linear output shape:     torch.Size([1, 4096])ReLU output shape:   torch.Size([1, 4096])Dropout output shape:    torch.Size([1, 4096])Linear output shape:     torch.Size([1, 10])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">ratio = <span class="hljs-number">4</span><br>small_conv_arch = [(pair[<span class="hljs-number">0</span>], pair[<span class="hljs-number">1</span>] // ratio) <span class="hljs-keyword">for</span> pair <span class="hljs-keyword">in</span> conv_arch]<br>net = vgg(small_conv_arch)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.randn(size=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>))<br><span class="hljs-keyword">for</span> blk <span class="hljs-keyword">in</span> net:<br>    X = blk(X)<br>    <span class="hljs-built_in">print</span>(blk.__class__.__name__, <span class="hljs-string">&#x27;output shape:\t&#x27;</span>, X.shape)<br></code></pre></td></tr></table></figure><pre><code class="hljs">Sequential output shape:     torch.Size([1, 16, 112, 112])Sequential output shape:     torch.Size([1, 32, 56, 56])Sequential output shape:     torch.Size([1, 64, 28, 28])Sequential output shape:     torch.Size([1, 128, 14, 14])Sequential output shape:     torch.Size([1, 128, 7, 7])Flatten output shape:    torch.Size([1, 6272])Linear output shape:     torch.Size([1, 4096])ReLU output shape:   torch.Size([1, 4096])Dropout output shape:    torch.Size([1, 4096])Linear output shape:     torch.Size([1, 4096])ReLU output shape:   torch.Size([1, 4096])Dropout output shape:    torch.Size([1, 4096])Linear output shape:     torch.Size([1, 10])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">lr, num_epochs, batch_size = <span class="hljs-number">0.05</span>, <span class="hljs-number">10</span>, <span class="hljs-number">128</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="hljs-number">224</span>)<br>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br></code></pre></td></tr></table></figure><pre><code class="hljs">loss 0.177, train acc 0.935, test acc 0.8941091.9 examples/sec on cuda:0</code></pre><figure><img src="/typora-user-images/image-20230214140415757.png"alt="result" /><figcaption aria-hidden="true">result</figcaption></figure><h2 id="googlenetinception-v3">GoogleNet/Inception V3</h2><p>Inception 块</p><p>输入被copy成四块</p><figure><img src="/typora-user-images/image-20230214141018466.png"alt="image-20230214141018466" /><figcaption aria-hidden="true">image-20230214141018466</figcaption></figure><figure><img src="/typora-user-images/image-20230214141928373.png"alt="image-20230214141928373" /><figcaption aria-hidden="true">image-20230214141928373</figcaption></figure><p>用了很多1x1卷积，降低通道数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Inception</span>(nn.Module):<br>    <span class="hljs-comment"># c1--c4是每条路径的输出通道数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(Inception, self).__init__(**kwargs)<br>        <span class="hljs-comment"># 线路1，单1x1卷积层</span><br>        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 线路2，1x1卷积层后接3x3卷积层</span><br>        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="hljs-number">0</span>], kernel_size=<span class="hljs-number">1</span>)<br>        self.p2_2 = nn.Conv2d(c2[<span class="hljs-number">0</span>], c2[<span class="hljs-number">1</span>], kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 线路3，1x1卷积层后接5x5卷积层</span><br>        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="hljs-number">0</span>], kernel_size=<span class="hljs-number">1</span>)<br>        self.p3_2 = nn.Conv2d(c3[<span class="hljs-number">0</span>], c3[<span class="hljs-number">1</span>], kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)<br>        <span class="hljs-comment"># 线路4，3x3最大汇聚层后接1x1卷积层</span><br>        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>)<br>        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        p1 = F.relu(self.p1_1(x))<br>        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))<br>        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))<br>        p4 = F.relu(self.p4_2(self.p4_1(x)))<br>        <span class="hljs-comment"># 在通道维度上连结输出</span><br>        <span class="hljs-keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">b1 = nn.Sequential(nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>),<br>                   nn.ReLU(),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">b2 = nn.Sequential(nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">1</span>),<br>                   nn.ReLU(),<br>                   nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>                   nn.ReLU(),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">b3 = nn.Sequential(Inception(<span class="hljs-number">192</span>, <span class="hljs-number">64</span>, (<span class="hljs-number">96</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">32</span>), <span class="hljs-number">32</span>),<br>                   Inception(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">192</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">96</span>), <span class="hljs-number">64</span>),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">b4 = nn.Sequential(Inception(<span class="hljs-number">480</span>, <span class="hljs-number">192</span>, (<span class="hljs-number">96</span>, <span class="hljs-number">208</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">48</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">160</span>, (<span class="hljs-number">112</span>, <span class="hljs-number">224</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">128</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">256</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">112</span>, (<span class="hljs-number">144</span>, <span class="hljs-number">288</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">528</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">b5 = nn.Sequential(Inception(<span class="hljs-number">832</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   Inception(<span class="hljs-number">832</span>, <span class="hljs-number">384</span>, (<span class="hljs-number">192</span>, <span class="hljs-number">384</span>), (<span class="hljs-number">48</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>                   nn.Flatten())<br><br>net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.rand(size=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">96</span>, <span class="hljs-number">96</span>))<br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    X = layer(X)<br>    <span class="hljs-built_in">print</span>(layer.__class__.__name__,<span class="hljs-string">&#x27;output shape:\t&#x27;</span>, X.shape)<br></code></pre></td></tr></table></figure><pre><code class="hljs">Sequential output shape:     torch.Size([1, 64, 24, 24])Sequential output shape:     torch.Size([1, 192, 12, 12])Sequential output shape:     torch.Size([1, 480, 6, 6])Sequential output shape:     torch.Size([1, 832, 3, 3])Sequential output shape:     torch.Size([1, 1024])Linear output shape:     torch.Size([1, 10])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">lr, num_epochs, batch_size = <span class="hljs-number">0.1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">128</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="hljs-number">96</span>)<br>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br></code></pre></td></tr></table></figure><pre><code class="hljs">loss 0.246, train acc 0.907, test acc 0.8931690.6 examples/sec on cuda:0</code></pre><figure><img src="/typora-user-images/image-20230214200912069.png"alt="image-20230214200912069" /><figcaption aria-hidden="true">image-20230214200912069</figcaption></figure><h2 id="resnet">ResNet</h2><figure><img src="/typora-user-images/image-20230214201006292.png"alt="image-20230214201006292" /><figcaption aria-hidden="true">image-20230214201006292</figcaption></figure><figure><img src="/typora-user-images/image-20230214201016846.png"alt="image-20230214201016846" /><figcaption aria-hidden="true">image-20230214201016846</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Residual</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_channels, num_channels,</span><br><span class="hljs-params">                 use_1x1conv=<span class="hljs-literal">False</span>, strides=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.conv1 = nn.Conv2d(input_channels, num_channels,<br>                               kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>, stride=strides)<br>        self.conv2 = nn.Conv2d(num_channels, num_channels,<br>                               kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">if</span> use_1x1conv:<br>            self.conv3 = nn.Conv2d(input_channels, num_channels,<br>                                   kernel_size=<span class="hljs-number">1</span>, stride=strides)<br>        <span class="hljs-keyword">else</span>:<br>            self.conv3 = <span class="hljs-literal">None</span><br>        self.bn1 = nn.BatchNorm2d(num_channels)<br>        self.bn2 = nn.BatchNorm2d(num_channels)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        Y = F.relu(self.bn1(self.conv1(X)))<br>        Y = self.bn2(self.conv2(Y))<br>        <span class="hljs-keyword">if</span> self.conv3:<br>            X = self.conv3(X)<br>        Y += X<br>        <span class="hljs-keyword">return</span> F.relu(Y)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">b1 = nn.Sequential(nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>),<br>                   nn.BatchNorm2d(<span class="hljs-number">64</span>), nn.ReLU(),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>)<br>                  )<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">resnet_block</span>(<span class="hljs-params">input_channels, num_channels, num_residuals,</span><br><span class="hljs-params">                 first_block=<span class="hljs-literal">False</span></span>):<br>    blk = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_residuals):<br>        <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> first_block:<br>            blk.append(Residual(input_channels, num_channels,<br>                                use_1x1conv=<span class="hljs-literal">True</span>, strides=<span class="hljs-number">2</span>))<br>        <span class="hljs-keyword">else</span>:<br>            blk.append(Residual(num_channels, num_channels))<br>    <span class="hljs-keyword">return</span> blk<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">b2 = nn.Sequential(*resnet_block(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-number">2</span>, first_block=<span class="hljs-literal">True</span>))<br>b3 = nn.Sequential(*resnet_block(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">2</span>))<br>b4 = nn.Sequential(*resnet_block(<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>))<br>b5 = nn.Sequential(*resnet_block(<span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">2</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">net = nn.Sequential(b1, b2, b3, b4, b5,<br>                    nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>                    nn.Flatten(), nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.rand(size=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>))<br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    X = layer(X)<br>    <span class="hljs-built_in">print</span>(layer.__class__.__name__,<span class="hljs-string">&#x27;output shape:\t&#x27;</span>, X.shape)<br></code></pre></td></tr></table></figure><pre><code class="hljs">Sequential output shape:     torch.Size([1, 64, 56, 56])Sequential output shape:     torch.Size([1, 64, 56, 56])Sequential output shape:     torch.Size([1, 128, 28, 28])Sequential output shape:     torch.Size([1, 256, 14, 14])Sequential output shape:     torch.Size([1, 512, 7, 7])AdaptiveAvgPool2d output shape:  torch.Size([1, 512, 1, 1])Flatten output shape:    torch.Size([1, 512])Linear output shape:     torch.Size([1, 10])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">lr, num_epochs, batch_size = <span class="hljs-number">0.05</span>, <span class="hljs-number">10</span>, <span class="hljs-number">256</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="hljs-number">96</span>)<br>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br></code></pre></td></tr></table></figure><pre><code class="hljs">loss 0.015, train acc 0.996, test acc 0.8932613.9 examples/sec on cuda:0</code></pre><figure><img src="/typora-user-images/image-20230214201027140.png"alt="image-20230214201027140" /><figcaption aria-hidden="true">image-20230214201027140</figcaption></figure>]]></content>
    
    
    
    <tags>
      
      <tag>notes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[cs231n] Summary</title>
    <link href="/2023/02/18/%5Bcs231n%5DSummary/"/>
    <url>/2023/02/18/%5Bcs231n%5DSummary/</url>
    
    <content type="html"><![CDATA[<h1 id="cs231n-summary">cs231n: Summary</h1><h2 id="neural-networks">Neural Networks</h2><h3id="image-classification-data-driven-approach-k-nearest-neighbor-trainvaltest-splits"><ahref="https://cs231n.github.io/classification/">Image Classification:Data-driven Approach, k-Nearest Neighbor, train/val/test splits</a></h3><h4 id="image-classification">Image Classification</h4><p>Using a set of labeled images to predict categories of a set of testimages. Then we can measure the accuracy of the predictions.</p><h4 id="nearest-neighbor-classifier">Nearest Neighbor classifier</h4><ul><li>Choose a distance(L1, L2, etc.)</li><li>Calculate the sum of the distance between each text data and all thetrain data. Get the closest one. The label of this data is what theclassifier predict.</li></ul><h4 id="knn-classifier">kNN classifier</h4><p>Find the top k closest images and then have them vote on the label ofthe test image.</p><h4 id="validation-set-cross-validation">Validation set,cross-validation</h4><p><img src="/typora-user-images/image-20230203181619887.png" alt="image-20230203181619887" style="zoom:80%;" /></p><p>In this picture, fold 5 is the validation set. For cross-validation,we let fold 1-5 be validation set separately to help us choose somehyperparameters.</p><h3 id="linear-classification-support-vector-machine-softmax"><ahref="https://cs231n.github.io/linear-classify/">Linear classification:Support Vector Machine, Softmax</a></h3><p>What we want: a map from images to label scores. <spanclass="math inline">\(\Rightarrow\)</span> Score function, Lossfunction</p><h4 id="score-function">Score function</h4><p><span class="math inline">\(x_i\)</span> is a picture and <spanclass="math inline">\(W\)</span> is a matrix named weights. And <spanclass="math inline">\(b\)</span> is bias.<br /><span class="math display">\[f(x_i,W,b)=Wx_i+b\]</span> Sometimes we can extend <spanclass="math inline">\(W\)</span>:</p><p><img src="/typora-user-images/image-20230203184649909.png" alt="image-20230203184649909" style="zoom: 67%;" /></p><h5 id="preprocessing-center-the-data">Preprocessing: center thedata</h5><p>For photos, pixel value: [0~255]</p><p>Now: [0…255] <span class="math inline">\(\Rightarrow\)</span> [-127.. 127] <span class="math inline">\(\Rightarrow\)</span> [-1,1]</p><h4 id="loss-function">Loss function</h4><h5 id="multiclass-support-vector-machine-loss">Multiclass SupportVector Machine loss</h5><p>For image <span class="math inline">\(x_i\)</span> with label <spanclass="math inline">\(y_i\)</span>. Score function is <spanclass="math inline">\(f(x_i,W)\)</span>. Let <spanclass="math inline">\(s_j = f(x_i,W)_j\)</span>. Multiclass SVM loss:<span class="math display">\[L_i = \sum_{j\neq y_i}max(0,s_j-s_{y_i}+\Delta)\]</span></p><h6 id="regularization">Regularization</h6><p><span class="math display">\[R(W) = \sum_k\sum_l W_{k,l}^2\\L=\frac{1}{N}\sum_i L_i + \lambda R(W)\]</span></p><p>Or <span class="math display">\[L=\frac{1}{N}\sum_i\sum_{j\neqy_i}[max(0,f(x_i,W))_j-f(x_i,W)_{y_i}+\Delta]+\lambda\sum_k\sum_lW_{k,l}^2\]</span></p><h4 id="softmax-classifier">Softmax classifier</h4><p>With <span class="math inline">\(f(x_i,W)=Wx_i\)</span> unchanged,but for the loss: <span class="math display">\[L_i=-log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}})\;or\;L_i=-f_{y_i}+log\sum_je^{f_j}\]</span></p><h3 id="optimization-stochastic-gradient-descent"><ahref="https://cs231n.github.io/optimization-1/">Optimization: StochasticGradient Descent</a></h3><h4 id="visualizing-the-loss-function">Visualizing the lossfunction</h4><p><img src="/typora-user-images/image-20230203191407300.png" alt="image-20230203191407300" style="zoom:80%;" /></p><figure><img src="/typora-user-images/image-20230203191516168.png"alt="image-20230203191516168" /><figcaption aria-hidden="true">image-20230203191516168</figcaption></figure><h4 id="optimization">Optimization</h4><ul><li><p>Random search</p></li><li><p>Random local search</p></li><li><p>Following the gradient</p></li></ul><p><span class="math display">\[\frac{df(x)}{dx}=\lim_{h\rightarrow 0}\frac{f(x+h)-f(x)}{h}\]</span></p><p>Use this definition to calculate grad of all dims</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">eval_numerical_gradient</span>(<span class="hljs-params">f, x</span>):<br>  <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">  a naive implementation of numerical gradient of f at x</span><br><span class="hljs-string">  - f should be a function that takes a single argument</span><br><span class="hljs-string">  - x is the point (numpy array) to evaluate the gradient at</span><br><span class="hljs-string">  &quot;&quot;&quot;</span><br><br>  fx = f(x) <span class="hljs-comment"># evaluate function value at original point</span><br>  grad = np.zeros(x.shape)<br>  h = <span class="hljs-number">0.00001</span><br><br>  <span class="hljs-comment"># iterate over all indexes in x</span><br>  it = np.nditer(x, flags=[<span class="hljs-string">&#x27;multi_index&#x27;</span>], op_flags=[<span class="hljs-string">&#x27;readwrite&#x27;</span>])<br>  <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> it.finished:<br><br>    <span class="hljs-comment"># evaluate function at x+h</span><br>    ix = it.multi_index<br>    old_value = x[ix]<br>    x[ix] = old_value + h <span class="hljs-comment"># increment by h</span><br>    fxh = f(x) <span class="hljs-comment"># evalute f(x + h)</span><br>    x[ix] = old_value <span class="hljs-comment"># restore to previous value (very important!)</span><br><br>    <span class="hljs-comment"># compute the partial derivative</span><br>    grad[ix] = (fxh - fx) / h <span class="hljs-comment"># the slope</span><br>    it.iternext() <span class="hljs-comment"># step to next dimension</span><br><br>  <span class="hljs-keyword">return</span> grad<br></code></pre></td></tr></table></figure><p>In fact, we usually use <span class="math display">\[\frac{f(x+h)-f(x-h)}{2h}\]</span> But this method of calculation is expensive and not soaccurate. So maybe we can do it in a more “math” way.</p><p>Take loss function of SVM as an example: <spanclass="math display">\[L_i = \sum_{j\neq y_i}[max(0,w_j^Tx_i-w_{y_i}^Tx_i+\Delta)]\]</span> We can differentiate the function: <spanclass="math display">\[\Delta_{w_{y_i}}L_i = -(\sum_{j\neqy_i}1_{\{w_j^Tx_i-w_{y_i}^Tx_i+\Delta &gt;0\}}) \\\Delta_{w_{j}}L_i = 1_{\{w_j^Tx_i-w_{y_i}^Tx_i+\Delta &gt;0\}}x_i\]</span> With grad, we can do <strong>Gradient Descent</strong> bychoosing a suitable <strong>step size</strong>(or <strong>learningrate</strong>),</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>  weights_grad = evaluate_gradient(loss_fun, data, weights)<br>  weights += - step_size * weights_grad <span class="hljs-comment"># perform parameter update</span><br></code></pre></td></tr></table></figure><p>However, if the dataset is very big, this can be extremely expensive.So we introduce <strong>Mini-batch gradient descent</strong>. That meanswe can only evaluate on a small subset to get a gradient.</p><p>The extreme case of this method is the subset has only one image.This process is called <strong>Stochastic GradientDescent(SGD)</strong></p><h3 id="backpropagation-intuitions"><ahref="https://cs231n.github.io/optimization-2/">Backpropagation,Intuitions</a></h3><p>Make good use of chain rule</p><p><strong>Ex</strong></p><figure><img src="/typora-user-images/image-20230203202320171.png"alt="image-20230203202320171" /><figcaption aria-hidden="true">image-20230203202320171</figcaption></figure><ul><li>Add gate</li><li>Max gate</li><li>Multiply gate</li></ul><h3 id="neural-networks-part-1-setting-up-the-architecture"><ahref="https://cs231n.github.io/neural-networks-1/">Neural Networks Part1: Setting up the Architecture</a></h3><figure><img src="/typora-user-images/image-20230207121144890.png"alt="image-20230207121144890" /><figcaption aria-hidden="true">image-20230207121144890</figcaption></figure><p>input<span class="math inline">\(\rightarrow\)</span>input<spanclass="math inline">\(\cdot\)</span>weight<spanclass="math inline">\(\rightarrow\)</span>input<spanclass="math inline">\(\cdot\)</span>weight+bias<spanclass="math inline">\(\rightarrow\)</span>activate-f(input<spanclass="math inline">\(\cdot\)</span>weight+bias)</p><p><strong>Ex</strong> <span class="math display">\[s=W_2max(0,W_1x)\\s=W_3max(0,W_2max(0,W_1x))\]</span></p><h4 id="single-neuron-as-a-linear-classifier">Single neuron as a linearclassifier</h4><ul><li>Binary Softmax classifier</li><li>Binary SVM classifier</li><li>Regularization interpretation</li></ul><h4 id="commonly-used-activation-functions">Commonly used activationfunctions</h4><ul><li><strong>Sigmoid</strong></li></ul><p><span class="math display">\[\sigma(x)=\frac{1}{1+e^{-x}}\]</span></p><p>​ Shortcomings:</p><p>​ 1. kill gradients</p><p>​ 2. not zero-centered</p><p><img src="/typora-user-images/image-20230207123419875.png" alt="image-20230207123419875" style="zoom:50%;" /></p><ul><li><strong>Tanh</strong></li></ul><p><span class="math display">\[tanh(x)=2\sigma(2x)-1\]</span></p><p><img src="/typora-user-images/image-20230207123435272.png" alt="image-20230207123435272" style="zoom:50%;" /></p><p>​ Solve the problem of not zero-centered</p><ul><li><strong>ReLU</strong></li></ul><p><span class="math display">\[f(x)=max(0,x)\]</span></p><p>​ Advantages: greatly accelerate the convergence of stochasticdescent; not so expensive as tanh/sigmoid</p><p>​ Shortcoming: may die when a large gradient flow through a ReLUneuron. May solved by setting a proper learning rate</p><ul><li><strong>Leaky ReLU</strong></li></ul><p><span class="math display">\[f(x)=1_{(x&lt;0)}(\alpha x)+1_{(x&gt;=0)}(x)\]</span></p><p>​ in which <span class="math inline">\(\alpha\)</span> is very smalllike 0.01</p><ul><li><strong>Maxout</strong></li></ul><p><span class="math display">\[max(w_1^Tx+b_1,w_2^Tx+b_2)\]</span></p><blockquote><p><strong>Naming conventions</strong> When we talk about N-layer neuralnetwork, the input layer is not included in “N”.</p><p><strong>Output Layer</strong> Unlike other layers, the output layerneurons most commonly do not have an activation function. (while theoutput layer is used to present scores of every class, it’s easy tounderstand)</p><p><strong>Sizing neural networks</strong> Measure the size of neural:number of neurons/number of parameters</p></blockquote><h4 id="representational-power">Representational power</h4><p>Surveys has proven that given any continuous function f(x)and someϵ&gt;0, there exists a Neural Network g(x) with one hidden layer (with areasonable choice of non-linearity, e.g. sigmoid) such that∀x,∣f(x)−g(x)∣&lt;ϵ. In other words, the neural network can approximateany continuous function.</p><p>Practically, deeper networks can work better than asingle-hidden-layer network</p><p>For <strong>neural network</strong>, usually 3-layer networks will bebetter than 2-layer nets. But more deeper(4,5,6-layer) network rarelyhelps much more. But for <strong>convolutional network </strong>, it isdifferent. Depth is a very important factor.</p><p>Regularization is very important, which can elite overfitting.</p><h3 id="neural-networks-part-2-setting-up-the-data-and-the-loss"><ahref="https://cs231n.github.io/neural-networks-2/">Neural Networks Part2: Setting up the Data and the Loss</a></h3><h4 id="data-preprocessing">Data Preprocessing</h4><h5 id="mean-subtraction">Mean subtraction</h5><p>code: <code>X-=np.mean(X, axis = 0)</code></p><h5 id="normalization">Normalization</h5><p>code: <code>X/=np.std(X, axis = 0)</code></p><h5 id="pca-and-whitening">PCA and Whitening</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Assume input data matrix X of size [N x D]</span><br>X -= np.mean(X, axis = <span class="hljs-number">0</span>) <span class="hljs-comment"># zero-center the data (important)</span><br>cov = np.dot(X.T, X) / X.shape[<span class="hljs-number">0</span>] <span class="hljs-comment"># get the data covariance matrix</span><br></code></pre></td></tr></table></figure><p>compute the SVD factorization of the data covariance matrix:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">U,S,V = np.linalg.svd(cov)<br></code></pre></td></tr></table></figure><p>where the columns of<code class="language-plaintext highlighter-rouge">U</code> are theeigenvectors and<code class="language-plaintext highlighter-rouge">S</code> is a 1-Darray of the singular values. To decorrelate the data, we project theoriginal (but zero-centered) data into the eigenbasis:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">Xrot = np.dot(X, U)<br></code></pre></td></tr></table></figure><p>dimensionality reduction:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">Xrot_reduced = np.dot(X, U[:,:<span class="hljs-number">100</span>]) <span class="hljs-comment"># Xrot_reduced becomes [N x 100]</span><br></code></pre></td></tr></table></figure><p>For <strong>whitening</strong>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># whiten the data:</span><br><span class="hljs-comment"># divide by the eigenvalues (which are square roots of the singular values)</span><br>Xwhite = Xrot / np.sqrt(S + <span class="hljs-number">1e-5</span>)<br></code></pre></td></tr></table></figure><figure><img src="/typora-user-images/image-20230207144339020.png"alt="image-20230207144339020" /><figcaption aria-hidden="true">image-20230207144339020</figcaption></figure><p><strong>Common pitfall</strong> The mean must be computed only overthe training data and then subtracted equally from all splits(train/val/test).</p><h4 id="weight-initialization">Weight Initialization</h4><p><strong>Pitfall: all zero initialization</strong> That will makeevery neuron do same thing.</p><p><strong>Small random numbers</strong> aim: <em>symmetrybreaking</em>. <code>W=0.01*np.random.randn(D, H)</code></p><p><strong>Calibrating the variances with 1/sqrt(n)</strong><code>W=np.random.randn(n) / sqrt(n)</code> where n is the number of itsinputs</p><p>Other: <strong>Sparse initialization</strong>, <strong>Initializingthe biases(0)</strong>, <strong>Batch Normalization</strong></p><h4 id="regularization-1">Regularization</h4><p><strong>L2 regularization</strong> May the most common. for all <spanclass="math inline">\(w\)</span>, add <spanclass="math inline">\(\frac{1}{2}\lambda w^2\)</span> to theobjective.</p><p><strong>L1 regularization</strong> for each weight <spanclass="math inline">\(w\)</span>, we add the term <spanclass="math inline">\(\lambda |w|\)</span> to the objective.</p><p>We can also combine the L1 regularization with the L2 regularization:<span class="math inline">\(\lambda _1|w|+\lambda_2w^2\)</span>, whichis called Elastic net regularization.</p><p><strong>Max norm constraints</strong> Enforce an absolute upper boundon the magnitude of the weight vector.</p><p><strong>Dropout</strong> keep a neuron active with some probability<span class="math inline">\(p\)</span> or setting it to zerootherwise</p><p>And there are many other methods about regularization. <strong>Biasregularization, per-layer regularization</strong>…</p><h4 id="loss-functions">Loss functions</h4><p>Let <span class="math inline">\(f=f(x_i;W)\)</span> to be theactivations of the output layer in a Neural Network.</p><p><strong>Classification</strong></p><p>SVM: <span class="math display">\[L_i=\sum_{j\neq y_i}max(0,f_j-f_{y_i}+1)\]</span></p><blockquote><p>sometimes use <spanclass="math inline">\(max(0,(f_j-f_{y_i}+1)^2)\)</span></p></blockquote><p>Softmax: <span class="math display">\[L_i=-log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}})\]</span> <strong>Problem: Large number of classes</strong> When the setof labels is very large, Softmax becomes very expensive. It may behelpful to use <em>Hierarchical Softmax</em>.</p><p><strong>Attribute classification</strong></p><p>Both losses above assume that there is a single correct answer <spanclass="math inline">\(y_i\)</span>. But what if <spanclass="math inline">\(y_i\)</span> is a binary vector where everyexample may or may not have a certain attribute <spanclass="math display">\[L_i=\sum_jmax(0,1-y_{ij}f_j)\]</span> where <span class="math inline">\(y_{ij}\)</span> is either +1or -1</p><h3 id="neural-networks-part-3-learning-and-evaluation"><ahref="https://cs231n.github.io/neural-networks-3/">Neural Networks Part3: Learning and Evaluation</a></h3><p>Talking about learning process.</p><h4 id="gradient-checks">Gradient Checks</h4><p><span class="math display">\[\frac{df(x)}{dx}=\frac{f(x+h)-f(x-h)}{2h}\]</span></p><p>In order to compare the numerical gradient <spanclass="math inline">\(f_n&#39;\)</span> and analytic gradient <spanclass="math inline">\(f_a&#39;\)</span>, we can use relative error:<span class="math display">\[\frac{|f_a&#39;-f_b&#39;|}{max(|f_a&#39;|,|f_n&#39;|)}\]</span> ​ In practive:</p><ul><li>relative error &gt;<span class="math inline">\(1e-2\)</span>:probably wrong</li><li>1e-2&gt;relative error&gt;1e-4: uncomfortable…</li><li>1e-4&gt;relative error: OK…but without kinks(e.g. tanh and softmax),to high.</li><li>1e-7&gt;relative error: happy</li></ul><blockquote><p>should use double precision</p></blockquote><blockquote><p>if jump kinks, may not be exact</p></blockquote><p>In order to avoid above problems:</p><ul><li>Use only few datapoints</li><li>be careful with h</li></ul><figure><img src="/typora-user-images/image-20230207205258779.png"alt="image-20230207205258779" /><figcaption aria-hidden="true">image-20230207205258779</figcaption></figure><ul><li><p>Don’t let the regularization overwhelm the data</p></li><li><p>Remember to turn off dropout/augmentations</p></li></ul><h4 id="before-learning-sanity-checks-tipstricks">Before learning:sanity checks Tips/Tricks</h4><ul><li>Trace loss function</li></ul><p><img src="/typora-user-images/image-20230207210235489.png" alt="image-20230207210235489" style="zoom:80%;" /></p><p>​ The right picture may mean the data is so small</p><ul><li>Trace train/val accuracy</li></ul><p>​<img src="/typora-user-images/image-20230207210422908.png" alt="image-20230207210422908" style="zoom:80%;" /></p><ul><li>Ratio of weights: updates</li></ul><p>​ <strong>Ex</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># assume parameter vector W and its gradient vector dW</span><br>param_scale = np.linalg.norm(W.ravel())<br>update = -learning_rate*dW <span class="hljs-comment"># simple SGD update</span><br>update_scale = np.linalg.norm(update.ravel())<br>W += update <span class="hljs-comment"># the actual update</span><br><span class="hljs-built_in">print</span> update_scale / param_scale <span class="hljs-comment"># want ~1e-3</span><br></code></pre></td></tr></table></figure><ul><li><p>Activation / Gradient distributions per layer</p></li><li><p>First-layer Visualizations</p></li></ul><p>​ <strong>Ex</strong></p><p><img src="/typora-user-images/image-20230207211414465.png" alt="image-20230207211414465" style="zoom:80%;" /></p><p>​ Left: many noise. may in trouble. Right: Nice, smooth</p><h4 id="parameter-updates">Parameter updates</h4><h5 id="first-ordersgd-momentum-nesterov-momentum">First-order(SGD),momentum, Nesterov momentum</h5><ul><li><strong>Vanilla</strong> update</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x += - learning_rate * dx<br></code></pre></td></tr></table></figure><ul><li><strong>Momentum</strong> update</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Momentum update</span><br>v = mu * v - learning_rate * dx <span class="hljs-comment"># integrate velocity</span><br>x += v <span class="hljs-comment"># integrate position</span><br></code></pre></td></tr></table></figure><p>​ mu can be seen as the coefficient of friction in physics. (typical0.9)</p><ul><li><strong>Nesterov</strong> Momentum</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">x_ahead = x + mu * v<br><span class="hljs-comment"># evaluate dx_ahead (the gradient at x_ahead instead of at x)</span><br>v = mu * v - learning_rate * dx_ahead<br>x += v<br></code></pre></td></tr></table></figure><figure><img src="/typora-user-images/image-20230207213105160.png"alt="image-20230207213105160" /><figcaption aria-hidden="true">image-20230207213105160</figcaption></figure><p>​ In practice, people like to rename <spanclass="math inline">\(x\_head\)</span> as <spanclass="math inline">\(x\)</span> :</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">v_prev = v <span class="hljs-comment"># back this up</span><br>v = mu * v - learning_rate * dx <span class="hljs-comment"># velocity update stays the same</span><br>x += -mu * v_prev + (<span class="hljs-number">1</span> + mu) * v <span class="hljs-comment"># position update changes form</span><br></code></pre></td></tr></table></figure><h5 id="annealing-the-learning-rate">Annealing the learning rate</h5><p>If the learning rate is too high, the system contains too muchkinetic energy, unable to settle down into deeper, but narrower parts ofthe loss function.</p><p>Normally, there are three methods to decay the learning rate:</p><ul><li><strong>Step decay</strong> Reduce the learning rate every fewepochs.</li><li><strong>Exponential decay</strong> In math: <spanclass="math inline">\(\alpha = \alpha_0e^{-kt}\)</span> in which <spanclass="math inline">\(\alpha_0,k\)</span> are hyperparameters and <spanclass="math inline">\(t\)</span> is the iteration number.</li><li><strong>1/t decay</strong> In math: <spanclass="math inline">\(\alpha = \alpha _0/(1+kt)\)</span> in which <spanclass="math inline">\(\alpha_0,k\)</span> are hyperparameters and <spanclass="math inline">\(t\)</span> is the iteration number</li></ul><p>​ In practice, we find that the step decay is slightly preferablebecause the hyperparameters it involves</p><h5 id="second-order-methods">Second order methods</h5><p>Basing on Newton’s method: <span class="math display">\[x\leftarrow x-[Hf(x)]^{-1}\nabla f(x)\]</span> Multiplying by the inverse Hessian leads the optimization totake more aggressive steps in directions of shallow curvature andshorter steps in directions of steep curvature</p><p>However, because of the expensive cost of calculating the Hessianmatrix, this method is impractical.</p><h5 id="per-parameter-adaptive-learning-rate-methods">Per-parameteradaptive learning rate methods</h5><p><strong>Adagrad</strong> is an adaptive learning rate methodoriginally proposed by <ahref="http://jmlr.org/papers/v12/duchi11a.html">Duchi et al.</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Assume the gradient dx and parameter vector x</span><br>cache += dx**<span class="hljs-number">2</span><br>x += - learning_rate * dx / (np.sqrt(cache) + eps)<br></code></pre></td></tr></table></figure><p><code>eps</code>: 1e-4~1e-8</p><p>shortcoming: the monotonic learning rate usually proves tooaggressive and stops learning too early</p><p><strong>RMSprop</strong> <ahref="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">slide29 of Lecture 6</a> of Geoff Hinton’s Coursera class: reduce Adagrad’saggressive</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">cache = decay_rate * cache + (<span class="hljs-number">1</span> - decay_rate) * dx**<span class="hljs-number">2</span><br>x += - learning_rate * dx / (np.sqrt(cache) + eps)<br></code></pre></td></tr></table></figure><p>in which <code>decay_rate</code> is a hyperparameter: 0.9, 0.99,0.999</p><p><strong>Adam</strong> a recently proposed update looks a bit likeRMSprop with momentum</p><p>simplified:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">m = beta1*m + (<span class="hljs-number">1</span>-beta1)*dx<br>v = beta2*v + (<span class="hljs-number">1</span>-beta2)*(dx**<span class="hljs-number">2</span>)<br>x += - learning_rate * m / (np.sqrt(v) + eps)<br></code></pre></td></tr></table></figure><p>recommend: <code>eps = 1e-8</code>, <code>beta1 = 0.9</code>,<code>beta2 = 0.999</code></p><p>With the <em>bias correction</em> mechanism, the update looks asfollows:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># t is your iteration counter going from 1 to infinity</span><br>m = beta1*m + (<span class="hljs-number">1</span>-beta1)*dx<br>mt = m / (<span class="hljs-number">1</span>-beta1**t)<br>v = beta2*v + (<span class="hljs-number">1</span>-beta2)*(dx**<span class="hljs-number">2</span>)<br>vt = v / (<span class="hljs-number">1</span>-beta2**t)<br>x += - learning_rate * mt / (np.sqrt(vt) + eps)<br></code></pre></td></tr></table></figure><h4 id="hyperparameter-optimization">Hyperparameter optimization</h4><p>The most common hyperparameters in context of Neural Networksinclude:</p><ul><li>the initial learning rate</li><li>learning rate decay schedule(such as the decay constant)</li><li>regularization strength(L2 penalty, dropout strength)</li></ul><p><strong>Implementation</strong> make a worker and master</p><p><strong>Prefer one validation fold to cross-validation</strong> asingle validation set of respectable size substantially simplifies thecode base, without the need for cross-validation with multiple folds</p><p><strong>Hyperparameter ranges</strong><code>learning_rate = 10 ** uniform(-6, 1)</code></p><p><strong>Prefer random search to grid search</strong> can be easilyunderstand through following image:</p><figure><img src="/typora-user-images/image-20230208141840955.png"alt="image-20230208141840955" /><figcaption aria-hidden="true">image-20230208141840955</figcaption></figure><p><strong>Careful with best values on border</strong> if we find thatthe results is on the border, we may set a bad range and miss the truebest result.</p><p><strong>Stage your search from coarse to fine</strong></p><p><strong>Bayesian Hyperparameter Optimization</strong> <ahref="https://github.com/JasperSnoek/spearmint">Spearmint</a>, <ahref="http://www.cs.ubc.ca/labs/beta/Projects/SMAC/">SMAC</a>, and <ahref="http://jaberg.github.io/hyperopt/">Hyperopt</a>. However, inpractical settings with ConvNets it is still relatively difficult tobeat random search in a carefully-chosen intervals.</p><h4 id="evaluation">Evaluation</h4><h5 id="model-ensembles">Model Ensembles</h5><p>A reliable way to improve the performance of Neural Networks by a fewpercent: train multiple independent models, and at test time averagetheir predictions.</p><p>The number of models <span class="math inline">\(\uparrow\)</span>performance <span class="math inline">\(\uparrow\)</span> the variety ofmodels <span class="math inline">\(\uparrow\)</span> performance <spanclass="math inline">\(\uparrow\)</span></p><p>Some approaches to forming an ensemble</p><ul><li><strong>Same model, different initializations</strong></li></ul><p>​ Use cross-validation to determine the best hyperparameters, thentrain multiple models with the best set of hyperparameters but withdifferent random initialization.</p><p>​ shortcoming: variety is only due to iitialization</p><ul><li><p><strong>Top models discovered duringcross-validation</strong></p><p>Use cross-validation to determine the best hyperparameters, then pickthe top few (e.g. 10) models to form the ensemble.</p></li></ul><p>​ shortcoming: may include suboptimal models</p><ul><li><strong>Different checkpoints of a single model</strong></li></ul><p>​ Taking different checkpoints of a single network over time istraining is very expensive</p><p>​ shortcoming: lack of variety</p><p>​ advantage: very cheap</p><ul><li><strong>Running average of parameters during training</strong></li></ul><p>​</p><p>Shortcoming of model ensembles: take longer to evaluate on testexample.</p><p>A good idea: “distill” a good ensemble back to a single model byincorporating the ensemble log likelihoods into a modifiedobjective.</p><h2 id="convolutional-neural-networks">Convolutional NeuralNetworks</h2><h3id="convolutional-neural-networks-architectures-convolution-pooling-layers"><ahref="https://cs231n.github.io/convolutional-networks/">ConvolutionalNeural Networks: Architectures, Convolution / Pooling Layers</a></h3><p>CNN base on an assumption that the inputs are images.</p><h4 id="layers-used-to-build-convnets">Layers used to buildConvNets</h4><p><strong>Convolutional Layer, Pooling Layer, Fully-ConnectedLayer</strong></p><figure><img src="/typora-user-images/image-20230208144748173.png"alt="image-20230208144748173" /><figcaption aria-hidden="true">image-20230208144748173</figcaption></figure><h5 id="convolutional-layer">Convolutional Layer</h5><p><strong>filter</strong> with size like <spanclass="math inline">\(5\times 5\times 3\)</span></p><p>During the forward pass, we slide (more precisely, convolve) eachfilter across the width and height of the input volume and compute dotproducts between the entries of the filter and the input at anyposition</p><p><span class="math inline">\(\rightarrow\)</span> produce a separate2-dimensional activation map</p><p>The spatial extent of this connectivity: a hyperparameter called the<strong>receptive field</strong></p><p><strong>Depth, Stride, Zero-padding</strong></p><p><strong>Depth</strong>: depend on the number of filter. Called “deepcolumn” or “fiber”</p><p><strong>Stride</strong>: the number of pixel the filter will movewhen we slide it</p><p><strong>Zero-padding</strong>: pad the input volume with zeros aroundthe border</p><p>In math. the input volume size :<spanclass="math inline">\(W\)</span>, the receptive size of the Conv Layerneurons: <span class="math inline">\(F\)</span>, the stride with whichthey are applied: <span class="math inline">\(S\)</span>, the amount ofzero padding used: <span class="math inline">\(P\)</span></p><p>then the output: <spanclass="math inline">\((W-F+2P)/S+1\)</span></p><p><strong>Summary</strong></p><ul><li><p>Accept size: <span class="math inline">\(W_1\times H_1\timesD_1\)</span></p></li><li><p>Require:</p><ul><li>number of filters <span class="math inline">\(K\)</span></li><li>spatial extent <span class="math inline">\(F\)</span></li><li>stride <span class="math inline">\(S\)</span></li><li>the amount of zero padding <spanclass="math inline">\(P\)</span></li></ul></li><li><p>Produce size: <span class="math inline">\(W_2 \times H_2 \timesD_2\)</span></p><ul><li><span class="math inline">\(W_2=(W_1-F+2P)/S+1\)</span></li><li><span class="math inline">\(H_2=(H_1-F+2P)/S+1\)</span></li><li><span class="math inline">\(D_2=K\)</span></li></ul></li></ul><p>common set: <span class="math inline">\(F=3, S=1, P=1\)</span></p><p><strong>Backpropagation</strong> The backward pass for a convolutionoperation (for both the data and the weights) is also a convolution (butwith spatially-flipped filters).</p><p><strong><span class="math inline">\(1\times 1\)</span>convolution</strong> note that we have 3 channels. so it’s notmeaningless</p><p><strong>Dilated convolutions</strong> have filters that have spacesbetween each cell, called dilation.</p><p>##### Pooling layer</p><p>Common: Max</p><figure><img src="/typora-user-images/image-20230208152427080.png"alt="image-20230208152427080" /><figcaption aria-hidden="true">image-20230208152427080</figcaption></figure><ul><li><p>Accept size <span class="math inline">\(W_1\times H_1\timesD_1\)</span></p></li><li><p>Hyperparameters</p><ul><li>spatial extent <span class="math inline">\(F\)</span></li><li>stride <span class="math inline">\(S\)</span></li></ul></li><li><p>Produce size <span class="math inline">\(W_2\times H_2\timesD_2\)</span></p><ul><li><span class="math inline">\(W_2=(W_1-F)/S+1\)</span></li><li><span class="math inline">\(H_2=(H_1-F)/S+1\)</span></li><li><span class="math inline">\(D_2=D_1\)</span></li></ul></li></ul><p>Common: <span class="math inline">\(F=3, S=2\)</span> more commonly<span class="math inline">\(F=2, S=2\)</span></p><h5 id="normalization-layer">Normalization Layer</h5><p>These layers have since fallen out of favor because in practice theircontribution has been shown to be minimal, if any. For various types ofnormalizations, see the discussion in Alex Krizhevsky’s <ahref="http://code.google.com/p/cuda-convnet/wiki/LayerParams#Local_response_normalization_layer_(same_map)">cuda-convnetlibrary API</a>.</p><h5 id="fully-connected-layer">Fully-connected layer</h5><p>Just like Neural Network section</p><h5 id="converting-fully-connected-layers-to-conv-layers">ConvertingFully Connected layers to CONV layers</h5><p>Each of these conversions could in practice involve manipulating(e.g. reshaping) the weight matrix <spanclass="math inline">\(W\)</span> in each FC layer into CONV layerfilters. It turns out that this conversion allows us to “slide” theoriginal ConvNet very efficiently across many spatial positions in alarger image, in a single forward pass.</p><h4 id="convnet-architectures">ConvNet Architectures</h4><p>The most common pattern:</p><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clean">INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt; [FC -&gt; RELU]*K -&gt; FC<br></code></pre></td></tr></table></figure><p><code>N &gt;= 0</code> (and usually <code>N &lt;= 3</code>),<code>M &gt;= 0</code>, <code>K &gt;= 0</code> (and usually<code>K &lt; 3</code>).</p><ul><li><p><code>INPUT -&gt; FC</code>, implements a linearclassifier.</p></li><li><p><code>INPUT -&gt; CONV -&gt; RELU -&gt; FC</code></p></li><li><p><code>INPUT -&gt; [CONV -&gt; RELU -&gt; POOL]*2 -&gt; FC -&gt; RELU -&gt; FC</code>.Here we see that there is a single CONV layer between every POOLlayer.</p></li><li><p><code>INPUT -&gt; [CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; POOL]*3 -&gt; [FC -&gt; RELU]*2 -&gt; FC</code>Here we see two CONV layers stacked before every POOL layer.</p></li></ul><p><em>Prefer a stack of small filter CONV to one large receptive fieldCONV layer</em>.</p><h4 id="layer-sizing-patterns">Layer Sizing Patterns</h4><ul><li><strong>Input Layer</strong></li></ul><p>​ Should be divisible by 2 many times.</p><p>​ Ex. 32 (e.g. CIFAR-10), 64, 96 (e.g. STL-10), or 224 (e.g. commonImageNet ConvNets), 384, and 512.</p><ul><li><strong>Conv Layer</strong></li></ul><p>​ 3x3 or 5x5 Step <span class="math inline">\(S=1\)</span>, zero</p><ul><li><strong>Pool Layer</strong></li></ul><p>​ 2x2 , stride = 2(sometimes 3x3, stride = 2)</p><p><strong>Use stride of 1 in CONV</strong>:</p><ol type="1"><li>Smaller strides work better in practice.</li><li>Allows us to leave all spatial down-sampling to the POOL layers,with the CONV layers only transforming the input volume depth-wise.</li></ol><p><strong>Use padding</strong>:</p><p>If the CONV layers were to not zero-pad the inputs and only performvalid convolutions, then the size of the volumes would reduce by a smallamount after each CONV, and the <strong>information at theborders</strong> would be “washed away” too quickly.</p><p><strong>Compromising based on memory constrains</strong>:</p><p>people prefer to make the compromise at only the first CONV layer ofthe network. For example, one compromise might be to use a first CONVlayer with filter sizes of 7x7 and stride of 2 (as seen in a ZF net). Asanother example, an AlexNet uses filter sizes of 11x11 and stride of4.</p><h4 id="case-studies">Case Studies</h4><ul><li><strong>LeNet</strong></li><li><strong>AlexNet</strong></li><li><strong>ZF Net</strong></li><li><strong>GoogleNet</strong></li><li><strong>VGGNet</strong></li><li><strong>ResNet</strong></li></ul><p>VGG:</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs routeros">INPUT: [224x224x3]        memory:  224<span class="hljs-number">*224</span><span class="hljs-number">*3</span>=150K   weights: 0<br>CONV3-64: [224x224x64]  memory:  224<span class="hljs-number">*224</span><span class="hljs-number">*64</span>=3.2M   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*3</span>)<span class="hljs-number">*64</span> = 1,728<br>CONV3-64: [224x224x64]  memory:  224<span class="hljs-number">*224</span><span class="hljs-number">*64</span>=3.2M   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*64</span>)<span class="hljs-number">*64</span> = 36,864<br>POOL2: [112x112x64]  memory:  112<span class="hljs-number">*112</span><span class="hljs-number">*64</span>=800K   weights: 0<br>CONV3-128: [112x112x128]  memory:  112<span class="hljs-number">*112</span><span class="hljs-number">*128</span>=1.6M   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*64</span>)<span class="hljs-number">*128</span> = 73,728<br>CONV3-128: [112x112x128]  memory:  112<span class="hljs-number">*112</span><span class="hljs-number">*128</span>=1.6M   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*128</span>)<span class="hljs-number">*128</span> = 147,456<br>POOL2: [56x56x128]  memory:  56<span class="hljs-number">*56</span><span class="hljs-number">*128</span>=400K   weights: 0<br>CONV3-256: [56x56x256]  memory:  56<span class="hljs-number">*56</span><span class="hljs-number">*256</span>=800K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*128</span>)<span class="hljs-number">*256</span> = 294,912<br>CONV3-256: [56x56x256]  memory:  56<span class="hljs-number">*56</span><span class="hljs-number">*256</span>=800K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*256</span>)<span class="hljs-number">*256</span> = 589,824<br>CONV3-256: [56x56x256]  memory:  56<span class="hljs-number">*56</span><span class="hljs-number">*256</span>=800K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*256</span>)<span class="hljs-number">*256</span> = 589,824<br>POOL2: [28x28x256]  memory:  28<span class="hljs-number">*28</span><span class="hljs-number">*256</span>=200K   weights: 0<br>CONV3-512: [28x28x512]  memory:  28<span class="hljs-number">*28</span><span class="hljs-number">*512</span>=400K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*256</span>)<span class="hljs-number">*512</span> = 1,179,648<br>CONV3-512: [28x28x512]  memory:  28<span class="hljs-number">*28</span><span class="hljs-number">*512</span>=400K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*512</span>)<span class="hljs-number">*512</span> = 2,359,296<br>CONV3-512: [28x28x512]  memory:  28<span class="hljs-number">*28</span><span class="hljs-number">*512</span>=400K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*512</span>)<span class="hljs-number">*512</span> = 2,359,296<br>POOL2: [14x14x512]  memory:  14<span class="hljs-number">*14</span><span class="hljs-number">*512</span>=100K   weights: 0<br>CONV3-512: [14x14x512]  memory:  14<span class="hljs-number">*14</span><span class="hljs-number">*512</span>=100K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*512</span>)<span class="hljs-number">*512</span> = 2,359,296<br>CONV3-512: [14x14x512]  memory:  14<span class="hljs-number">*14</span><span class="hljs-number">*512</span>=100K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*512</span>)<span class="hljs-number">*512</span> = 2,359,296<br>CONV3-512: [14x14x512]  memory:  14<span class="hljs-number">*14</span><span class="hljs-number">*512</span>=100K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*512</span>)<span class="hljs-number">*512</span> = 2,359,296<br>POOL2: [7x7x512]  memory:  7<span class="hljs-number">*7</span><span class="hljs-number">*512</span>=25K  weights: 0<br>FC: [1x1x4096]  memory:  4096  weights: 7<span class="hljs-number">*7</span><span class="hljs-number">*512</span><span class="hljs-number">*4096</span> = 102,760,448<br>FC: [1x1x4096]  memory:  4096  weights: 4096<span class="hljs-number">*4096</span> = 16,777,216<br>FC: [1x1x1000]  memory:  1000 weights: 4096<span class="hljs-number">*1000</span> = 4,096,000<br><br>TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~<span class="hljs-number">*2</span> <span class="hljs-keyword">for</span> bwd)<br>TOTAL params: 138M parameters<br></code></pre></td></tr></table></figure><p><strong>Computational Considerations</strong></p><p>There are three major sources of memory to keep track of:</p><ul><li><p>From the intermediate volume sizes</p></li><li><p>From the parameter sizes</p></li><li><p>Every ConvNet implementation has to maintain<strong>miscellaneous</strong> memory, such as the image data batches,perhaps their augmented versions, etc.</p></li></ul><h3id="transfer-learning-and-fine-tuning-convolutional-neural-networks"><ahref="https://cs231n.github.io/transfer-learning/">Transfer Learning andFine-tuning Convolutional Neural Networks</a></h3><h4 id="transfer-learning">Transfer Learning</h4><figure><img src="/typora-user-images/image-20230210153134566.png"alt="image-20230210153134566" /><figcaption aria-hidden="true">image-20230210153134566</figcaption></figure>]]></content>
    
    
    
    <tags>
      
      <tag>notes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>[iGEM] Modelling Group: introduction about Markdown and LaTeX</title>
    <link href="/2023/01/25/Modelling-Group-introduction-about-Markdown-and-LaTeX/"/>
    <url>/2023/01/25/Modelling-Group-introduction-about-Markdown-and-LaTeX/</url>
    
    <content type="html"><![CDATA[<h1 id="markdown">Markdown</h1><p>What is markdown?</p><ul><li>language(light)</li><li>easy to control</li></ul><h2 id="titles">Titles</h2><h1 id="im-1">I'm 1</h1><h2 id="im-2">I'm 2</h2><h3 id="im-3">I'm 3</h3><h4 id="im-4">I'm 4</h4><h5 id="im-5">I'm 5</h5><h6 id="im-6">I'm 6</h6><p class="heading" id="im-7">I'm 7</p><p>for title: 1-6, no 7</p><h2 id="bold-italic-bolditalic">bold, italic, bold&amp;italic</h2><p>I am a <em>sentence</em>. I like eat <strong>beef</strong>.</p><p>I am a <strong><em>student</em></strong>.</p><h2 id="links-photos">links, photos</h2><p><a href="https://www.baidu.com">Baidu</a></p><figure><img src="/img/me.jpg" alt="me" /><figcaption aria-hidden="true">me</figcaption></figure><h2 id="quote">quote</h2><blockquote><p>Lu Xun: people should eat.</p></blockquote><h2 id="list-2-types">list: 2 types</h2><ul><li>one<ul><li>two</li></ul></li><li>three<ul><li>four<ul><li>five</li></ul></li></ul></li></ul><ol type="1"><li>1</li><li>3</li></ol><p>if no " ":</p><p>1.123</p><h2 id="code">code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">sum</span>=<span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>):<br>  <span class="hljs-built_in">sum</span> += i<br></code></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs C"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;stdio.h&gt;</span></span><br><span class="hljs-type">int</span> <span class="hljs-title function_">main</span><span class="hljs-params">()</span><br>&#123;<br>  balabala;<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="table">table</h2><table><thead><tr class="header"><th style="text-align: left;">Header One</th><th style="text-align: left;">Header Two</th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">Item One</td><td style="text-align: left;">Item Two</td></tr></tbody></table><h2 id="others">others</h2><p><del>I eat rubbish yesterday</del></p><p><u>this sentence is important</u></p><p>I must dosomething.<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="guess">[1]</span></a></sup></p><h2 id="math">math</h2><p>We all know that 2+3=5</p><p>we all know that <spanclass="math inline">\(1+\frac{1}{2}=\frac{3}{2}\)</span>。<spanclass="math inline">\(2^2=4\)</span>, <spanclass="math inline">\(CO_2\)</span>, <spanclass="math inline">\(x_{11}\)</span> <span class="math display">\[1+\frac{1}{2}\]</span></p><p><span class="math display">\[\begin{pmatrix}x_{11} &amp; x_{12} \\x_{21} &amp; x_{22}\end{pmatrix}\]</span></p><p><span class="math display">\[X=\sum^{n}_{i=1} \frac{\Pi_{j=1}^{m}x_{ij}}{\sigma_{i}k_i^2}\]</span></p><p><span class="math display">\[\begin{equation}x=\frac{\partial T}{\partial V}c_{Boltzman}\alpha_x\end{equation}\]</span></p><p><span class="math display">\[\begin{equation*}y=\frac{\partial T}{\partial M}c_{Boltzman}\alpha_y\end{equation*}\]</span></p><p><span class="math display">\[\begin{equation}z=\frac{\partial T}{\partial K}c_{Boltzman}\alpha_z\end{equation}\]</span></p><p><span class="math display">\[\begin{align}1&amp;=0+1\\2&amp;=1+1-0-1+1\\444&amp;=400+40+4\end{align}\]</span></p><p><span class="math display">\[\begin{align*}a+b+c&amp;=x\\x&amp;=a+b+c\end{align*}\]</span></p><p><span class="math display">\[A+B\rightarrow C\\\]</span> (of course not end…)</p><hr /><h1 id="latex">LaTex</h1><p><span class="math inline">\(\LaTeX\)</span></p><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>guess<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    
    <tags>
      
      <tag>teach</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello world</title>
    <link href="/2023/01/23/Hello-world/"/>
    <url>/2023/01/23/Hello-world/</url>
    
    <content type="html"><![CDATA[<h1 id="hello-world">HELLO WORLD</h1><h2 id="my-first-try-of-blog">My First Try of Blog</h2><p>Thanks to the help of Wang Zhili and Hu shukai, I managed to build myfirst blog today.</p><p>My blog will all in English, recording some courses notes and dailyfeelings.</p>]]></content>
    
    
    
    <tags>
      
      <tag>intro</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
