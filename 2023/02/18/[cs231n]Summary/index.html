

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/me.jpg">
  <link rel="icon" href="/img/me.jpg">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Jamil Yu">
  <meta name="keywords" content="">
  
    <meta name="description" content="Notes and summary about cs231n from Stanford University">
<meta property="og:type" content="article">
<meta property="og:title" content="[cs231n] Summary">
<meta property="og:url" content="http://jamil-yu.github.io/2023/02/18/[cs231n]Summary/index.html">
<meta property="og:site_name" content="Jamil">
<meta property="og:description" content="Notes and summary about cs231n from Stanford University">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://jamil-yu.github.io/typora-user-images/image-20230203181619887.png">
<meta property="og:image" content="http://jamil-yu.github.io/typora-user-images/image-20230203184649909.png">
<meta property="og:image" content="http://jamil-yu.github.io/typora-user-images/image-20230203191407300.png">
<meta property="og:image" content="http://jamil-yu.github.io/typora-user-images/image-20230203191516168.png">
<meta property="og:image" content="http://jamil-yu.github.io/typora-user-images/image-20230203202320171.png">
<meta property="og:image" content="http://jamil-yu.github.io/typora-user-images/image-20230207121144890.png">
<meta property="og:image" content="http://jamil-yu.github.io/typora-user-images/image-20230207123419875.png">
<meta property="og:image" content="http://jamil-yu.github.io/typora-user-images/image-20230207123435272.png">
<meta property="og:image" content="http://jamil-yu.github.io/typora-user-images/image-20230207144339020.png">
<meta property="og:image" content="http://jamil-yu.github.io/typora-user-images/image-20230207205258779.png">
<meta property="og:image" content="http://jamil-yu.github.io/typora-user-images/image-20230207210235489.png">
<meta property="og:image" content="http://jamil-yu.github.io/typora-user-images/image-20230207210422908.png">
<meta property="og:image" content="http://jamil-yu.github.io/typora-user-images/image-20230207211414465.png">
<meta property="og:image" content="http://jamil-yu.github.io/typora-user-images/image-20230207213105160.png">
<meta property="og:image" content="http://jamil-yu.github.io/typora-user-images/image-20230208141840955.png">
<meta property="og:image" content="http://jamil-yu.github.io/typora-user-images/image-20230208144748173.png">
<meta property="og:image" content="http://jamil-yu.github.io/typora-user-images/image-20230208152427080.png">
<meta property="og:image" content="http://jamil-yu.github.io/typora-user-images/image-20230210153134566.png">
<meta property="article:published_time" content="2023-02-18T14:27:33.000Z">
<meta property="article:modified_time" content="2023-03-13T12:00:01.195Z">
<meta property="article:author" content="Jamil Yu">
<meta property="article:tag" content="notes">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://jamil-yu.github.io/typora-user-images/image-20230203181619887.png">
  
  
  
  <title>[cs231n] Summary - Jamil</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"jamil-yu.github.io","root":"/","version":"1.9.4","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Jamil-Yu&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-friends"></i>
                <span>link</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/background.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="[cs231n] Summary"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-02-18 22:27" pubdate>
          February 18, 2023 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          19k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          156 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">[cs231n] Summary</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="cs231n-summary">cs231n: Summary</h1>
<h2 id="neural-networks">Neural Networks</h2>
<h3
id="image-classification-data-driven-approach-k-nearest-neighbor-trainvaltest-splits"><a
target="_blank" rel="noopener" href="https://cs231n.github.io/classification/">Image Classification:
Data-driven Approach, k-Nearest Neighbor, train/val/test splits</a></h3>
<h4 id="image-classification">Image Classification</h4>
<p>Using a set of labeled images to predict categories of a set of test
images. Then we can measure the accuracy of the predictions.</p>
<h4 id="nearest-neighbor-classifier">Nearest Neighbor classifier</h4>
<ul>
<li>Choose a distance(L1, L2, etc.)</li>
<li>Calculate the sum of the distance between each text data and all the
train data. Get the closest one. The label of this data is what the
classifier predict.</li>
</ul>
<h4 id="knn-classifier">kNN classifier</h4>
<p>Find the top k closest images and then have them vote on the label of
the test image.</p>
<h4 id="validation-set-cross-validation">Validation set,
cross-validation</h4>
<p><img src="/typora-user-images/image-20230203181619887.png" srcset="/img/loading.gif" lazyload alt="image-20230203181619887" style="zoom:80%;" /></p>
<p>In this picture, fold 5 is the validation set. For cross-validation,
we let fold 1-5 be validation set separately to help us choose some
hyperparameters.</p>
<h3 id="linear-classification-support-vector-machine-softmax"><a
target="_blank" rel="noopener" href="https://cs231n.github.io/linear-classify/">Linear classification:
Support Vector Machine, Softmax</a></h3>
<p>What we want: a map from images to label scores. <span
class="math inline">\(\Rightarrow\)</span> Score function, Loss
function</p>
<h4 id="score-function">Score function</h4>
<p><span class="math inline">\(x_i\)</span> is a picture and <span
class="math inline">\(W\)</span> is a matrix named weights. And <span
class="math inline">\(b\)</span> is bias.<br />
<span class="math display">\[
f(x_i,W,b)=Wx_i+b
\]</span> Sometimes we can extend <span
class="math inline">\(W\)</span>:</p>
<p><img src="/typora-user-images/image-20230203184649909.png" srcset="/img/loading.gif" lazyload alt="image-20230203184649909" style="zoom: 67%;" /></p>
<h5 id="preprocessing-center-the-data">Preprocessing: center the
data</h5>
<p>For photos, pixel value: [0~255]</p>
<p>Now: [0…255] <span class="math inline">\(\Rightarrow\)</span> [-127
.. 127] <span class="math inline">\(\Rightarrow\)</span> [-1,1]</p>
<h4 id="loss-function">Loss function</h4>
<h5 id="multiclass-support-vector-machine-loss">Multiclass Support
Vector Machine loss</h5>
<p>For image <span class="math inline">\(x_i\)</span> with label <span
class="math inline">\(y_i\)</span>. Score function is <span
class="math inline">\(f(x_i,W)\)</span>. Let <span
class="math inline">\(s_j = f(x_i,W)_j\)</span>. Multiclass SVM loss:
<span class="math display">\[
L_i = \sum_{j\neq y_i}max(0,s_j-s_{y_i}+\Delta)
\]</span></p>
<h6 id="regularization">Regularization</h6>
<p><span class="math display">\[
R(W) = \sum_k\sum_l W_{k,l}^2\\
L=\frac{1}{N}\sum_i L_i + \lambda R(W)
\]</span></p>
<p>Or <span class="math display">\[
L=\frac{1}{N}\sum_i\sum_{j\neq
y_i}[max(0,f(x_i,W))_j-f(x_i,W)_{y_i}+\Delta]+\lambda
\sum_k\sum_lW_{k,l}^2
\]</span></p>
<h4 id="softmax-classifier">Softmax classifier</h4>
<p>With <span class="math inline">\(f(x_i,W)=Wx_i\)</span> unchanged,
but for the loss: <span class="math display">\[
L_i=-log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}})\;or\;L_i=-f_{y_i}+log\sum_je^{f_j}
\]</span></p>
<h3 id="optimization-stochastic-gradient-descent"><a
target="_blank" rel="noopener" href="https://cs231n.github.io/optimization-1/">Optimization: Stochastic
Gradient Descent</a></h3>
<h4 id="visualizing-the-loss-function">Visualizing the loss
function</h4>
<p><img src="/typora-user-images/image-20230203191407300.png" srcset="/img/loading.gif" lazyload alt="image-20230203191407300" style="zoom:80%;" /></p>
<figure>
<img src="/typora-user-images/image-20230203191516168.png" srcset="/img/loading.gif" lazyload
alt="image-20230203191516168" />
<figcaption aria-hidden="true">image-20230203191516168</figcaption>
</figure>
<h4 id="optimization">Optimization</h4>
<ul>
<li><p>Random search</p></li>
<li><p>Random local search</p></li>
<li><p>Following the gradient</p></li>
</ul>
<p><span class="math display">\[
\frac{df(x)}{dx}=\lim_{h\rightarrow 0}\frac{f(x+h)-f(x)}{h}
\]</span></p>
<p>Use this definition to calculate grad of all dims</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">eval_numerical_gradient</span>(<span class="hljs-params">f, x</span>):<br>  <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">  a naive implementation of numerical gradient of f at x</span><br><span class="hljs-string">  - f should be a function that takes a single argument</span><br><span class="hljs-string">  - x is the point (numpy array) to evaluate the gradient at</span><br><span class="hljs-string">  &quot;&quot;&quot;</span><br><br>  fx = f(x) <span class="hljs-comment"># evaluate function value at original point</span><br>  grad = np.zeros(x.shape)<br>  h = <span class="hljs-number">0.00001</span><br><br>  <span class="hljs-comment"># iterate over all indexes in x</span><br>  it = np.nditer(x, flags=[<span class="hljs-string">&#x27;multi_index&#x27;</span>], op_flags=[<span class="hljs-string">&#x27;readwrite&#x27;</span>])<br>  <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> it.finished:<br><br>    <span class="hljs-comment"># evaluate function at x+h</span><br>    ix = it.multi_index<br>    old_value = x[ix]<br>    x[ix] = old_value + h <span class="hljs-comment"># increment by h</span><br>    fxh = f(x) <span class="hljs-comment"># evalute f(x + h)</span><br>    x[ix] = old_value <span class="hljs-comment"># restore to previous value (very important!)</span><br><br>    <span class="hljs-comment"># compute the partial derivative</span><br>    grad[ix] = (fxh - fx) / h <span class="hljs-comment"># the slope</span><br>    it.iternext() <span class="hljs-comment"># step to next dimension</span><br><br>  <span class="hljs-keyword">return</span> grad<br></code></pre></td></tr></table></figure>
<p>In fact, we usually use <span class="math display">\[
\frac{f(x+h)-f(x-h)}{2h}
\]</span> But this method of calculation is expensive and not so
accurate. So maybe we can do it in a more “math” way.</p>
<p>Take loss function of SVM as an example: <span
class="math display">\[
L_i = \sum_{j\neq y_i}[max(0,w_j^Tx_i-w_{y_i}^Tx_i+\Delta)]
\]</span> We can differentiate the function: <span
class="math display">\[
\Delta_{w_{y_i}}L_i = -(\sum_{j\neq
y_i}1_{\{w_j^Tx_i-w_{y_i}^Tx_i+\Delta &gt;0\}}) \\
\Delta_{w_{j}}L_i = 1_{\{w_j^Tx_i-w_{y_i}^Tx_i+\Delta &gt;0\}}x_i
\]</span> With grad, we can do <strong>Gradient Descent</strong> by
choosing a suitable <strong>step size</strong>(or <strong>learning
rate</strong>),</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:<br>  weights_grad = evaluate_gradient(loss_fun, data, weights)<br>  weights += - step_size * weights_grad <span class="hljs-comment"># perform parameter update</span><br></code></pre></td></tr></table></figure>
<p>However, if the dataset is very big, this can be extremely expensive.
So we introduce <strong>Mini-batch gradient descent</strong>. That means
we can only evaluate on a small subset to get a gradient.</p>
<p>The extreme case of this method is the subset has only one image.
This process is called <strong>Stochastic Gradient
Descent(SGD)</strong></p>
<h3 id="backpropagation-intuitions"><a
target="_blank" rel="noopener" href="https://cs231n.github.io/optimization-2/">Backpropagation,
Intuitions</a></h3>
<p>Make good use of chain rule</p>
<p><strong>Ex</strong></p>
<figure>
<img src="/typora-user-images/image-20230203202320171.png" srcset="/img/loading.gif" lazyload
alt="image-20230203202320171" />
<figcaption aria-hidden="true">image-20230203202320171</figcaption>
</figure>
<ul>
<li>Add gate</li>
<li>Max gate</li>
<li>Multiply gate</li>
</ul>
<h3 id="neural-networks-part-1-setting-up-the-architecture"><a
target="_blank" rel="noopener" href="https://cs231n.github.io/neural-networks-1/">Neural Networks Part
1: Setting up the Architecture</a></h3>
<figure>
<img src="/typora-user-images/image-20230207121144890.png" srcset="/img/loading.gif" lazyload
alt="image-20230207121144890" />
<figcaption aria-hidden="true">image-20230207121144890</figcaption>
</figure>
<p>input<span class="math inline">\(\rightarrow\)</span>input<span
class="math inline">\(\cdot\)</span>weight<span
class="math inline">\(\rightarrow\)</span>input<span
class="math inline">\(\cdot\)</span>weight+bias<span
class="math inline">\(\rightarrow\)</span>activate-f(input<span
class="math inline">\(\cdot\)</span>weight+bias)</p>
<p><strong>Ex</strong> <span class="math display">\[
s=W_2max(0,W_1x)\\
s=W_3max(0,W_2max(0,W_1x))
\]</span></p>
<h4 id="single-neuron-as-a-linear-classifier">Single neuron as a linear
classifier</h4>
<ul>
<li>Binary Softmax classifier</li>
<li>Binary SVM classifier</li>
<li>Regularization interpretation</li>
</ul>
<h4 id="commonly-used-activation-functions">Commonly used activation
functions</h4>
<ul>
<li><strong>Sigmoid</strong></li>
</ul>
<p><span class="math display">\[
\sigma(x)=\frac{1}{1+e^{-x}}
\]</span></p>
<p>​ Shortcomings:</p>
<p>​ 1. kill gradients</p>
<p>​ 2. not zero-centered</p>
<p><img src="/typora-user-images/image-20230207123419875.png" srcset="/img/loading.gif" lazyload alt="image-20230207123419875" style="zoom:50%;" /></p>
<ul>
<li><strong>Tanh</strong></li>
</ul>
<p><span class="math display">\[
tanh(x)=2\sigma(2x)-1
\]</span></p>
<p><img src="/typora-user-images/image-20230207123435272.png" srcset="/img/loading.gif" lazyload alt="image-20230207123435272" style="zoom:50%;" /></p>
<p>​ Solve the problem of not zero-centered</p>
<ul>
<li><strong>ReLU</strong></li>
</ul>
<p><span class="math display">\[
f(x)=max(0,x)
\]</span></p>
<p>​ Advantages: greatly accelerate the convergence of stochastic
descent; not so expensive as tanh/sigmoid</p>
<p>​ Shortcoming: may die when a large gradient flow through a ReLU
neuron. May solved by setting a proper learning rate</p>
<ul>
<li><strong>Leaky ReLU</strong></li>
</ul>
<p><span class="math display">\[
f(x)=1_{(x&lt;0)}(\alpha x)+1_{(x&gt;=0)}(x)
\]</span></p>
<p>​ in which <span class="math inline">\(\alpha\)</span> is very small
like 0.01</p>
<ul>
<li><strong>Maxout</strong></li>
</ul>
<p><span class="math display">\[
max(w_1^Tx+b_1,w_2^Tx+b_2)
\]</span></p>
<blockquote>
<p><strong>Naming conventions</strong> When we talk about N-layer neural
network, the input layer is not included in “N”.</p>
<p><strong>Output Layer</strong> Unlike other layers, the output layer
neurons most commonly do not have an activation function. (while the
output layer is used to present scores of every class, it’s easy to
understand)</p>
<p><strong>Sizing neural networks</strong> Measure the size of neural:
number of neurons/number of parameters</p>
</blockquote>
<h4 id="representational-power">Representational power</h4>
<p>Surveys has proven that given any continuous function f(x)and some
ϵ&gt;0, there exists a Neural Network g(x) with one hidden layer (with a
reasonable choice of non-linearity, e.g. sigmoid) such that
∀x,∣f(x)−g(x)∣&lt;ϵ. In other words, the neural network can approximate
any continuous function.</p>
<p>Practically, deeper networks can work better than a
single-hidden-layer network</p>
<p>For <strong>neural network</strong>, usually 3-layer networks will be
better than 2-layer nets. But more deeper(4,5,6-layer) network rarely
helps much more. But for <strong>convolutional network </strong>, it is
different. Depth is a very important factor.</p>
<p>Regularization is very important, which can elite overfitting.</p>
<h3 id="neural-networks-part-2-setting-up-the-data-and-the-loss"><a
target="_blank" rel="noopener" href="https://cs231n.github.io/neural-networks-2/">Neural Networks Part
2: Setting up the Data and the Loss</a></h3>
<h4 id="data-preprocessing">Data Preprocessing</h4>
<h5 id="mean-subtraction">Mean subtraction</h5>
<p>code: <code>X-=np.mean(X, axis = 0)</code></p>
<h5 id="normalization">Normalization</h5>
<p>code: <code>X/=np.std(X, axis = 0)</code></p>
<h5 id="pca-and-whitening">PCA and Whitening</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Assume input data matrix X of size [N x D]</span><br>X -= np.mean(X, axis = <span class="hljs-number">0</span>) <span class="hljs-comment"># zero-center the data (important)</span><br>cov = np.dot(X.T, X) / X.shape[<span class="hljs-number">0</span>] <span class="hljs-comment"># get the data covariance matrix</span><br></code></pre></td></tr></table></figure>
<p>compute the SVD factorization of the data covariance matrix:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">U,S,V = np.linalg.svd(cov)<br></code></pre></td></tr></table></figure>
<p>where the columns of
<code class="language-plaintext highlighter-rouge">U</code> are the
eigenvectors and
<code class="language-plaintext highlighter-rouge">S</code> is a 1-D
array of the singular values. To decorrelate the data, we project the
original (but zero-centered) data into the eigenbasis:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">Xrot = np.dot(X, U)<br></code></pre></td></tr></table></figure>
<p>dimensionality reduction:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">Xrot_reduced = np.dot(X, U[:,:<span class="hljs-number">100</span>]) <span class="hljs-comment"># Xrot_reduced becomes [N x 100]</span><br></code></pre></td></tr></table></figure>
<p>For <strong>whitening</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># whiten the data:</span><br><span class="hljs-comment"># divide by the eigenvalues (which are square roots of the singular values)</span><br>Xwhite = Xrot / np.sqrt(S + <span class="hljs-number">1e-5</span>)<br></code></pre></td></tr></table></figure>
<figure>
<img src="/typora-user-images/image-20230207144339020.png" srcset="/img/loading.gif" lazyload
alt="image-20230207144339020" />
<figcaption aria-hidden="true">image-20230207144339020</figcaption>
</figure>
<p><strong>Common pitfall</strong> The mean must be computed only over
the training data and then subtracted equally from all splits
(train/val/test).</p>
<h4 id="weight-initialization">Weight Initialization</h4>
<p><strong>Pitfall: all zero initialization</strong> That will make
every neuron do same thing.</p>
<p><strong>Small random numbers</strong> aim: <em>symmetry
breaking</em>. <code>W=0.01*np.random.randn(D, H)</code></p>
<p><strong>Calibrating the variances with 1/sqrt(n)</strong>
<code>W=np.random.randn(n) / sqrt(n)</code> where n is the number of its
inputs</p>
<p>Other: <strong>Sparse initialization</strong>, <strong>Initializing
the biases(0)</strong>, <strong>Batch Normalization</strong></p>
<h4 id="regularization-1">Regularization</h4>
<p><strong>L2 regularization</strong> May the most common. for all <span
class="math inline">\(w\)</span>, add <span
class="math inline">\(\frac{1}{2}\lambda w^2\)</span> to the
objective.</p>
<p><strong>L1 regularization</strong> for each weight <span
class="math inline">\(w\)</span>, we add the term <span
class="math inline">\(\lambda |w|\)</span> to the objective.</p>
<p>We can also combine the L1 regularization with the L2 regularization:
<span class="math inline">\(\lambda _1|w|+\lambda_2w^2\)</span>, which
is called Elastic net regularization.</p>
<p><strong>Max norm constraints</strong> Enforce an absolute upper bound
on the magnitude of the weight vector.</p>
<p><strong>Dropout</strong> keep a neuron active with some probability
<span class="math inline">\(p\)</span> or setting it to zero
otherwise</p>
<p>And there are many other methods about regularization. <strong>Bias
regularization, per-layer regularization</strong>…</p>
<h4 id="loss-functions">Loss functions</h4>
<p>Let <span class="math inline">\(f=f(x_i;W)\)</span> to be the
activations of the output layer in a Neural Network.</p>
<p><strong>Classification</strong></p>
<p>SVM: <span class="math display">\[
L_i=\sum_{j\neq y_i}max(0,f_j-f_{y_i}+1)
\]</span></p>
<blockquote>
<p>sometimes use <span
class="math inline">\(max(0,(f_j-f_{y_i}+1)^2)\)</span></p>
</blockquote>
<p>Softmax: <span class="math display">\[
L_i=-log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}})
\]</span> <strong>Problem: Large number of classes</strong> When the set
of labels is very large, Softmax becomes very expensive. It may be
helpful to use <em>Hierarchical Softmax</em>.</p>
<p><strong>Attribute classification</strong></p>
<p>Both losses above assume that there is a single correct answer <span
class="math inline">\(y_i\)</span>. But what if <span
class="math inline">\(y_i\)</span> is a binary vector where every
example may or may not have a certain attribute <span
class="math display">\[
L_i=\sum_jmax(0,1-y_{ij}f_j)
\]</span> where <span class="math inline">\(y_{ij}\)</span> is either +1
or -1</p>
<h3 id="neural-networks-part-3-learning-and-evaluation"><a
target="_blank" rel="noopener" href="https://cs231n.github.io/neural-networks-3/">Neural Networks Part
3: Learning and Evaluation</a></h3>
<p>Talking about learning process.</p>
<h4 id="gradient-checks">Gradient Checks</h4>
<p><span class="math display">\[
\frac{df(x)}{dx}=\frac{f(x+h)-f(x-h)}{2h}
\]</span></p>
<p>In order to compare the numerical gradient <span
class="math inline">\(f_n&#39;\)</span> and analytic gradient <span
class="math inline">\(f_a&#39;\)</span>, we can use relative error:
<span class="math display">\[
\frac{|f_a&#39;-f_b&#39;|}{max(|f_a&#39;|,|f_n&#39;|)}
\]</span> ​ In practive:</p>
<ul>
<li>relative error &gt;<span class="math inline">\(1e-2\)</span>:
probably wrong</li>
<li>1e-2&gt;relative error&gt;1e-4: uncomfortable…</li>
<li>1e-4&gt;relative error: OK…but without kinks(e.g. tanh and softmax),
to high.</li>
<li>1e-7&gt;relative error: happy</li>
</ul>
<blockquote>
<p>should use double precision</p>
</blockquote>
<blockquote>
<p>if jump kinks, may not be exact</p>
</blockquote>
<p>In order to avoid above problems:</p>
<ul>
<li>Use only few datapoints</li>
<li>be careful with h</li>
</ul>
<figure>
<img src="/typora-user-images/image-20230207205258779.png" srcset="/img/loading.gif" lazyload
alt="image-20230207205258779" />
<figcaption aria-hidden="true">image-20230207205258779</figcaption>
</figure>
<ul>
<li><p>Don’t let the regularization overwhelm the data</p></li>
<li><p>Remember to turn off dropout/augmentations</p></li>
</ul>
<h4 id="before-learning-sanity-checks-tipstricks">Before learning:
sanity checks Tips/Tricks</h4>
<ul>
<li>Trace loss function</li>
</ul>
<p><img src="/typora-user-images/image-20230207210235489.png" srcset="/img/loading.gif" lazyload alt="image-20230207210235489" style="zoom:80%;" /></p>
<p>​ The right picture may mean the data is so small</p>
<ul>
<li>Trace train/val accuracy</li>
</ul>
<p>​
<img src="/typora-user-images/image-20230207210422908.png" srcset="/img/loading.gif" lazyload alt="image-20230207210422908" style="zoom:80%;" /></p>
<ul>
<li>Ratio of weights: updates</li>
</ul>
<p>​ <strong>Ex</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># assume parameter vector W and its gradient vector dW</span><br>param_scale = np.linalg.norm(W.ravel())<br>update = -learning_rate*dW <span class="hljs-comment"># simple SGD update</span><br>update_scale = np.linalg.norm(update.ravel())<br>W += update <span class="hljs-comment"># the actual update</span><br><span class="hljs-built_in">print</span> update_scale / param_scale <span class="hljs-comment"># want ~1e-3</span><br></code></pre></td></tr></table></figure>
<ul>
<li><p>Activation / Gradient distributions per layer</p></li>
<li><p>First-layer Visualizations</p></li>
</ul>
<p>​ <strong>Ex</strong></p>
<p><img src="/typora-user-images/image-20230207211414465.png" srcset="/img/loading.gif" lazyload alt="image-20230207211414465" style="zoom:80%;" /></p>
<p>​ Left: many noise. may in trouble. Right: Nice, smooth</p>
<h4 id="parameter-updates">Parameter updates</h4>
<h5 id="first-ordersgd-momentum-nesterov-momentum">First-order(SGD),
momentum, Nesterov momentum</h5>
<ul>
<li><strong>Vanilla</strong> update</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">x += - learning_rate * dx<br></code></pre></td></tr></table></figure>
<ul>
<li><strong>Momentum</strong> update</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Momentum update</span><br>v = mu * v - learning_rate * dx <span class="hljs-comment"># integrate velocity</span><br>x += v <span class="hljs-comment"># integrate position</span><br></code></pre></td></tr></table></figure>
<p>​ mu can be seen as the coefficient of friction in physics. (typical
0.9)</p>
<ul>
<li><strong>Nesterov</strong> Momentum</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">x_ahead = x + mu * v<br><span class="hljs-comment"># evaluate dx_ahead (the gradient at x_ahead instead of at x)</span><br>v = mu * v - learning_rate * dx_ahead<br>x += v<br></code></pre></td></tr></table></figure>
<figure>
<img src="/typora-user-images/image-20230207213105160.png" srcset="/img/loading.gif" lazyload
alt="image-20230207213105160" />
<figcaption aria-hidden="true">image-20230207213105160</figcaption>
</figure>
<p>​ In practice, people like to rename <span
class="math inline">\(x\_head\)</span> as <span
class="math inline">\(x\)</span> :</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">v_prev = v <span class="hljs-comment"># back this up</span><br>v = mu * v - learning_rate * dx <span class="hljs-comment"># velocity update stays the same</span><br>x += -mu * v_prev + (<span class="hljs-number">1</span> + mu) * v <span class="hljs-comment"># position update changes form</span><br></code></pre></td></tr></table></figure>
<h5 id="annealing-the-learning-rate">Annealing the learning rate</h5>
<p>If the learning rate is too high, the system contains too much
kinetic energy, unable to settle down into deeper, but narrower parts of
the loss function.</p>
<p>Normally, there are three methods to decay the learning rate:</p>
<ul>
<li><strong>Step decay</strong> Reduce the learning rate every few
epochs.</li>
<li><strong>Exponential decay</strong> In math: <span
class="math inline">\(\alpha = \alpha_0e^{-kt}\)</span> in which <span
class="math inline">\(\alpha_0,k\)</span> are hyperparameters and <span
class="math inline">\(t\)</span> is the iteration number.</li>
<li><strong>1/t decay</strong> In math: <span
class="math inline">\(\alpha = \alpha _0/(1+kt)\)</span> in which <span
class="math inline">\(\alpha_0,k\)</span> are hyperparameters and <span
class="math inline">\(t\)</span> is the iteration number</li>
</ul>
<p>​ In practice, we find that the step decay is slightly preferable
because the hyperparameters it involves</p>
<h5 id="second-order-methods">Second order methods</h5>
<p>Basing on Newton’s method: <span class="math display">\[
x\leftarrow x-[Hf(x)]^{-1}\nabla f(x)
\]</span> Multiplying by the inverse Hessian leads the optimization to
take more aggressive steps in directions of shallow curvature and
shorter steps in directions of steep curvature</p>
<p>However, because of the expensive cost of calculating the Hessian
matrix, this method is impractical.</p>
<h5 id="per-parameter-adaptive-learning-rate-methods">Per-parameter
adaptive learning rate methods</h5>
<p><strong>Adagrad</strong> is an adaptive learning rate method
originally proposed by <a
target="_blank" rel="noopener" href="http://jmlr.org/papers/v12/duchi11a.html">Duchi et al.</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Assume the gradient dx and parameter vector x</span><br>cache += dx**<span class="hljs-number">2</span><br>x += - learning_rate * dx / (np.sqrt(cache) + eps)<br></code></pre></td></tr></table></figure>
<p><code>eps</code>: 1e-4~1e-8</p>
<p>shortcoming: the monotonic learning rate usually proves too
aggressive and stops learning too early</p>
<p><strong>RMSprop</strong> <a
target="_blank" rel="noopener" href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">slide
29 of Lecture 6</a> of Geoff Hinton’s Coursera class: reduce Adagrad’s
aggressive</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">cache = decay_rate * cache + (<span class="hljs-number">1</span> - decay_rate) * dx**<span class="hljs-number">2</span><br>x += - learning_rate * dx / (np.sqrt(cache) + eps)<br></code></pre></td></tr></table></figure>
<p>in which <code>decay_rate</code> is a hyperparameter: 0.9, 0.99,
0.999</p>
<p><strong>Adam</strong> a recently proposed update looks a bit like
RMSprop with momentum</p>
<p>simplified:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">m = beta1*m + (<span class="hljs-number">1</span>-beta1)*dx<br>v = beta2*v + (<span class="hljs-number">1</span>-beta2)*(dx**<span class="hljs-number">2</span>)<br>x += - learning_rate * m / (np.sqrt(v) + eps)<br></code></pre></td></tr></table></figure>
<p>recommend: <code>eps = 1e-8</code>, <code>beta1 = 0.9</code>,
<code>beta2 = 0.999</code></p>
<p>With the <em>bias correction</em> mechanism, the update looks as
follows:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># t is your iteration counter going from 1 to infinity</span><br>m = beta1*m + (<span class="hljs-number">1</span>-beta1)*dx<br>mt = m / (<span class="hljs-number">1</span>-beta1**t)<br>v = beta2*v + (<span class="hljs-number">1</span>-beta2)*(dx**<span class="hljs-number">2</span>)<br>vt = v / (<span class="hljs-number">1</span>-beta2**t)<br>x += - learning_rate * mt / (np.sqrt(vt) + eps)<br></code></pre></td></tr></table></figure>
<h4 id="hyperparameter-optimization">Hyperparameter optimization</h4>
<p>The most common hyperparameters in context of Neural Networks
include:</p>
<ul>
<li>the initial learning rate</li>
<li>learning rate decay schedule(such as the decay constant)</li>
<li>regularization strength(L2 penalty, dropout strength)</li>
</ul>
<p><strong>Implementation</strong> make a worker and master</p>
<p><strong>Prefer one validation fold to cross-validation</strong> a
single validation set of respectable size substantially simplifies the
code base, without the need for cross-validation with multiple folds</p>
<p><strong>Hyperparameter ranges</strong>
<code>learning_rate = 10 ** uniform(-6, 1)</code></p>
<p><strong>Prefer random search to grid search</strong> can be easily
understand through following image:</p>
<figure>
<img src="/typora-user-images/image-20230208141840955.png" srcset="/img/loading.gif" lazyload
alt="image-20230208141840955" />
<figcaption aria-hidden="true">image-20230208141840955</figcaption>
</figure>
<p><strong>Careful with best values on border</strong> if we find that
the results is on the border, we may set a bad range and miss the true
best result.</p>
<p><strong>Stage your search from coarse to fine</strong></p>
<p><strong>Bayesian Hyperparameter Optimization</strong> <a
target="_blank" rel="noopener" href="https://github.com/JasperSnoek/spearmint">Spearmint</a>, <a
target="_blank" rel="noopener" href="http://www.cs.ubc.ca/labs/beta/Projects/SMAC/">SMAC</a>, and <a
target="_blank" rel="noopener" href="http://jaberg.github.io/hyperopt/">Hyperopt</a>. However, in
practical settings with ConvNets it is still relatively difficult to
beat random search in a carefully-chosen intervals.</p>
<h4 id="evaluation">Evaluation</h4>
<h5 id="model-ensembles">Model Ensembles</h5>
<p>A reliable way to improve the performance of Neural Networks by a few
percent: train multiple independent models, and at test time average
their predictions.</p>
<p>The number of models <span class="math inline">\(\uparrow\)</span>
performance <span class="math inline">\(\uparrow\)</span> the variety of
models <span class="math inline">\(\uparrow\)</span> performance <span
class="math inline">\(\uparrow\)</span></p>
<p>Some approaches to forming an ensemble</p>
<ul>
<li><strong>Same model, different initializations</strong></li>
</ul>
<p>​ Use cross-validation to determine the best hyperparameters, then
train multiple models with the best set of hyperparameters but with
different random initialization.</p>
<p>​ shortcoming: variety is only due to iitialization</p>
<ul>
<li><p><strong>Top models discovered during
cross-validation</strong></p>
<p>Use cross-validation to determine the best hyperparameters, then pick
the top few (e.g. 10) models to form the ensemble.</p></li>
</ul>
<p>​ shortcoming: may include suboptimal models</p>
<ul>
<li><strong>Different checkpoints of a single model</strong></li>
</ul>
<p>​ Taking different checkpoints of a single network over time is
training is very expensive</p>
<p>​ shortcoming: lack of variety</p>
<p>​ advantage: very cheap</p>
<ul>
<li><strong>Running average of parameters during training</strong></li>
</ul>
<p>​</p>
<p>Shortcoming of model ensembles: take longer to evaluate on test
example.</p>
<p>A good idea: “distill” a good ensemble back to a single model by
incorporating the ensemble log likelihoods into a modified
objective.</p>
<h2 id="convolutional-neural-networks">Convolutional Neural
Networks</h2>
<h3
id="convolutional-neural-networks-architectures-convolution-pooling-layers"><a
target="_blank" rel="noopener" href="https://cs231n.github.io/convolutional-networks/">Convolutional
Neural Networks: Architectures, Convolution / Pooling Layers</a></h3>
<p>CNN base on an assumption that the inputs are images.</p>
<h4 id="layers-used-to-build-convnets">Layers used to build
ConvNets</h4>
<p><strong>Convolutional Layer, Pooling Layer, Fully-Connected
Layer</strong></p>
<figure>
<img src="/typora-user-images/image-20230208144748173.png" srcset="/img/loading.gif" lazyload
alt="image-20230208144748173" />
<figcaption aria-hidden="true">image-20230208144748173</figcaption>
</figure>
<h5 id="convolutional-layer">Convolutional Layer</h5>
<p><strong>filter</strong> with size like <span
class="math inline">\(5\times 5\times 3\)</span></p>
<p>During the forward pass, we slide (more precisely, convolve) each
filter across the width and height of the input volume and compute dot
products between the entries of the filter and the input at any
position</p>
<p><span class="math inline">\(\rightarrow\)</span> produce a separate
2-dimensional activation map</p>
<p>The spatial extent of this connectivity: a hyperparameter called the
<strong>receptive field</strong></p>
<p><strong>Depth, Stride, Zero-padding</strong></p>
<p><strong>Depth</strong>: depend on the number of filter. Called “deep
column” or “fiber”</p>
<p><strong>Stride</strong>: the number of pixel the filter will move
when we slide it</p>
<p><strong>Zero-padding</strong>: pad the input volume with zeros around
the border</p>
<p>In math. the input volume size :<span
class="math inline">\(W\)</span>, the receptive size of the Conv Layer
neurons: <span class="math inline">\(F\)</span>, the stride with which
they are applied: <span class="math inline">\(S\)</span>, the amount of
zero padding used: <span class="math inline">\(P\)</span></p>
<p>then the output: <span
class="math inline">\((W-F+2P)/S+1\)</span></p>
<p><strong>Summary</strong></p>
<ul>
<li><p>Accept size: <span class="math inline">\(W_1\times H_1\times
D_1\)</span></p></li>
<li><p>Require:</p>
<ul>
<li>number of filters <span class="math inline">\(K\)</span></li>
<li>spatial extent <span class="math inline">\(F\)</span></li>
<li>stride <span class="math inline">\(S\)</span></li>
<li>the amount of zero padding <span
class="math inline">\(P\)</span></li>
</ul></li>
<li><p>Produce size: <span class="math inline">\(W_2 \times H_2 \times
D_2\)</span></p>
<ul>
<li><span class="math inline">\(W_2=(W_1-F+2P)/S+1\)</span></li>
<li><span class="math inline">\(H_2=(H_1-F+2P)/S+1\)</span></li>
<li><span class="math inline">\(D_2=K\)</span></li>
</ul></li>
</ul>
<p>common set: <span class="math inline">\(F=3, S=1, P=1\)</span></p>
<p><strong>Backpropagation</strong> The backward pass for a convolution
operation (for both the data and the weights) is also a convolution (but
with spatially-flipped filters).</p>
<p><strong><span class="math inline">\(1\times 1\)</span>
convolution</strong> note that we have 3 channels. so it’s not
meaningless</p>
<p><strong>Dilated convolutions</strong> have filters that have spaces
between each cell, called dilation.</p>
<p>##### Pooling layer</p>
<p>Common: Max</p>
<figure>
<img src="/typora-user-images/image-20230208152427080.png" srcset="/img/loading.gif" lazyload
alt="image-20230208152427080" />
<figcaption aria-hidden="true">image-20230208152427080</figcaption>
</figure>
<ul>
<li><p>Accept size <span class="math inline">\(W_1\times H_1\times
D_1\)</span></p></li>
<li><p>Hyperparameters</p>
<ul>
<li>spatial extent <span class="math inline">\(F\)</span></li>
<li>stride <span class="math inline">\(S\)</span></li>
</ul></li>
<li><p>Produce size <span class="math inline">\(W_2\times H_2\times
D_2\)</span></p>
<ul>
<li><span class="math inline">\(W_2=(W_1-F)/S+1\)</span></li>
<li><span class="math inline">\(H_2=(H_1-F)/S+1\)</span></li>
<li><span class="math inline">\(D_2=D_1\)</span></li>
</ul></li>
</ul>
<p>Common: <span class="math inline">\(F=3, S=2\)</span> more commonly
<span class="math inline">\(F=2, S=2\)</span></p>
<h5 id="normalization-layer">Normalization Layer</h5>
<p>These layers have since fallen out of favor because in practice their
contribution has been shown to be minimal, if any. For various types of
normalizations, see the discussion in Alex Krizhevsky’s <a
target="_blank" rel="noopener" href="http://code.google.com/p/cuda-convnet/wiki/LayerParams#Local_response_normalization_layer_(same_map)">cuda-convnet
library API</a>.</p>
<h5 id="fully-connected-layer">Fully-connected layer</h5>
<p>Just like Neural Network section</p>
<h5 id="converting-fully-connected-layers-to-conv-layers">Converting
Fully Connected layers to CONV layers</h5>
<p>Each of these conversions could in practice involve manipulating
(e.g. reshaping) the weight matrix <span
class="math inline">\(W\)</span> in each FC layer into CONV layer
filters. It turns out that this conversion allows us to “slide” the
original ConvNet very efficiently across many spatial positions in a
larger image, in a single forward pass.</p>
<h4 id="convnet-architectures">ConvNet Architectures</h4>
<p>The most common pattern:</p>
<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs clean">INPUT -&gt; [[CONV -&gt; RELU]*N -&gt; POOL?]*M -&gt; [FC -&gt; RELU]*K -&gt; FC<br></code></pre></td></tr></table></figure>
<p><code>N &gt;= 0</code> (and usually <code>N &lt;= 3</code>),
<code>M &gt;= 0</code>, <code>K &gt;= 0</code> (and usually
<code>K &lt; 3</code>).</p>
<ul>
<li><p><code>INPUT -&gt; FC</code>, implements a linear
classifier.</p></li>
<li><p><code>INPUT -&gt; CONV -&gt; RELU -&gt; FC</code></p></li>
<li><p><code>INPUT -&gt; [CONV -&gt; RELU -&gt; POOL]*2 -&gt; FC -&gt; RELU -&gt; FC</code>.
Here we see that there is a single CONV layer between every POOL
layer.</p></li>
<li><p><code>INPUT -&gt; [CONV -&gt; RELU -&gt; CONV -&gt; RELU -&gt; POOL]*3 -&gt; [FC -&gt; RELU]*2 -&gt; FC</code>
Here we see two CONV layers stacked before every POOL layer.</p></li>
</ul>
<p><em>Prefer a stack of small filter CONV to one large receptive field
CONV layer</em>.</p>
<h4 id="layer-sizing-patterns">Layer Sizing Patterns</h4>
<ul>
<li><strong>Input Layer</strong></li>
</ul>
<p>​ Should be divisible by 2 many times.</p>
<p>​ Ex. 32 (e.g. CIFAR-10), 64, 96 (e.g. STL-10), or 224 (e.g. common
ImageNet ConvNets), 384, and 512.</p>
<ul>
<li><strong>Conv Layer</strong></li>
</ul>
<p>​ 3x3 or 5x5 Step <span class="math inline">\(S=1\)</span>, zero</p>
<ul>
<li><strong>Pool Layer</strong></li>
</ul>
<p>​ 2x2 , stride = 2(sometimes 3x3, stride = 2)</p>
<p><strong>Use stride of 1 in CONV</strong>:</p>
<ol type="1">
<li>Smaller strides work better in practice.</li>
<li>Allows us to leave all spatial down-sampling to the POOL layers,
with the CONV layers only transforming the input volume depth-wise.</li>
</ol>
<p><strong>Use padding</strong>:</p>
<p>If the CONV layers were to not zero-pad the inputs and only perform
valid convolutions, then the size of the volumes would reduce by a small
amount after each CONV, and the <strong>information at the
borders</strong> would be “washed away” too quickly.</p>
<p><strong>Compromising based on memory constrains</strong>:</p>
<p>people prefer to make the compromise at only the first CONV layer of
the network. For example, one compromise might be to use a first CONV
layer with filter sizes of 7x7 and stride of 2 (as seen in a ZF net). As
another example, an AlexNet uses filter sizes of 11x11 and stride of
4.</p>
<h4 id="case-studies">Case Studies</h4>
<ul>
<li><strong>LeNet</strong></li>
<li><strong>AlexNet</strong></li>
<li><strong>ZF Net</strong></li>
<li><strong>GoogleNet</strong></li>
<li><strong>VGGNet</strong></li>
<li><strong>ResNet</strong></li>
</ul>
<p>VGG:</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs routeros">INPUT: [224x224x3]        memory:  224<span class="hljs-number">*224</span><span class="hljs-number">*3</span>=150K   weights: 0<br>CONV3-64: [224x224x64]  memory:  224<span class="hljs-number">*224</span><span class="hljs-number">*64</span>=3.2M   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*3</span>)<span class="hljs-number">*64</span> = 1,728<br>CONV3-64: [224x224x64]  memory:  224<span class="hljs-number">*224</span><span class="hljs-number">*64</span>=3.2M   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*64</span>)<span class="hljs-number">*64</span> = 36,864<br>POOL2: [112x112x64]  memory:  112<span class="hljs-number">*112</span><span class="hljs-number">*64</span>=800K   weights: 0<br>CONV3-128: [112x112x128]  memory:  112<span class="hljs-number">*112</span><span class="hljs-number">*128</span>=1.6M   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*64</span>)<span class="hljs-number">*128</span> = 73,728<br>CONV3-128: [112x112x128]  memory:  112<span class="hljs-number">*112</span><span class="hljs-number">*128</span>=1.6M   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*128</span>)<span class="hljs-number">*128</span> = 147,456<br>POOL2: [56x56x128]  memory:  56<span class="hljs-number">*56</span><span class="hljs-number">*128</span>=400K   weights: 0<br>CONV3-256: [56x56x256]  memory:  56<span class="hljs-number">*56</span><span class="hljs-number">*256</span>=800K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*128</span>)<span class="hljs-number">*256</span> = 294,912<br>CONV3-256: [56x56x256]  memory:  56<span class="hljs-number">*56</span><span class="hljs-number">*256</span>=800K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*256</span>)<span class="hljs-number">*256</span> = 589,824<br>CONV3-256: [56x56x256]  memory:  56<span class="hljs-number">*56</span><span class="hljs-number">*256</span>=800K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*256</span>)<span class="hljs-number">*256</span> = 589,824<br>POOL2: [28x28x256]  memory:  28<span class="hljs-number">*28</span><span class="hljs-number">*256</span>=200K   weights: 0<br>CONV3-512: [28x28x512]  memory:  28<span class="hljs-number">*28</span><span class="hljs-number">*512</span>=400K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*256</span>)<span class="hljs-number">*512</span> = 1,179,648<br>CONV3-512: [28x28x512]  memory:  28<span class="hljs-number">*28</span><span class="hljs-number">*512</span>=400K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*512</span>)<span class="hljs-number">*512</span> = 2,359,296<br>CONV3-512: [28x28x512]  memory:  28<span class="hljs-number">*28</span><span class="hljs-number">*512</span>=400K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*512</span>)<span class="hljs-number">*512</span> = 2,359,296<br>POOL2: [14x14x512]  memory:  14<span class="hljs-number">*14</span><span class="hljs-number">*512</span>=100K   weights: 0<br>CONV3-512: [14x14x512]  memory:  14<span class="hljs-number">*14</span><span class="hljs-number">*512</span>=100K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*512</span>)<span class="hljs-number">*512</span> = 2,359,296<br>CONV3-512: [14x14x512]  memory:  14<span class="hljs-number">*14</span><span class="hljs-number">*512</span>=100K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*512</span>)<span class="hljs-number">*512</span> = 2,359,296<br>CONV3-512: [14x14x512]  memory:  14<span class="hljs-number">*14</span><span class="hljs-number">*512</span>=100K   weights: (3<span class="hljs-number">*3</span><span class="hljs-number">*512</span>)<span class="hljs-number">*512</span> = 2,359,296<br>POOL2: [7x7x512]  memory:  7<span class="hljs-number">*7</span><span class="hljs-number">*512</span>=25K  weights: 0<br>FC: [1x1x4096]  memory:  4096  weights: 7<span class="hljs-number">*7</span><span class="hljs-number">*512</span><span class="hljs-number">*4096</span> = 102,760,448<br>FC: [1x1x4096]  memory:  4096  weights: 4096<span class="hljs-number">*4096</span> = 16,777,216<br>FC: [1x1x1000]  memory:  1000 weights: 4096<span class="hljs-number">*1000</span> = 4,096,000<br><br>TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~<span class="hljs-number">*2</span> <span class="hljs-keyword">for</span> bwd)<br>TOTAL params: 138M parameters<br></code></pre></td></tr></table></figure>
<p><strong>Computational Considerations</strong></p>
<p>There are three major sources of memory to keep track of:</p>
<ul>
<li><p>From the intermediate volume sizes</p></li>
<li><p>From the parameter sizes</p></li>
<li><p>Every ConvNet implementation has to maintain
<strong>miscellaneous</strong> memory, such as the image data batches,
perhaps their augmented versions, etc.</p></li>
</ul>
<h3
id="transfer-learning-and-fine-tuning-convolutional-neural-networks"><a
target="_blank" rel="noopener" href="https://cs231n.github.io/transfer-learning/">Transfer Learning and
Fine-tuning Convolutional Neural Networks</a></h3>
<h4 id="transfer-learning">Transfer Learning</h4>
<figure>
<img src="/typora-user-images/image-20230210153134566.png" srcset="/img/loading.gif" lazyload
alt="image-20230210153134566" />
<figcaption aria-hidden="true">image-20230210153134566</figcaption>
</figure>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/notes/">#notes</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>[cs231n] Summary</div>
      <div>http://jamil-yu.github.io/2023/02/18/[cs231n]Summary/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Jamil Yu</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>February 18, 2023</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/02/19/%5BDL%5DBasic-Net/" title="[DL] Basic-Net">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">[DL] Basic-Net</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/01/25/Modelling-Group-introduction-about-Markdown-and-LaTeX/" title="[iGEM] Modelling Group: introduction about Markdown and LaTeX">
                        <span class="hidden-mobile">[iGEM] Modelling Group: introduction about Markdown and LaTeX</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <div class="disqus" style="width:100%">
    <div id="disqus_thread"></div>
    
      <script>
        Fluid.utils.loadComments('#disqus_thread', function() {
          Fluid.utils.createCssLink('https://lib.baomitu.com/disqusjs/1.3.0/disqusjs.css');
          Fluid.utils.createScript('https://lib.baomitu.com/disqusjs/1.3.0/disqus.js', function() {
            new DisqusJS({
              shortname: 'fluid',
              apikey: ''
            });
          });
        });
      </script>
    
    <noscript>Please enable JavaScript to view the comments</noscript>
  </div>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  









    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
